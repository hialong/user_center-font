<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[myVault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>myVault</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 12 May 2024 12:50:25 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 12 May 2024 12:50:24 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[项目部署（阿里云）]]></title><description><![CDATA[ 
 <br><br>购买就直接省略了，由于当前 20240429 我不知道怎么买最划算，所以就直接试用阿里云三个月<br>
参考网址 <a data-tooltip-position="top" aria-label="https://blog.csdn.net/BThinker/article/details/123358697" rel="noopener" class="external-link" href="https://blog.csdn.net/BThinker/article/details/123358697" target="_blank">Docker 安装 (完整详细版)_docker安装-CSDN博客</a><br>工具包不知道干嘛的（没运行也能下载）<br>## 安装Docker所需要的一些工具包
sudo yum install -y yum-utils
复制<br>仓库地址需要添加，不然后面下不下来<br>## 建立Docker仓库 (映射仓库地址)
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

## 下载docker
sudo yum install docker-ce docker-ce-cli containerd.io

复制<br>然后就是直接运行 docker 了<br>sudo systemctl start docker
复制<br>然后检查一下，直接 ps -a<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429170153.png" referrerpolicy="no-referrer"><br><br>参考<a data-tooltip-position="top" aria-label="https://blog.csdn.net/BThinker/article/details/123471514" rel="noopener" class="external-link" href="https://blog.csdn.net/BThinker/article/details/123471514" target="_blank">Docker 安装 Mysql 容器 (完整详细版)_linux docker mysql安装-CSDN博客</a><br>
直接看本地的 mysql 版本号我直接用的 mysql --help 看的<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429170444.png" referrerpolicy="no-referrer"><br>所以直接去虚拟机里面直接 pull<br>## 后面是版本号，不加冒号和版本号就是下最新的
docker pull mysql:x.xx.xx
复制<br>然后运行，这里的密码随便设置一下, 要注意<br>docker run \
--name mysql \
-d \
-p 3306:3306 \
--restart unless-stopped \
-v /mydata/mysql/log:/var/log/mysql \
-v /mydata/mysql/data:/var/lib/mysql \
-v /mydata/mysql/conf:/etc/mysql \
-e MYSQL_ROOT_PASSWORD=${password} \
mysql:8.0.35
复制<br>用上面的命令后，实例一直处于 restrating 状态，所以先去看看日志<br>docker logs -f mysql
复制<br>日志如下<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429172045.png" referrerpolicy="no-referrer"><br>
看起来是没找到对应的配置文件，<br>
顺便注意一下，删除实例这样<br>docker rm -f ${pid}
复制<br>所以重写一下配置文件，对应到 conf. d 就行了，删掉重整<br>docker run \
--name mysql \
-d \
-p 3306:3306 \
--restart unless-stopped \
-v /mydata/mysql/log:/var/log/mysql \
-v /mydata/mysql/data:/var/lib/mysql \
-v /mydata/mysql/conf:/etc/mysql/conf.d \
-e MYSQL_ROOT_PASSWORD=${password} \
mysql:8.0.35
复制<br>ok<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429173305.png" referrerpolicy="no-referrer"><br>现在就是跑数据库了<br>
直接登录, 自己输入密码<br>mysql -uroot -p

复制<br>注意如果要外部访问还要自己开放端口才行阿里云的看这个 <a data-tooltip-position="top" aria-label="https://blog.csdn.net/qq_43781399/article/details/112650755" rel="noopener" class="external-link" href="https://blog.csdn.net/qq_43781399/article/details/112650755" target="_blank">Docker配置MySQL容器+远程连接（全流程）_连接docker中的mysql-CSDN博客</a><br>腾讯云就搜一下别的，配置安全组就好<br>然后运行一下写好的建表语句，那么我们的 mysql 就配置好了<br><br>编写 dockerfile，然后进入对应的代码目录 docker build -t, 注意后面有个点 <br>docker build -t user-center-backend:v0.0.1 .
复制<br>关于 dockerfile 前面试了几次，里面是需要安装 git 的，新的虚拟机需要先 yum install git<br>dockerfile 如下（目前就 alpine 的 3.19 以后的版本好像可以安装 jdk21 前面的版本不行的）<br>#Docker 镜像构建
#@author: hailong
FROM alpine:3.19.1 as builder

#添加说明等信息
LABEL name=bkp description="jdk21 base on alpine image"

#安装jdk
RUN apk add --no-cache --no-cache openjdk21

#安装maven
RUN apk add --no-cache --no-cache maven

# Copy local code to the container image.
WORKDIR /app
COPY pom.xml .
COPY src ./src

# Build a release artifact.
RUN mvn package -DskipTests

# Run the web service on container startup.
CMD ["java","-jar","/app/target/user-center-backend-0.0.1-SNAPSHOT.jar","--spring.profiles.active=prod"]
复制<br>搞定后开始运行容器，这个-d 就是后台运行的意思，不然你会前台运行 docker 等你退出，就停止了<br>docker run --name user_center_backend  -d -p 8881:8881 user-center-backend:v0.0.1
复制<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240507174219.png" referrerpolicy="no-referrer"><br>浏览器试试，就是访问成功了<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240507174455.png" referrerpolicy="no-referrer"><br><br>先 build dist 包，然后编写 dockerfile 如下<br>FROM nginx  
  
WORKDIR /usr/share/nginx/html/  
USER root  
  
COPY ./docker/nginx.conf /etc/nginx/conf.d/default.conf  
  
COPY ./dist  /usr/share/nginx/html/  
  
EXPOSE 80  
  
CMD ["nginx", "-g", "daemon off;"]
复制<br>这里面有一个 docker 文件夹也要记得写，里面是 nginx 的配置 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240508191804.png" referrerpolicy="no-referrer"><br>server {  
    listen 80;  
  
    # gzip config    gzip on;  
    gzip_min_length 1k;  
    gzip_comp_level 9;  
    gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml;  
    gzip_vary on;  
    gzip_disable "MSIE [1-6]\.";  
  
    root /usr/share/nginx/html;  
    include /etc/nginx/mime.types;  
  
    location / {  
        try_files $uri /index.html;  
    }  
}


复制<br>然后上环境之后，打镜像包<br> docker build -t user-center-front:v0.0.1 .
复制<br>然后创建运行容器<br>docker run --name user_center_front  -d -p 80:80 user-center-front:v0.0.1
复制<br>几个坑首先是 nginx 里面的配置，空格少打了都会报错，if 和 = 前后都要有空格，不然会出错<br>session 丢失问题, 用跨域浏览器可能会导致 sessionId 丢失问题，这里涉及到 sessionId 的问题，服务器需要获取到 sessionID 才能获取到对应的 session，而 session 是和浏览器会话是相关的，所以使用了不安全的浏览器配置（Chrome 的取消跨域检测）就可能会导致后端获取不到对应的session<br>nginx 配置跨域问题需要配置为自己的端口，跨域不止是 ip 而且还有端口<br><br># 向/etc/apt/sources.list.d/debian.sources添加网易的源
echo 'deb http://mirrors.163.com/debian/ jessie main non-free contrib' &gt;&gt; /etc/apt/sources.list.d/debian.sources &amp;&amp; \
echo 'deb http://mirrors.163.com/debian/ jessie-updates main non-free contrib' &gt;&gt; /etc/apt/sources.list.d/debian.sources &amp;&amp; \
echo 'deb http://mirrors.163.com/debian-security/ jessie/updates main non-free contrib' &gt;&gt; /etc/apt/sources.list.d/debian.sources
复制<br>然后 apt-get update, apt-get install vim<br><br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/weixin_38650077/article/details/106540825" rel="noopener" class="external-link" href="https://blog.csdn.net/weixin_38650077/article/details/106540825" target="_blank">IDEA 远程调试[Debug]_idea 远程调试客户端应该连接哪个 ip-CSDN 博客</a><br>
<br>有时间尝试一下能不能远程调试
]]></description><link>1-项目\用户中心项目\项目部署（阿里云）.html</link><guid isPermaLink="false">1-项目/用户中心项目/项目部署（阿里云）.md</guid><pubDate>Sat, 11 May 2024 03:52:31 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429170153.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240429170153.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[引入knife4j]]></title><description><![CDATA[ 
 <br><br>springBoot 2 版本的时候用的 springfox 下的 swagger，但是 springboot3 不一样，踩了坑，改了很多，分两点讲<br><br>首先引入依赖<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141239.png" referrerpolicy="no-referrer"><br>&lt;!-- https://doc.xiaominfo.com/knife4j/documentation/get_start.html--&gt;  
&lt;dependency&gt;  
&lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt;  
&lt;artifactId&gt;knife4j-spring-boot-starter&lt;/artifactId&gt;  
&lt;version&gt;3.0.3&lt;/version&gt;  
&lt;/dependency&gt;
复制<br>然后创建一个类随便配置一下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141306.png" referrerpolicy="no-referrer"><br><br>咱们根据文档可以看到<br>
<a data-tooltip-position="top" aria-label="https://doc.xiaominfo.com/docs/quick-start" rel="noopener" class="external-link" href="https://doc.xiaominfo.com/docs/quick-start" target="_blank">快速开始 | Knife4j</a><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141405.png" referrerpolicy="no-referrer"><br>
这个依赖都不一样了，那么配置自然就不一样了<br>
所以先引入<br>&lt;dependency&gt;  
&lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt;  
&lt;artifactId&gt;knife4j-openapi3-jakarta-spring-boot-starter&lt;/artifactId&gt;  
&lt;version&gt;4.4.0&lt;/version&gt;  
&lt;/dependency&gt;
复制<br>然后配置也不一样了<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141510.png" referrerpolicy="no-referrer">]]></description><link>1-项目\用户中心项目\引入knife4j.html</link><guid isPermaLink="false">1-项目/用户中心项目/引入knife4j.md</guid><pubDate>Sun, 12 May 2024 08:13:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141239.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511141239.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[自定义注解实现权限校验]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://www.cnblogs.com/sgh1023/p/13363679.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/sgh1023/p/13363679.html" target="_blank">Spring Boot使用AOP的正确姿势 - James_Shangguan - 博客园</a><br>首先我们需要 springboot 中的 aop 来帮助我们实现切面编程，这样才有办法实现通过注解在操作之前进行权限的校验<br>添加依赖<br>&lt;dependency&gt;  
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  
&lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;  
&lt;/dependency&gt;
复制<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511193102.png" referrerpolicy="no-referrer"><br>拥有了这个依赖我们就可以使用注解进行切点切面的定义了<br><br> @Pointcut 注解可以在一个切面内定义可重用的切点，表达式可以写很多如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511193710.png" referrerpolicy="no-referrer"><br>@before、@after 等就是明确在切点前后进行操作的一些操作<br>
如同下面这样<br>@Aspect
@Component
public class AopAdvice {

    @Pointcut("execution (* com.shangguan.aop.controller.*.*(..))")
    public void test() {

    }

    @Before("test()")
    public void beforeAdvice() {
        System.out.println("beforeAdvice...");
    }

    @After("test()")
    public void afterAdvice() {
        System.out.println("afterAdvice...");
    }

    @Around("test()")
    public void aroundAdvice(ProceedingJoinPoint proceedingJoinPoint) {
        System.out.println("before");
        try {
            proceedingJoinPoint.proceed();
        } catch (Throwable t) {
            t.printStackTrace();
        }
        System.out.println("after");
    }

}

复制<br><br><a data-tooltip-position="top" aria-label="https://www.cnblogs.com/songsongsun/p/14517977.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/songsongsun/p/14517977.html" target="_blank">使用自定义注解和切面AOP实现Java程序增强 - 皮皮松 - 博客园</a><br>
除了@pointcut 我们还有一种方式可以进行定义切点，那就是自定义注解实际上，这样的方式我们也可以做到操作的拦截<br>@Around("@annotation(authCheck)")<br>其中 authCheck 就是一个自定义的注解，这样我们就可以在这个方法（所有被自定义注解注解了的方法）前后都织入我们的操作<br>那么首先就是写我们的自定义注解<br><br>直接如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511194126.png" referrerpolicy="no-referrer"><br><br>我们需要一些小技巧，这样可以通过上下文获取request<br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/javacrazy_/article/details/116588422" rel="noopener" class="external-link" href="https://blog.csdn.net/javacrazy_/article/details/116588422" target="_blank">RequestContextHolder详解-CSDN博客</a><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511195410.png" referrerpolicy="no-referrer"><br>关于这个 attribute 有很多实现类，点进去就能看到<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511195744.png" referrerpolicy="no-referrer"><br>由于我们这个简单的项目，里面应该就是 servlet，所以我们直接强转成 servletRequestAttribute 就行了，就能拿到当前的 request 了<br>实现如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511210926.png" referrerpolicy="no-referrer"><br>就行了<br>需要注意的几个点<br>
一个是拦截器需要注解<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511223424.png" referrerpolicy="no-referrer"><br>
另一个是启动类也需要注解<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511223447.png" referrerpolicy="no-referrer"><br>
该函数是 Spring AOP 中的一个注解@EnableAspectJAutoProxy，用于启用 AspectJ 自动代理。<br>
proxyTargetClass = true 表示使用 CGLIB 动态代理目标类，即使目标类没有实现接口。<br>
exposeProxy = true 表示将代理对象暴露给 ThreadLocal，以便在需要时可以手动获取代理对象。<br>
这个注解通常用于开启切面编程，使得我们可以使用注解方式定义切面，并在运行时自动织入到目标对象中。]]></description><link>1-项目\用户中心项目\自定义注解实现权限校验.html</link><guid isPermaLink="false">1-项目/用户中心项目/自定义注解实现权限校验.md</guid><pubDate>Sat, 11 May 2024 14:35:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511193102.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240511193102.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[监听回调]]></title><description><![CDATA[ 
 <br><br>监听和回调本质就是对象对事件的处理方式：<br>
1. 监听是处理者不断监视是否有对应事件发生，当有事件发生时就采取相应的处理事件的措施，<br>
2. 回调是事件发生后通过通过事件主动调用目标对象处理方法来处理事件的方式（这也属于一种通信方式）<br>
实际上就是处理事务的主体不一样了，承担该责任的人不一样，产生的效果是一样的，但是很明显，回调产生的消耗会更少<br><br>监听和回调都是为了处理多任务的一种处理方式，而中断是多任务处理的具体实现。<br>
我们知道，在计算机中管理的外围设备很多，管理起来非常复杂，并且这种管理的过程本身就是多任务的。因为外设输入的不确定性，处理器需要一种机制，当外设有输入请求时立即处理这种请求。<br>为什么要立即处理外设请求呢？如果不立即处理的话，举个例子，键盘必须要等到计算机处理完正在进行的任务时才能输入。另一方面，诸如网卡这一类的设备，存在一个缓冲区，也就造成了网卡数据是有声明周期的，如果不立即处理，后边来的网络数据会由于缓冲区满了而导致前边的数据丢失。<br>不用多说，对构造这样一个系统，回调的方式才是最好的，有外围设备信号时，直接通过一种机制来调用 CPU（如果抽象的够好的话，cpu 可以看作一个可调用函数，我们只管往这个函数填充任务序列作为参数）执行处理的代码即可<br>但是实际上，CPU 是不支持回调这种操作的，因为 CPU 是个只管读 IP 寄存器并且执行指令的傻子<br>so，中断就是为了解决这个问题产生的。<br>
中断的执行流程其实很简单：在CPU接受到事件的“回调”指令时，会通过中断来打断正在进行的任务的执行，并且将目前的一些任务数据保存起来，然后CPU会跳转执行中断处理的函数，当中断处理函数执行完毕后，CPU又会把刚刚保存的任务数据恢复，并且紧接着执行刚刚的任务
<br>而中断打断CPU的执行是CPU本身支持的功能。]]></description><link>2-领域\后端学习\计算机与网络\监听回调.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/监听回调.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate></item><item><title><![CDATA[计算机网络学习]]></title><description><![CDATA[ 
 <br><br><br><br>在Osi这个标准模型中，每个分层都接受由他下一层所提供的特定服务，并负责为自己的上一层提供特定的服务<br>​	上下层之间的交互所遵循的约定叫叫接口，同一层次之间的交互遵循的约定就叫做协议<br>- 物理层
- 数据链路层
- 网络层
- 运输层
- 会话层
- 表示层
- 应用层
复制<br><img alt="image-20231205002700022.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231205002700022.png" referrerpolicy="no-referrer"><br><br><img alt="image-20231205003935780.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231205003935780.png" referrerpolicy="no-referrer"><br><br>
<br>
应用层 ：构建一个HTTP请求报文传给运输层

<br>
运输层：运输层给报文添加一个TCP首部，使之成为TCP报文段，

该首部的作用是区分应用进程以及实现可靠传输


<br>
网络层:给报文段添加一个IP首部，使之成为IP数据报

​	作用是使得IP数据报可以在网络上传输，也就是被路由器转发


<br>
数据链路层：给IP数据报添加一个首部和一个尾部（ETH）使之成为帧

首部的目的是为了能让帧在网络传输的时候可以被相应的主机接收
尾部的目的是为了让收到的目的主机检测收到的帧是否有误码


<br>
物理层：将帧视为比特流，还会给比特流前面添加前导码，然后转化成相应的信号发送给传输媒体

前导码的作用是为了让目的主机做好接收帧的准备


<br><br> 同发送流程差不多，反过来拆分就是，就是层层封装和层层解码的区别<br><br>
<br>实体
<br>协议
<br>服务
<br><br>​	实体是指任何可以发送或者接收信息的硬件或软件进程<br>
<br>对等实体：收发双方相同层次中的实体，（我理解就是分层，我传输有五层，层层包装，你接受有五层层层解码，那么对应的五层结构中就一一对应为对等实体，我的应用对你的应用等）
<br><br>​	是指控制两个对等实体进行逻辑通信的规则的集合，如应用层在应用层协议的规则下（HTTP,SMTP等）进行通信<br><br>指每一层结构都会享受封装层提供的服务，就比如我链路层给你加完了前后部，我需要物理层提供转成比特流并发送的服务<br>下层使用上层的服务需要通过与下层交换一些命令，称之为服务原语<br>
<br>服务访问点：例如数据链路层的服务访问点为“帧”的类型字段
<br><img alt="image-20231205010923418.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231205010923418.png" referrerpolicy="no-referrer"><br><br><br>
<br>首先设计这个模型的原因就是如果各家的网络模型都不一样的话，就会导致大家只能用同一个产品，无法产品间的相互通信
<br>其次，分层的作用是方便更改和替换层间不同的协议，比如以打电话为例，底层传输是汉语交流，下层协议是电话，那么完全可以替换成无线电的新底层，变成用无线电对话
<br><br>
<br>OSI模型有7层，但是只是一个标准模型，实际上linux里面的模型是TCP/IP的四层
<br>TCP/IP有四层
<br><br>
<br>首先是OSI的七层结构（大致自己理解是这样的）

<br>最上级是应用层：应用顾名思义，他的协议就是用来处理不同应用间的通信的，qq对qq，微信对微信
<br>第二级是表示层，这里会对应用层的请求加上首部和尾部（每一层基本都是差不多这样，下面不多赘述），是负责把请求转换成标准的格式，相当于标准化
<br>然后是会话层，负责建立会话--》建立通信和断开通信，
<br>然后是传输层：可靠传输，我理解就是和目标主机对接的时候，告诉他这个数据是发送给你的？？，然后目标主机会检测首部和尾部，判断传输的数据是否安全能否解码发给上层？？
<br>网络层：将数据传到目标地址，负责寻址和路由选择-&gt;我理解就是总线（如果有）上所有的目标主机都会拿到这串数据，而非目标主机则不会收到数据
<br>数据链路层，这一层应该属于硬件层面了，协议会根据通信的方式，比如wifi/路由器的方式的不同而不同
<br>物理层，这一层就是把之前所有的数据转成比特0和1，即计算机信号发送了


<br>TCP/IP

<br>应用层，这里就有大名鼎鼎的HTTP协议了

<br>浏览器和服务端之间的通信用的是HTTP协议
<br>文件传输 FTP协议
<br>远程登录 TELENET 和SSH


<br>传输层：这个是用端口号来识别是那些程序之间相互通信的 

<br>包含了TCP协议，这个TCP协议很可靠，可以处理很多异常情况，但是问题就是他为了建立和断开连接，要七次发包收包，所以这个会导致网络流量的浪费，不利于视频之类的传输（为了解决这个问题，定义了许多复杂规范）
<br>包含UDP协议，甩手掌柜型，无连接的传输层协议，发完就不管我事了，对方有没有收到需要应用自己实现，常用于视频、广播之类的地方


<br>网络层：相当于OSI的第三层，网络层，使用的是IP协议

<br>IP是什么

<br>IP是协议
<br>他是指跨越网络发送数据包，使整个互联网都能收到数据的协议
<br>怎么传输数据包？-》用IP地址作为标识
<br>他不具备重发机制，就是没发送成功也不管，发就发了，属于非可靠性传输协议，但是为了这个问题，又多了一个ICMP协议
<br>ICMP协议，如果双方都有这个协议，一旦IP协议发送的数据包无法到达对方，那就给发送端发送一个异常通知




<br>网卡层（网络接口层/数据链路层）：这个可以跟下面合起来称网络通信层，这个就是说，配合你的硬件还要有软件驱动的协议配合，才能提供服务
<br>硬件（这个可有可无，是因为TCP|IP协议是确定互联的设备能通信的前提下提出来的，所以算不上第五层）


<br><br> OSI的模型更加细化，但是实际执行起来很麻烦，而TCP/IP模型将复杂的模型进行了简化，方便各个厂商的开发<br><br><img alt="image-20231206235843555.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231206235843555.png" referrerpolicy="no-referrer"><br><br><br>
IP协议的作用是把各种数据包传输给对方（数据包是网络传输的最小单位）,而确保发送到对方那要满足很多条件，最重要的两个是IP地址和MAC地址
​	IP地址可以换，是指节点分配到的地址
​	MAC地址是网卡所属的固定地址，基本上不会换
<br><br>
TCP协议用来提供可靠的字节流服务

<br>
什么是字节流服务？--》将大块数据分割成以报文段（segment）为单位的数据包进行传输

<br>
可靠怎么可靠？--》三次握手

​	重点的两个TCP的标志 SYN和ACK（synchronize和acknowledge）

<br>发送端发送一个带SYN标志的数据包给对方
<br>接受端返回一个SYN/ACK标志的数据包表示确认了信息
<br>发送端再发送一个ACK标志的数据包代表握手结束




<br><br>他提供域名到IP地址的解析服务<br><img alt="image-20231207002545814.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231207002545814.png" referrerpolicy="no-referrer"><br><br>
<br>
HTTP是无状态协议，也就是说HTTP这个响应没有持久化处理，发了就发了就不记得了

<br>
HTTP使用URI定位网络资源

<br>
HTTP的方法
- get：获取资源
复制

<br>
post：传输实体主体

<br>
put：传输文件之类的，但是Http1.1的put方法不带验证机制，所以不要用<br>
- Head:获取报文首部

<br>
DELETE:根据URI删除指定资源，跟put一样不安全

<br>
OPTIONS:询问支持的方法<br>
- <img alt="image-20231207004326286.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231207004326286.png" referrerpolicy="no-referrer">

<br>
TRACE:追踪路径

<br>
...



<br>
持久连接：之前的HTTP每次连接都要TCP握手三次，浪费性能，现在HTTP1.1所有连接都是默认持久连接（任一端没有提出断开，就一直连接）

<br>
持久连接使得管线化成为可能：就是发送请求不用等发送成功，并行发多个请求

<br><br>
<br>因为HTTP是一个无状态的协议，没有持久化，但是又想要去管理客户端的状态（比如一次登录，访问各个界面）让服务端管理是管不过来的，然后就有了Cookie，Cookie就是说，发送请求拿到响应后setCookie，下次发的时候带上Cookie，服务端就知道你已经登录了
<br><br>-  报文大致分为报文首部和报文主体，两个用空行来区分
复制<br>
<br>获取部分内容的范围请求<br>
-  <img alt="image-20231207005854719.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231207005854719.png" referrerpolicy="no-referrer">
<br><br>
<br>URL指的就是网页地址 <a data-tooltip-position="top" aria-label="http://www.baidu.com" rel="noopener" class="external-link" href="http://www.baidu.com" target="_blank">www.baidu.com</a>
<br>URI是Uniform Resource Identifier的缩写
<br>
URI是用字符串标识某一互联网资源，而URL是表示资源的地点，可见URL是URI的子集
<br><br>
<br>请求报文和响应报文，大致分为报文首部和报文主体两块， 报文中以空行为分割（CR+LF）
<br>分割发送，范围i请求+首部range来指定byte范围
<br><br>
<br>2XX 成功
<br>3XX 重定向
<br>4XX  客户端错误
<br>5XX 服务器错误
<br><br>- HTTP1.1规范允许HTTP服务器搭建多个Web站点，就是可以运行多个不同的网站
- 但是比如一个服务器运行了多个网站，客户端在访问的时候，会采用 www.aaa.com或者www.bbb.com这种方式，但是在互联网上经过DNS服务转换之后，会变成相同的域名 比如              “http://.192.125.28:8080” ,这时候要弄清楚到底访问的是哪个的话，就要在Host首部内完整指定主机名或域名的URI
复制<br>
<br>代理、网关、隧道

<br>代理是一种有转发功能的应用程序，它扮演了位于服务器和客户端“中间人"的角色，接收由客户端发送的请求并转发给服务器，同时也接收服务器返回的响应并转发给客户端。
<br>网关 是转发其他服务器通信数据的服务器，接收从客户端发送来的请求时，它就像自己拥有资源的源服务器一样对请求进行处理。有时客户端可能都不会察觉，自己的通信目标是一个网关
<br>隧道是在相隔甚远的客户端和服务器两者之间进行中转，并保持双方通信连接的应用程序。


<br><img alt="image-20231213010052025.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231213010052025.png" referrerpolicy="no-referrer"><br>HTTP请求的一些首部，可以使得http请求更加定制化<br>​	<img alt="image-20231213010209015.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231213010209015.png" referrerpolicy="no-referrer"><br>有时候，服务器返回的是缓存里面的数据，可以利用首部，去选择接受哪些数据<br><br>http协议与TCP/UDP类似，也是需要在实际传输的数据前面加上一些头数据，包含协议所必须的一些字段，比如发送方和接受方的端口号，包序号和标志位等，但是和TCP/UDP不同的是，HTTP是一个纯文本的协议，头部数据都是ASCII码的文本，很容易用肉眼阅读，其请求报文和响应报文基本相同，由三大块组成<br>​	<br>
<br>起始行（start line）(也就是请求行)：描述请求或响应的基本信息;
<br>头部字段集合（header）：使用 key-value 形式更详细地说明报文;
<br>.消息正文（entity）：实际传输的数据，它不一定是纯文本，可以是图片、视频等二进制数据。
<br><br>请求行由三部分组成，三个部分通常使用空格分隔，最后用CRLF表示换行结束<br>“.\img\image-20231227230603762.png” could not be found.<br>
<br>请求方法
<br>请求目标
<br>版本号
<br>举个例子<br>GET / HTTP/1.1 
复制<br>GET”是请求方法，“/”是请求目标，“HTTP/1.1”是版本号<br><br>也是三部分<br>
<br>版本号
<br>状态码
<br>原因
<br>“.\img\image-20231227230849528.png” could not be found.<br>HTTP/1.1 200 OK
复制<br>Http1.1就是版本号、200就是状态码、ok就是原因，成功就是OK<br>HTTP/1.1 404 Not Found
复制<br>这个后面就更详细的说明了原因 NOTFound<br><br>请求行或者状态行，加上头部字段集合就构成了HTTP报文里面完整的请求头或者响应头<br>“.\img\image-20231227231347322.png” could not be found.<br>而头部字段，是key value的形式，key value之间用  ： 分隔，最后用CRLF表示换行结束<br>
<br>
头部字段的注意事项

<br>字段名不区分大小写
<br>字段名不允许出现空格，但是可以用  "-" ，但不能使用下划线
<br>字段名后面必须紧跟着  ":" ，不能有空格，但是 “：” 后面的字段值前可以有多个空格
<br>字段顺序是没有意义的，可以任意排列不影响
<br>字段原则上不允许重复，除非这个字段本身的语义允许，例如 set-Cookie


<br>
常用头字段 
首先是头字段基本可以分为四类

<br>通用字段，在请求头和响应头里面都能出现
<br>请求字段：仅能出现在请求头里面，
<br>响应字段：仅能出现在响应头里面
<br>实体字段：实际上属于通用字段，但是专门描述body的额外信息


<br>然后是常见的一些字段<br>
<br>HOST 字段；属于请求字段，只能出现在请求头里面，也是必须要求出现的字段，如果请求头里面没有这个字段，那这个就是一个错误的请求头,host字段告诉服务器这个请求应该由哪个主机来处理,当一台计算机上托管了很多虚拟主机的时候，服务端就需要host字段来判断使用哪个主机
<br>User-Agent字段：这个是 用来描述发起HTTP请求的客户端，比如Chrome、Safari，这个字段只能稍微判断一下，因为每个浏览器都可能说自己是Chrome但是实际上是爬虫
<br>。。。
<br><br>
<br>
目前HTTP/1.1里面规定了八种方法，单词还必须是大写的

<br>
GET ：获取资源

<br>
HEAD： 获取资源元信息，只返回响应头，就是所谓的元信息

<br>
POST： 向资源提交数据

<br>
PUT：类似于POST

<br>
DELETE：DELETE，删除资源

<br>
CONNECT：建立特殊的连接隧道

<br>
OPTIONS：列出可对资源实行的方法

<br>
TRACE：追踪请求
请求是否接受是由服务器判断的，服务器可以返回 200 404 403 405 等



<br><br>get方法就是基本的获取资源的方法，但是可以搭配URI实现更精细的操作<br>比如 在URI使用“#”就可以获取页面后直接定位某个标签的位置<br>使用 If-Modified-Since  就变成了有条件的请求，仅当资源被修改的时候才会执行获取动作<br><br>HEAD方法与 GET 方法类似，也是请求从服务器获取资源，服务器的处理机制也是一样的，但服务器不会返回请求的实体数据，只会传回响应头，也就是资源的“元信息”。<br>HEAD 方法可以看做是 GET 方法的一个“简化版”或者“轻量版”。因为它的响应头与 GET 完全相同，所以可以用在很多并不真正需要资源的场合，避免传输 body 数据的浪费。<br><br>post就是发送请求，把请求放在请求body里面，然后拼好POST请求头，通过TCP协议发送给服务器，<br>PUT基本类似于POST，所以基本不用了，与post不同的就是他更倾向于更新<br><br>DELETE方法指示服务器删除资源，因为这个动作危险性太大，所以通常服务器不会执行真正的删除操作，而是对资源做一个删除标记。当然，更多的时候服务器就直接不处理 DELETE 请求。<br>CONNECT是一个比较特殊的方法，要求服务器为客户端和另一台远程服务器建立一条特殊的连接隧道，这时 Web 服务器在中间充当了代理的角色。<br>OPTIONS 方法要求服务器列出可对资源实行的操作方法，在响应头的 Allow 字段里返回。它的功能很有限，用处也不大，有的服务器（例如 Nginx）干脆就没有实现对它的支持。这个在工作中遇到过，跨域的时候，chrome服务器会先发送一个opional方法，通过了才发送真正的跨域请求<br>TRACE方法多用于对 HTTP 链路的测试或诊断，可以显示出请求 - 响应的传输路径。它的本意是好的，但存在漏洞，会泄漏网站的信息，所以 Web 服务器通常也是禁止使用。<br><br>关于请求方法还有两个面试时有可能会问到、比较重要的概念：安全与幂等。<br>在 HTTP 协议里，所谓的“安全”是指请求方法不会“破坏”服务器上的资源，即不会对服务器上的资源造成实质的修改。<br>按照这个定义，只有 GET 和 HEAD 方法是“安全”的，因为它们是“只读”操作，只要服务器不故意曲解请求方法的处理方式，无论 GET 和 HEAD 操作多少次，服务器上的数据都是“安全的”。<br>而 POST/PUT/DELETE 操作会修改服务器上的资源，增加或删除数据，所以是“不安全”的。<br>所谓的“幂等”实际上是一个数学用语，被借用到了 HTTP 协议里，意思是多次执行相同的操作，结果也都是相同的，即多次“幂”后结果“相等”。<br>很显然，GET 和 HEAD 既是安全的也是幂等的，DELETE 可以多次删除同一个资源，效果都是“资源不存在”，所以也是幂等的。<br>POST 和 PUT 的幂等性质就略费解一点。<br>按照 RFC 里的语义，POST 是“新增或提交数据”，多次提交数据会创建多个资源，所以不是幂等的；而 PUT 是“替换或更新数据”，多次更新一个资源，资源还是会第一次更新的状态，所以是幂等的。<br>对比一下 SQL 来加深理解：把 POST 理解成 INSERT，把 PUT 理解成 UPDATE，这样就很清楚了。多次 INSERT 会添加多条记录，而多次 UPDATE 只操作一条记录，而且效果相同。]]></description><link>2-领域\后端学习\计算机与网络\网络部分基础.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/网络部分基础.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231205002700022.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231205002700022.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HTTP报文是什么样子的]]></title><description><![CDATA[ 
 <br><br>http协议与TCP/UDP类似，也是需要在实际传输的数据前面加上一些头数据，包含协议所必须的一些字段，比如发送方和接受方的端口号，包序号和标志位等，但是和TCP/UDP不同的是，HTTP是一个纯文本的协议，头部数据都是ASCII码的文本，很容易用肉眼阅读，其请求报文和响应报文基本相同，由三大块组成<br>​	<br>
<br>起始行（start line）(也就是请求行)：描述请求或响应的基本信息;
<br>头部字段集合（header）：使用 key-value 形式更详细地说明报文;
<br>.消息正文（entity）：实际传输的数据，它不一定是纯文本，可以是图片、视频等二进制数据。
<br><br>请求行由三部分组成，三个部分通常使用空格分隔，最后用CRLF表示换行结束<br>
<img alt="image-20231227230603762.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231227230603762.png" referrerpolicy="no-referrer"><br>
<br>请求方法
<br>请求目标
<br>版本号
<br>举个例子<br>GET / HTTP/1.1 
复制<br>GET”是请求方法，“/”是请求目标，“HTTP/1.1”是版本号<br><br>也是三部分<br>
<br>版本号
<br>状态码
<br>原因
<br><img alt="image-20231227230849528.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231227230849528.png" referrerpolicy="no-referrer"><br>HTTP/1.1 200 OK
复制<br>Http1.1就是版本号、200就是状态码、ok就是原因，成功就是OK<br>HTTP/1.1 404 Not Found
复制<br>这个后面就更详细的说明了原因 NOTFound<br><br><a data-tooltip-position="top" aria-label="^b6305c" data-href="#^b6305c" href="\#^b6305c" class="internal-link" target="_self" rel="noopener">请求行</a>或者<a data-tooltip-position="top" aria-label="状态行（响应报文的起始行）" data-href="#状态行（响应报文的起始行）" href="\#状态行（响应报文的起始行）" class="internal-link" target="_self" rel="noopener">状态行</a>，加上头部字段集合就构成了HTTP报文里面完整的请求头或者响应头<br>
<img alt="image-20231227231347322.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231227231347322.png" referrerpolicy="no-referrer"> <br>而头部字段，是key value的形式，key value之间用  ： 分隔，最后用CRLF表示换行结束<br>
<br>
头部字段的注意事项

<br>字段名不区分大小写
<br>字段名不允许出现空格，但是可以用  "-" ，但不能使用下划线
<br>字段名后面必须紧跟着  ":" ，不能有空格，但是 “：” 后面的字段值前可以有多个空格
<br>字段顺序是没有意义的，可以任意排列不影响
<br>字段原则上不允许重复，除非这个字段本身的语义允许，例如 set-Cookie


<br>
常用头字段 
首先是头字段基本可以分为四类

<br>通用字段，在请求头和响应头里面都能出现
<br>请求字段：仅能出现在请求头里面，
<br>响应字段：仅能出现在响应头里面
<br>实体字段：实际上属于通用字段，但是专门描述body的额外信息


<br>然后是常见的一些字段<br>
<br>HOST 字段；属于请求字段，只能出现在请求头里面，也是必须要求出现的字段，如果请求头里面没有这个字段，那这个就是一个错误的请求头,host字段告诉服务器这个请求应该由哪个主机来处理,当一台计算机上托管了很多虚拟主机的时候，服务端就需要host字段来判断使用哪个主机
<br>User-Agent字段：这个是 用来描述发起HTTP请求的客户端，比如Chrome、Safari，这个字段只能稍微判断一下，因为每个浏览器都可能说自己是Chrome但是实际上是爬虫
<br>。。。
]]></description><link>2-领域\后端学习\计算机与网络\http报文是什么样子的.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/HTTP报文是什么样子的.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231227230603762.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231227230603762.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HTTP传输大文件的方法]]></title><description><![CDATA[ 
 <br>
Http 可以传输很多种类的数据，不仅是文本也能传输数据，甚至是图片视频，但是有一点需要注意，如果文件很大，如何在有限的带宽下高效快捷地传输这些大文件？
<br>
<br>未完待续 ✅ 2024-01-25
<br>
HTTP 不仅能传文本，也能传输图片，音频，视频所以我们需要解决，如何在有限的带宽下，高效快捷的传输这些文件
<br><br>首先一个方法就是，压缩，我们常见的 zip gzip 等，实现过程就是浏览器在发送请求的时候，会在请求头里面加上"Accept-Encoding"头字段，告诉服务器浏览器支持的压缩类型（gzip, deflate 等），这样服务器就会选择一种压缩算法，放进“Content-Encoding”响应头里，再把原数据压缩后发给浏览器。<br>但是缺点是这种方式其实只对文本类型有较好的支持，而图片、音频视频等多媒体数据本身就已经是高度压缩的，再用 gzip 处理也不会变小（甚至还有可能会增大一点），所以它就失效了<br><br>如果不能压缩，那我们就可以采取分块的方法，分解成多个小块，把这些小块分批发给浏览器，浏览器收到后再组装复原<br>
这样做的意义是浏览器和服务器都不需要在内存里保存文件的全部，网络也不会被大文件长时间占用<br><br>在我们发送请求的时候，在响应报文里加上“Transfer-Encoding: chunked”，就是表示开始分块传输了<br>Note
这里要注意的是，Transfer-Encoding: chunked”和“Content-Length”这两个字段是互斥的，也就是说响应报文里这两个字段不能同时出现，一个响应报文的传输要么是长度已知，要么是长度未知（chunked）
<br><br>分块的规则相似于响应头<br>
<br>每个分块包含两个部分，长度头和数据块；
<br>长度头是以 CRLF（回车换行，即\r\n）结尾的一行明文，用 16 进制数字表示长度；
<br>数据块紧跟在长度头后，最后也用 CRLF 结尾，但数据不包含 CRLF；
<br>最后用一个长度为 0 的块表示结束，即“0\r\n\r\n”<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125000624.png" referrerpolicy="no-referrer"><br>
需要说明的是，正常来说，我们是看不到分块的编码的，因为浏览器在收到分块传输的数据后会自动按照规则去掉分块编码，重新组装出内容，所以想要看到服务器发出的原始报文形态就得用 Telnet 手工发送请求（或者用 Wireshark 抓包），Telnet 抓包的结果如下 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125000825.png" referrerpolicy="no-referrer"><br>
可以看到，f 是表示长度的十六进制数，下面则是数据，最后是 0 长度分块结束。<br>
但是分块传输并不能完全解决上 G 的超大文件的传输，我们还有问题需要解决
<br><br>范围请求的场景在于，<br>
比如，你在看当下正热播的某穿越剧，想跳过片头，直接看正片，或者有段剧情很无聊，想拖动进度条快进几分钟，这实际上是想获取一个大文件其中的片段数据，而分块传输并没有这个能力。<br>Http 为了满足这样的需求，提出了范围请求的概念，允许客户端在请求头里面使用专用的字段表示只获取文件的一部分，相当于化整为零<br>范围请求不是 Web 服务器必备的功能，可以实现也可以不实现，所以服务器必须在响应头里使用字段“Accept-Ranges: bytes”明确告知客户端：“我是支持范围请求的”。<br>如果不支持的话该怎么办呢？服务器可以发送“Accept-Ranges: none”，或者干脆不发送“Accept-Ranges”字段，这样客户端就认为服务器没有实现范围请求功能，只能老老实实地收发整块文件了<br><br>如果支持，浏览器需要怎么发送请求呢，我们就可以在请求头里加上一个字段<br>Content-Range: bytes=x-y
复制<br>注意记数规则，要注意 x、y 表示的是“偏移量”，范围必须从 0 计数，例如前 10 个字节表示为“0-9”，第二个 10 字节表示为“10-19”，而“0-10”实际上是前 11 个字节（和 java 里面的数组很像，都是从 0 开始）<br>Range 的格式也很灵活，起点 x 和终点 y 可以省略，能够很方便地表示正数或者倒数的范围。假设文件是 100 个字节，那么：<br>
<br>“0-”表示从文档起点到文档终点，相当于“0-99”，即整个文件；
<br>“10-”是从第 10 个字节开始到文档末尾，相当于“10-99”；
<br>“-1”是文档的最后一个字节，相当于“99-99”；
<br>“-10”是从文档末尾倒数 10 个字节，相当于“90-99”。
<br>下面是一些注意的地方<br>
<br>x, y 必须满足范围，不能越界，否则服务器会返回状态码 416，告诉你范围有误
<br>如果范围是正确的，那么服务器会返回状态码“206 Partial Content”，和 200 的意思差不多，但表示 body 只是原数据的一部分 
<br>服务器要添加一个响应头字段Content-Range，告诉片段的实际偏移量和资源的总大小，格式是“bytes x-y/length”，与 Range 头区别在没有“=”，范围后多了总长度。例如，对于“0-10”的范围请求，值就是“bytes 0-10/100”
<br>下面举个例子（摘选网络）<br>
例如下面的这个请求使用 Range 字段获取了文件的前 32 个字节：<br>GET /16-2 HTTP/1.1
Host: www.chrono.com
Range: bytes=0-31
复制<br>返回的数据是（去掉了几个无关字段）：<br>HTTP/1.1 206 Partial Content
Content-Length: 32
Accept-Ranges: bytes
Content-Range: bytes 0-31/96
 
// this is a plain text json doc
复制<br>不仅看视频的拖拽进度需要范围请求，常用的下载工具里的多段下载、断点续传也是基于它实现的，要点是：<br>
<br>先发个 HEAD，看服务器是否支持范围请求，同时获取文件的大小；
<br>开 N 个线程，每个线程使用 Range 字段划分出各自负责下载的片段，发请求传输数据；
<br>下载意外中断也不怕，不必重头再来一遍，只要根据上次的下载记录，用 Range 请求剩下的那一部分就可以了。
<br><br>范围请求其实不止能一次获取一个片段，还支持在 Range 头里使用多个 x-y，一次性获取多个片段数据<br>
这个只在响应的报文支持使用一种特殊的 MIME 类型：“multipart/byteranges”，表示报文的 body 是由多段字节序列组成的，并且还要用一个参数“boundary=xxx”给出段之间的分隔标记<br>这个多段数据的传输比较类似于分块传输，<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125003143.png" referrerpolicy="no-referrer"> 但是他是用的 boundary 来表示区分不同的片段，这里的 boundary 会在报文里面返回<br>
每一个分段必须以“- -boundary”开始（前面加两个“-”），之后要用“Content-Type”和“Content-Range”标记这段数据的类型和所在范围，然后就像普通的响应头一样以回车换行结束，再加上分段数据，最后用一个“- -boundary- -”（前后各有两个“-”）表示所有的分段结束。<br>例如<br>GET /16-2 HTTP/1.1
Host: www.chrono.com
Range: bytes=0-9, 20-29
复制<br>得到的就会是下面这样：<br>HTTP/1.1 206 Partial Content
Content-Type: multipart/byteranges; boundary=00000000001
Content-Length: 189
Connection: keep-alive
Accept-Ranges: bytes
 
 
--00000000001
Content-Type: text/plain
Content-Range: bytes 0-9/96
 
// this is
--00000000001
Content-Type: text/plain
Content-Range: bytes 20-29/96
 
ext json d
--00000000001--
复制<br>报文里的“- -00000000001”就是多段的分隔符，使用它客户端就可以很容易地区分出多段 Range 数据。<br><br>
<br>大文件的传输大概分为三种方式

<br>压缩--只针对文本表现较好，大部分作为兜底的，标志是请求加上 Accept-Encoding ，响应就相应的返回 Content-Encoding
<br>分块，分块只解决了传输的痛点，但是没有解决视频之类的请求，其标志是使用响应头字段“Transfer-Encoding: chunked”来表示，分块的格式是 16 进制长度头 + 数据块，还有就是和 contentLength 请求头互斥；
<br>范围请求，这个实现了视频的拖拽和断点续传（很多下载器也是这么搞的），标志是使用请求头字段“Range”和响应头字段“Content-Range”，响应状态码必须是 206

<br>然后范围请求还可以多端同时请求，多加一个 x-y 就行，返回的时候类似于分块的返回，就是分割变成了 boundary


<br>最后，这四种方式，完全可以混在一起，比如压缩后在范围请求、分块传输等


]]></description><link>2-领域\后端学习\计算机与网络\http传输大文件的方法.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/HTTP传输大文件的方法.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125000624.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125000624.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Http的实体数据]]></title><description><![CDATA[ 
 <br>在 TCP/IP 协议里，传输数据基本上是 header +body 的格式，但是由于其是<a data-tooltip-position="top" aria-label="网络部分基础 > ^a8ddaf" data-href="网络部分基础#^a8ddaf" href="\2-领域\后端学习\计算机与网络\网络部分基础.html#^a8ddaf" class="internal-link" target="_self" rel="noopener">传输层</a> 的协议，他不会关心 body 里面的数据是什么<br>
关于协议部分详见 <a data-href="网络部分基础#模型中每一层的作用" href="\2-领域\后端学习\计算机与网络\网络部分基础.html#模型中每一层的作用" class="internal-link" target="_self" rel="noopener">网络部分基础 &gt; 模型中每一层的作用</a> 目前的广泛的是 TCP/IP 模型的五层
<br>但是 HTTP 协议则不同，他是应用层的协议，数据传输到达以后，HTTP 就要负责告诉上层（他的上层是传输层）这到底是什么数据，这里就要引出 HHTP 是如何标记 body 的数据类型的了，叫做 <a data-href="#MIME type" href="\#MIME_type" class="internal-link" target="_self" rel="noopener">MIME type</a><br><br>MimeType 把数据分成了八大类，每个大类下再细分出多个子类，形式是“type/subtype”的字符串，常见的有<br>
1. Text：即文本格式的可读数据，我们最熟悉的应该就是 text/html 了，表示超文本文档，此外还有纯文本 text/plain、样式表 text/css 等。<br>
2. Image：即图像文件，有 image/gif、image/jpeg、image/png 等。<br>
3. Audio/video：音频和视频数据，例如 audio/mpeg、video/mp 4 等。<br>
4. Application：数据格式不固定，可能是文本也可能是二进制，必须由上层应用程序来解释。常见有 application/json，application/javascript、application/pdf 等，另外，如果实在是不知道数据是什么类型，像刚才说的“黑盒”，就会是 application/octet-stream，即不透明的二进制数据。<br>但是 mime type 是不够的，有时候，http 为了节约带宽，还会压缩数据，所以我们有时候还需要另一个解读的方式就是 Encoding type，告诉数据是怎么解压的，常见的有<br>
1. Gzip：GNU zip 压缩格式，也是互联网上最流行的压缩格式；<br>
2. Deflate：zlib（deflate）压缩格式，流行程度仅次于 gzip；<br>
3. Br：一种专门为 HTTP 优化的新压缩算法（Brotli）。<br><br>此外，HTTP 协议还定义了两个 Accept 请求头字段和两个 Content 字段，用于客户端和服务器进行“内容协商”。也就是说，客户端用 Accept 头告诉服务器希望接收什么样的数据，而服务器用 Content 头告诉客户端实际发送了什么样的数据。<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240108223955.png" referrerpolicy="no-referrer"><br>
客户端希望返回的数据....... 以及服务器希望返回的数据 text/html encoding 是 gzip
<br>就比如我工作中就有遇到过，为了实现 tomcat 的过滤，需要把 vue 页面的 index. Html 变成 index. Vhtml 那这个时候浏览器就认不得了（实际上搜狐浏览器能通过 html 文件头判断出来他是 html，但是 chrome 不行）就要在返回的请求头上加上 Content-type 也是通过 tomcat 的 web. Xml 做到的<br><br>实际上这个就是国际化的问题<br>
所谓的“语言类型”就是人类使用的自然语言，例如英语、汉语、日语等，而这些自然语言可能还有下属的地区性方言，所以在需要明确区分的时候也要使用“type-subtype”的形式，不过这里的格式与数据类型不同，分隔符不是“/”，而是“-”。<br>举几个例子：en 表示任意的英语，en-US 表示美式英语，en-GB 表示英式英语，而 zh-CN 就表示我们最常使用的汉语。<br>关于自然语言的计算机处理还有一个更麻烦的东西叫做“字符集”。<br>在计算机发展的早期，各个国家和地区的人们“各自为政”，发明了许多字符编码方式来处理文字，比如英语世界用的 ASCII、汉语世界用的 GBK、BIG 5，日语世界用的 Shift_JIS 等。同样的一段文字，用一种编码显示正常，换另一种编码后可能就会变得一团糟。<br>所以后来就出现了 Unicode 和 UTF-8，把世界上所有的语言都容纳在一种编码方案里，UTF-8 也成为了互联网上的标准字符集。<br>
下面是一些例子<br>Accept-Language: zh-CN, zh, en
复制<br>Accept-Charset: gbk, utf-8 
Content-Type: text/html; charset=utf-8
复制<br><br>Q 的作用就是设置权重，来设置优先级<br>
权重的最大值是 1，最小值是 0.01，默认值是 1，如果值是 0 就表示拒绝。具体的形式是在数据类型或语言代码后面加一个“;”，然后是“q=value”。<br>比如<br>Accept: text/html,application/xml;q=0.9,*/*;q=0.8
复制<br>它表示浏览器最希望使用的是 HTML 文件，权重是 1，其次是 XML 文件，权重是 0.9，最后是任意数据类型，权重是 0.8。服务器收到请求头后，就会计算权重，再根据自己的实际情况优先输出 HTML 或者 XML。<br>
这里要注意的是 在 HTTP 中 ";"  号的意义是要小于 “,” 的<br><br>内容协商的过程和算法是不透明的，每个服务器都不一样，但是有时候，服务器会在响应头里面加上一个 Vary 字段用于记录协商的过程<br>Vary: Accept-Encoding,User-Agent,Accept
复制<br>这个 Vary 字段表示服务器依据了 Accept-Encoding、User-Agent 和 Accept 这三个头字段，然后决定了发回的响应报文。<br><br>
<br>数据类型表示实体数据的内容是什么，使用的是 MIME type，相关的头字段是 Accept 和 Content-Type；
<br>数据编码表示实体数据的压缩方式，相关的头字段是 Accept-Encoding 和 Content-Encoding；
<br>语言类型表示实体数据的自然语言，相关的头字段是 Accept-Language 和 Content-Language；
<br>字符集表示实体数据的编码方式，相关的头字段是 Accept-Charset 和 Content-Type；
<br>客户端需要在请求头里使用 Accept 等头字段与服务器进行“内容协商”，要求服务器返回最合适的数据；
<br>Accept 等头字段可以用“,”顺序列出多个可能的选项，还可以用“; q=”参数来精确指定权重。
]]></description><link>2-领域\后端学习\计算机与网络\http的实体数据.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/Http的实体数据.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240108223955.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240108223955.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[http的网址]]></title><description><![CDATA[ 
 <br>
要搞懂HTTP甚至网络应用，就必须搞定URI
<br><a data-tooltip-position="top" aria-label="https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E9%80%8F%E8%A7%86HTTP%E5%8D%8F%E8%AE%AE/11%20%20%E4%BD%A0%E8%83%BD%E5%86%99%E5%87%BA%E6%AD%A3%E7%A1%AE%E7%9A%84%E7%BD%91%E5%9D%80%E5%90%97%EF%BC%9F.md" rel="noopener" class="external-link" href="https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E9%80%8F%E8%A7%86HTTP%E5%8D%8F%E8%AE%AE/11%20%20%E4%BD%A0%E8%83%BD%E5%86%99%E5%87%BA%E6%AD%A3%E7%A1%AE%E7%9A%84%E7%BD%91%E5%9D%80%E5%90%97%EF%BC%9F.md" target="_blank">http学习地址</a><br><br>URI本质是一个字符串，用来唯一的标记资源的位置或者名字<br>他实际上不止能标记万维网的资源，也可以表示其他的，如邮件系统、本地文件系统等任意资源。而“资源”既可以是存在磁盘上的静态文本、页面数据，也可以是由 Java、PHP 提供的动态服务。<br><br><img alt="Pasted image 20231230011324" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/Pasted%20image%2020231230011324.png" referrerpolicy="no-referrer"><br>
scheme 之后，必须是三个特定的字符“://”，它把 scheme 和后面的部分分离开。<br>完整形态的URI<br>
<img alt="Pasted image 20231230012218" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/Pasted%20image%2020231230012218.png" referrerpolicy="no-referrer"><br>
‘#’ 表示一个锚点，或者是说标签，浏览器在获取到资源以后直接跳转到他指定的位置，但是片段标识符服务器是看不到的，就是说浏览器是不会把‘#fragment’发送给服务器，也就是说这是客户端的处理<br>我理解vue的路由应该也就是这个原理，router实际上就是标记了对应的资源信息<br><br>URI里面只能用ASCII码，如果要在URI里面使用英语之外的语言，就需要转义<br>
URI 转义的规则有点“简单粗暴”，直接把非 ASCII 码或特殊字符转换成十六进制字节值，然后前面再加上一个“%”。<br>例如，空格被转义成“%20”，“?”被转义成“%3F”。而中文、日文等则通常使用 UTF-8 编码后再转义，例如“银河”会被转义成“%E9%93%B6%E6%B2%B3”。<br><br>
<br>URI是 唯一标识资源的一个字符串
<br>URI的组成
<br>‘#’标记是客户端进行操作，不是服务端进行操作
<br>转义
]]></description><link>2-领域\后端学习\计算机与网络\http的网址.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/http的网址.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/Pasted%20image%2020231230011324.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/Pasted%20image%2020231230011324.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HTTP重定向和跳转]]></title><description><![CDATA[ 
 <br><br>关于重定向，有两个（常用）状态码要关注 ：<br>
301 是“永久重定向”，302 是“临时重定向”，浏览器收到这两个状态码就会跳转到新的 URI。<br>重定向的过程实际是发送了两个请求，比如第一个请求发送了返回 302，第二个请求就会使用 302 跳转到对应的页面<br>下面是某 302 的返回报文 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240203233621.png" referrerpolicy="no-referrer"><br>
也就是说，多了一个请求头 Location <br>Location”字段属于响应字段，必须出现在响应报文里。但只有配合&nbsp;301⁄302&nbsp;状态码才有意义，它标记了服务器要求重定向的 URI，这里就是要求浏览器跳转到“index.html”。<br>
浏览器收到&nbsp;301⁄302&nbsp;报文，会检查响应头里有没有“Location”。如果有，就从字段值里提取出 URI，发出新的 HTTP 请求，相当于自动替我们点击了这个链接。<br>在“Location”里的 URI 既可以使用绝对 URI<br>
所谓“绝对 URI”，就是完整形式的 URI，包括 scheme、host: port、path 等<br>
也可以使用相对 URI<br>
“相对 URI”，就是省略了 scheme 和 host: port，只有 path 和 query 部分，是不完整的，但可以从请求上下文里计算得到<br>Note
如果只是站内跳转是可以用相对 URI 的，但是如果要跳到别的地方就必须绝对 URI，否则浏览器就会当相对 URI 理解
<br><br>
<br>301俗称“永久重定向”（Moved Permanently），意思是原 URI 已经“永久”性地不存在了，今后的所有请求都必须改用新的 URI，浏览器看到 301，就知道原来的 URI“过时”了，就会做适当的优化。比如历史记录、更新书签，下次可能就会直接用新的 URI 访问，省去了再次跳转的成本。搜索引擎的爬虫看到 301，也会更新索引库，不再使用老的 URI。
<br>302俗称“临时重定向”（“Moved Temporarily”），意思是原 URI 处于“临时维护”状态，新的 URI 是起“顶包”作用的“临时工”。浏览器或者爬虫看到 302，会认为原来的 URI 仍然有效，但暂时不可用，所以只会执行简单的跳转页面，不记录新的 URI，也不会有其他的多余动作，下次访问还是用原 URI
<br>303 See Other：类似 302，但要求重定向后的请求改为 GET 方法，访问一个结果页面，避免 POST/PUT 重复操作；
<br>307 Temporary Redirect：类似 302，但重定向后请求里的方法和实体不允许变动，含义比 302 更明确；
<br>308 Permanent Redirect：类似 307，不允许重定向后的请求变动，但它是 301“永久重定向”的含义。
<br><br>
<br>资源不可用，服务端可以使用这个消息头来告知
<br>增加访问入口，比如申请多个入口，然后转到主站，比如访问 qq. com 就会跳转到 qq 的主站 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240203234710.png" referrerpolicy="no-referrer">
<br><br>
<br>性能问题，毕竟是发送两次请求，所以性能多少有影响
<br>循环跳转，A-&gt;B  B-&gt;C C-&gt;A 这种无限循环，如果重定向的策略设置欠考虑，可能会出现“A=&gt;B=&gt;C=&gt;A”的无限循环，不停地在这个链路里转圈圈，所以 HTTP 协议特别规定，浏览器必须具有检测“循环跳转”的能力，在发现这种情况时应当停止发送请求并给出错误提示
]]></description><link>2-领域\后端学习\计算机与网络\http重定向和跳转.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/HTTP重定向和跳转.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240203233621.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240203233621.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HTTP状态码]]></title><description><![CDATA[<a class="tag" href="?query=tag:计算机网络" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#计算机网络</a> 
 <br><a href=".?query=tag:计算机网络" class="tag" target="_blank" rel="noopener">#计算机网络</a> <br><br><a data-href="HTTP报文是什么样子的" href="\2-领域\后端学习\计算机与网络\http报文是什么样子的.html" class="internal-link" target="_self" rel="noopener">HTTP报文是什么样子的</a><br>
由前面的可以知道，响应报文由响应头加响应数据组成，响应头又由状态行（或者请求行）和头字段构成<a data-href="HTTP报文是什么样子的#^5b6ad0" href="\2-领域\后端学习\计算机与网络\http报文是什么样子的.html#^5b6ad0" class="internal-link" target="_self" rel="noopener">HTTP报文是什么样子的 &gt; ^5b6ad0</a><br>
由<a data-tooltip-position="top" aria-label="HTTP报文是什么样子的 > 状态行（响应报文的起始行）" data-href="HTTP报文是什么样子的#状态行（响应报文的起始行）" href="\2-领域\后端学习\计算机与网络\http报文是什么样子的.html#状态行（响应报文的起始行）" class="internal-link" target="_self" rel="noopener">状态行</a>可以看到里面的结构有三大部分<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106225950.png" referrerpolicy="no-referrer"><br>
<br>
开头的 Version 部分是 HTTP 协议的版本号，通常是 HTTP/1.1，用处不是很大。

<br>
后面的 Reason 部分是原因短语，是状态码的简短文字描述，例如“OK”“Not Found”等等，也可以自定义。但它只是为了兼容早期的文本客户端而存在，提供的信息很有限，目前的大多数客户端都会忽略它。

<br>
所以，状态行里有用的就只剩下中间的状态码（Status Code）了。它是一个十进制数字，以代码的形式表示服务器对请求的处理结果，就像我们通常编写程序时函数返回的错误码一样。不过你要注意，它的名字是“状态码”而不是“错误码”。也就是说，它的含义不仅是错误，更重要的意义在于表达 HTTP 数据处理的“状态”，客户端可以依据代码适时转换处理状态，例如继续发送请求、切换协议，重定向跳转等，有那么点 TCP 状态转换的意思。

<br><br>标准状态码是三位数，取值从000到999<br>RFC把状态码分为了五类<br>
- 1××：提示信息，表示目前是协议处理的中间状态，还需要后续的操作；<br>
- 2××：成功，报文已经收到并被正确处理；<br>
- 3××：重定向，资源位置发生变动，需要客户端重新发送请求；<br>
- 4××：客户端错误，请求报文有误，服务器无法处理；<br>
- 5××：服务器错误，服务器在处理请求时内部发生了错误。<br>而错误码的作用就是对客户端和服务端的通信<br>
客户端作为请求方 ，能通过状态码知道请求的状态，从而判断下一步该怎么做<br>
服务方作为接收方，处理请求的时候要尽量清晰的返回正确的状态码，帮助客户端判断下一步动作]]></description><link>2-领域\后端学习\计算机与网络\http状态码.html</link><guid isPermaLink="false">2-领域/后端学习/计算机与网络/HTTP状态码.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106225950.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106225950.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[哈希相关]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://dwjns0knc7o.feishu.cn/record/F4NFrzCngeZzsPc83O4cYME5nlc" rel="noopener" class="external-link" href="https://dwjns0knc7o.feishu.cn/record/F4NFrzCngeZzsPc83O4cYME5nlc" target="_blank">有效的字母异位词</a><br>
<br>遇到 hash 的相关思路就是数组、set、map来解决，

<br>范围可控用数组，
<br>数值比较大的话，用 set，
<br>有出现 key 对应 value 的话，就用 map


<br>值的连续是可以用一起加或者一起减某个数保证顺序
<br><br><a data-tooltip-position="top" aria-label="https://dwjns0knc7o.feishu.cn/record/R32FrWdyyet6UhcfzmucueESnVc" rel="noopener" class="external-link" href="https://dwjns0knc7o.feishu.cn/record/R32FrWdyyet6UhcfzmucueESnVc" target="_blank">两个数组的交集</a><br>
<br>红黑树是什么? 📅 2024-02-22 ✅ 2024-02-22
<br>
<br>可以用 set 解决，也可以用数组解决，主要就是在于数据的多少，基本上，如果数据有极限，那么基本都能用数组解决，1000，10000 之类的，复杂度都是 o(1)
<br>遇到映射基本思路就是往 hash 靠
<br>
<br>set 写法，需要去重两遍，我理解是节约时间了，如果数据非常大，一个去重，能减少非常多的第二遍遍历
<br>//leetcode submit region begin(Prohibit modification and deletion)
class Solution {
    // stream流
    //public int[] intersection(int[] nums1, int[] nums2) {
    //    return Arrays.stream(nums1).filter(Arrays.asList(nums2)::contains).toArray();
    //}

    // set方式
    public int[] intersection(int[] nums1, int[] nums2) {
        HashSet&lt;Integer&gt; integerHashSet = new HashSet&lt;&gt;();
        HashSet&lt;Integer&gt; integers = new HashSet&lt;&gt;();
        // 将数组转为set
        // 题意要求结果不重复，就可以这样写
        for (int i:nums1) integerHashSet.add(i);
        for (int i:nums2) {
            if(integerHashSet.contains(i)){
                integers.add(i);
            }
        }
        return integers.stream().mapToInt(Integer::valueOf).toArray();
    }
}
//leetcode submit region end(Prohibit modification and deletion)
复制<br>
<br>数组写法则是由于，题意给出了数据大小要小于 1000 ，那么我们就可以先创建一个大于 1000 的数组（hash），然后每个值都是 0（初始值），然后遍历第一个数组，如果映射上了，就加一，就是给 nums1 的每个值映射上 1，然后遍历 nums2 的时候，判断这个 hash 数组里面是否包含 nums2 的元素（hash[i] == 1?）, 最后再遍历一遍数组，就能判断交集了（用 set 记录交集，去个重）
<br><br><a data-tooltip-position="top" aria-label="https://dwjns0knc7o.feishu.cn/record/KuBdrAUG5eWoOUcH05rcvSfOncf" rel="noopener" class="external-link" href="https://dwjns0knc7o.feishu.cn/record/KuBdrAUG5eWoOUcH05rcvSfOncf" target="_blank">两数之和</a><br>
<br>算法未完待续 📅 2024-03-12 ✅ 2024-03-12<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312231119.png" referrerpolicy="no-referrer">
<br>
<br>判断想要的数是否出现在之前的遍历里面，那么可以想到使用 hash 法
<br>采用 map 的方式，key 为值，value 为下标，这是需要思考的，因为我们要查找的应该是数据，通过数据来找到对应的下标，所以可以定下来是以值为 key 下标为value
<br><br><a data-tooltip-position="top" aria-label="https://dwjns0knc7o.feishu.cn/record/V79jrjPcreJS4tcxbkuc8goEnRc" rel="noopener" class="external-link" href="https://dwjns0knc7o.feishu.cn/record/V79jrjPcreJS4tcxbkuc8goEnRc" target="_blank">四数相加</a><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314231015.png" referrerpolicy="no-referrer"><br>
思路<br>
<br>最开始的暴力想法是遍历四个数组强行加，但是这样时间复杂度是 n^4 ，所以可以考虑遍历俩俩一组，遍历两组，前面一组数字的负数是否在后面一组出现过，这样两个 n^2 时间复杂度就是 n^2
<br>由于不需要遍历，可以考虑 map 的方式，key 是每一组出来的值，value 是出现的次数，当第二组出现前面一组的负数的时候，count 加上对应的 vlaue 值
<br>正常解题思路，这里要注意的是 value++ 和++value 是不一样的，put 进 map 的时候会报错<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314234353.png" referrerpolicy="no-referrer"><br>
看优化的解题思路，这里采用了 foreach，然后用到了 map 中的 getOrDefault, 更优雅<br>public int fourSumCount(int[] nums1, int[] nums2, int[] nums3, int[] nums4) {
        HashMap&lt;Integer, Integer&gt; countAB = new HashMap&lt;&gt;();
        for(int u:nums1){
            for(int v:nums2){
                // 计算出所有的AB相加的可能，并且记录出现的次数
                countAB.put(u+v,countAB.getOrDefault(u+v,0)+1);
            }
        }
        int ans = 0;
        for(int u:nums3){
            for(int v:nums4){
                if (countAB.containsKey(-u - v)) {
                    ans += countAB.get(-u - v);
                }
            }
        }
        return ans;
    }
复制]]></description><link>2-领域\后端学习\算法\哈希相关.html</link><guid isPermaLink="false">2-领域/后端学习/算法/哈希相关.md</guid><pubDate>Thu, 14 Mar 2024 15:46:25 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312231119.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312231119.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[上环境的一些命令]]></title><description><![CDATA[ 
 <br><br><br>
<br>首先是安装 nginx 这个可以参考这篇文章 <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/425790769" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/425790769" target="_blank">安装nginx 虚拟机版本</a>
<br>然后需要修改安装好后的 nginx 里面的 conf 目录下的 nginx. config 文件，两个地方，一个要改上面的用户，变成 root 不是 nobody，第二个要配置里面的 root 路径为你前端项目 dist 里面的地址，下面 html 不用变
<br>记得修改完配置刷新一下 nginx 的配置<br>nginx -s reload
复制<br><br>
<br>
先安装 java。然后安装 git maven 

<br>
git 拉取代码，然后 mvn package -DskipTests 打包

<br>
 然后讲踩坑的点，安装 java 的时候发现自己的 jdk 版本是 21 所以需要自己去下载 tar. gz 的包，安装好之后配置一下，参考这个文章<a data-tooltip-position="top" aria-label="https://blog.csdn.net/napoay/article/details/79864770" rel="noopener" class="external-link" href="https://blog.csdn.net/napoay/article/details/79864770" target="_blank">解决CentOS默认JDK无法替换问题_虚拟机中安装jdk不能替代本来的-CSDN博客</a>

<br>
 虚拟机运行这个 jar 包的时候，原本的命令类似

<br> java -jar user-center-0.0.1-SNAPSHOT.jar --spring.profiles.active=prod
复制<br>但是你可以后台运行命令就变成这样<br>nohup java -jar user-center-0.0.1-SNAPSHOT.jar --spring.profiles.active=prod &amp;
复制<br>运行后就在后台运行了<br>一些有意思的命令<br>#给文件加所有人权限 a是所有人，x是执行权限
chmod a+x user-center-0.0.1-SNAPSHOT.jar 
# 查看后台进程
ps -ef |grep xxx
# 删除进程
kill -9 [pid]
# 看文件大小，用kb mb表示
ls -alh
# 查找文件
whereis xxx
# 看端口号
netstat -ntlp
# 看历史命令
history
复制<br><br>主要要看 dockerFile 这个文件，然后 docker build 命令打镜像包<br>启动：docker run<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240420141404.png" referrerpolicy="no-referrer">]]></description><link>2-领域\后端学习\虚机\上环境的一些命令.html</link><guid isPermaLink="false">2-领域/后端学习/虚机/上环境的一些命令.md</guid><pubDate>Sat, 20 Apr 2024 14:01:17 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240420141404.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240420141404.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[红黑树是什么？]]></title><description><![CDATA[ 
 <br><br>红黑树本质其实也是对概念模型 : 2-3-4 树的一种实现，而 2-3-4 树是阶数为 4 的B 树 <br>
<br>没写完到时候看这个<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/273829162" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/273829162" target="_blank">图解：什么是红黑树？ - 知乎</a>
]]></description><link>2-领域\后端学习\java\红黑树是什么？.html</link><guid isPermaLink="false">2-领域/后端学习/Java/红黑树是什么？.md</guid><pubDate>Tue, 12 Mar 2024 14:08:51 GMT</pubDate></item><item><title><![CDATA[平衡多路查找树（B树）]]></title><description><![CDATA[ 
 <br><br>首先介绍二分法，二分法是我们常用的一种查找算法，具体实现思路如下<br>
<br> 首先对数据集进行排序。
<br> 找到数据集中间位置的节点。
<br> 用查找的条件和中间节点进行比较，等于则直接返回，中间节点数据小于查找条件则说明数据在排序列表的左边，大于则说明数据在排序列表的右边。
<br>也就是说，如果我们能保证数据的有序性，并且预先把数据分段，然后把数据的中间节点储存好，那么查找的时候就会更简单，所以我们演化出了跳表  <a data-href="Redis 跳表" href="\2-领域\后端学习\redis\知识点\redis-跳表.html" class="internal-link" target="_self" rel="noopener">Redis 跳表</a> 和树的结构<br>下面就是树的结构<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220233103.png" referrerpolicy="no-referrer"><br>那么这个就是二叉树<br><br>我们构建二叉树需要怎样的操作呢
平衡算法<br>
相同的数据插入的先后顺序不一样就可能变成线性结构，如下 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220233502.png" referrerpolicy="no-referrer"> 所以我们必须要有一种方式来保证二叉树节点的平衡，让树的节点高度差不会太大，这个时候就衍生了一些平衡算法，最终我们的二叉树就有像 AVL 树和红黑树这些新产品，我们也称这些新产品为平衡二叉树，，平衡二叉树通常会保证树的左右两边的节点层级相差不会大于2。
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220234020.png" referrerpolicy="no-referrer"><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220234031.png" referrerpolicy="no-referrer"><br><br>当二叉树节点分布不均匀的时候，会极大影响数据查询的性能，为了保证数据的均衡性，就有了平衡二叉树的结构，平衡算法暂不赘述，有时间看看<br>
<br> 平衡算法可以研究一下<br>
这里讲一下构建规则
<br><br>三大特点<br>
<br> 非叶子节点只能允许最多两个子节点存在
<br> 每一个节点左边子节点值小于当前节点，右边值大（算法算出来的值）
<br> 通过平衡算法（比如 Treap、AVL、红黑树）保证左右节点的高度相差不超过 2 层
<br><br>
<br>没写完到时候看这个 <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/27700617" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/27700617" target="_blank">平衡二叉树、B树、B+树、B*树 理解其中一种你就都明白了 - 知乎</a> 📅 2024-02-22 ✅ 2024-02-22<br>
B 树和平衡二叉树的不同之处是：B 树属于多叉树又名平衡多路查找树（查找路径不止两个），数据库索引技术里大量使用着 B 树和 B+树的数据结构。
<br> 注意 B 树和 B-tree 是同一种树
<br><br>有四大特点，说实话没太明白<br>
<br>排序方式：所有关键字是按递增次序排列，左小右大
<br>子节点数：非叶节点（根节点和枝节点）的子节点数 &gt;1、且子节点数&lt;=M、且M&gt;=2，空树除外（注：M 阶代表一个树节点最多有多少个查找路径，M=M 路,当 M=2则是2叉树,M=3则是3叉）
<br>关键字数：枝节点的关键字数量大于等于 ceil(m/2)-1个且小于等于 M-1（注：ceil()是个朝正无穷方向取整的函数如 ceil(1.1)结果为 2);关键字就是指的是一个节点里面的'值'的数量，比如下面的图，M 是一个关键字，他有两个子树，DG 是两个关键字，下面有三个子树，每一层加 1
<br>所有叶子节点均在同一层、叶子节点除了包含了关键字 和 关键字记录的指针外，也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子;
<br>上面比较难理解，下图可以看 B 树，通过他的插入和查询可以很好的理解 b 树 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222222707.png" referrerpolicy="no-referrer"><br><br>他的查找过程如下<br>
如上图我要从上图中找到E字母，查找流程如下<br>
<br>获取根节点的关键字进行比较，当前根节点关键字为 M，E&lt;M（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）；
<br>拿到关键字 D 和 G，D&lt;E&lt;G 所以直接找到 D 和 G 中间的节点；
<br>拿到 E 和 F，因为 E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回 null）；
<br><br>
<br> 定义一个5阶树（平衡5路查找树;），现在我们要把3、8、31、11、23、29、50、28、53 这些数字构建出一个5阶树出来

<br>节点拆分规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须&lt;=5-1（这里关键字数&gt;4就要进行节点拆分）
<br>排序规则：满足节点本身比左边节点大，比右边节点小的排序规则
<br> 首先插入 3、8、31、11<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222223559.png" referrerpolicy="no-referrer">
<br> 然后插入23、29<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222223641.png" referrerpolicy="no-referrer">
<br> 最后插入 50 28 53<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222223707.png" referrerpolicy="no-referrer">


<br><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222224333.png" referrerpolicy="no-referrer"><br>
<br>节点合并规则：当前是要组成一个5路查找树，那么此时 m=5,关键字数必须大于等于 ceil(m/2)-1（所以这里关键字数&lt;2就要进行节点合并）。
<br>满足节点本身比左边节点大，比右边节点小的排序规则。
<br>关键字数小于二时先从子节点取，子节点没有符合条件时就向父节点取，取中间值往父节点放。
<br><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222225201.png" referrerpolicy="no-referrer"><br>
上图数字 1、3、7是叶子节点；（因为他们下面没有分叉出子节点，所以称为：叶子节点）【度为0】<br>
数字2、8是子节点； （除了根节点、叶子节点之外的，都称为：子节点）【度为1】<br>
数字5是根节点；（因为他是最顶部，所以称为：根节点）【度为2】<br>关键字 就理解为数据就行了<br><br>平衡二叉树和 b 树都是一种数据结构，他们的特点是平衡，这种结构能让人更快的查询到需要的数据，两者的查询使我们能够在最坏情况下也保持 O(LogN)的时间复杂度实现查找，两者也有区别<br>平衡二叉树：<br>
<br>平衡二叉树从二叉树过来，因为二叉树的规则，左小右大不足以控制二叉树的高度，可能会出现二叉树变成一条线性结构的情况出现，所以加上了平衡算法以及一些规则补齐了这些，规则包括

<br> 非叶子节点只能允许最多两个子节点存在，就是说，分叉最多分俩，但是最下面的叶子节点除外
<br> 左小右大，值是根据 hash 算法算出来的
<br> 平衡算法，这个不太清楚，但是这个保证了树左右的高度差不超过 2 层，不会左边一大堆，右边只有一层


<br>B 树：<br>
B 树看上去跟平衡二叉树没什么大区别，但是他有几个比较大的不同<br>
<br>他不限制子节点数一定为 2，

<br> 子节点数：非叶节点（根节点和枝节点）的子节点数 &gt;1、且子节点数量&lt;=M 、且M&gt;=2，空树除外（注：M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉）；


<br>他的所有叶子节点均在一层
<br>关键字的设定，关键字就是一个节点里面存的那个值，一个结点中包含多个关键字（如n个关键字），那么它就对应有n+1个孩子结点
<br> 他的优势在于相对平衡二叉树在节点空间的利用率上进行改进，B 树在每个节点保存更多的数据，减少了树的高度，从而提升了查找的性能
<br><br>B+ 树是在 B 树的基础上又一次的改进，提升了两个方面<br>
<br>查询的稳定性
<br>数据排序方面更加友好
<br><br>
<br>B+ 树的非叶子节点不保存具体的数据，只保存关键字的索引，所有的数据都保存到叶子节点，所以每次查询的次数都会是一样的，查询速度就比较稳定，而 B 树在查找的过程中，不同关键字查找的速度都不同，有的可能在根节点，有的可能在叶子节点
<br>B+树的叶子节点有序排列，左边结尾数据会保存右边节点开始数据的指针
<br>其非叶子节点的子节点数=关键字数
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303165808.png" referrerpolicy="no-referrer"><br><br>
<br>B+树查询速度更稳定
<br>B+树天然具备排序功能，B+树所有叶子节点数据构成了一个有序链表，查询大小区间数据的时候更加方便
<br>B+树遍历速度更快，它只需要遍历所有的叶子节点就行，不需要对每一层进行遍历
]]></description><link>2-领域\后端学习\java\平衡多路查找树（b树）.html</link><guid isPermaLink="false">2-领域/后端学习/Java/平衡多路查找树（B树）.md</guid><pubDate>Sun, 03 Mar 2024 10:01:41 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220233103.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240220233103.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[JAVA基础巩固]]></title><description><![CDATA[ 
 <br><br><br>
<br>javaSE和javaEE是基础版和企业开发版本的区别，javaEE更适合开发企业级应用或web<br>

<br>java9开始就不需要区分JRE和JDK的关系了<br>

<br>java从源码到运行的过程：

<br>.java-&gt; javac编译-&gt;.class-&gt;解释器&amp;JIT-》机器能理解的代码<br>



<br>java和C++的对比

<br>java不提供指针访问内存，所以内存更安全<br>

<br>java类是单继承的，接口可以多继承，但是C++支持多继承<br>

<br>C++支持操作符重载和方法重载，java只支持方法重载<br>



<br>移位运算符

<br>x&gt;&gt;1 // 右移运算相当于除2，数字是几就是除2的几次方<br>

<br>x&lt;&lt;1 //左移，相当于×2，同上<br>

<br>由于左移位数大于等于 32 位操作时，会先求余（%）后再进行左移操作，所以代码左移 42 位相当于左移 10 位（42%32=10）<br>



<br>八种基本数据类型 byte short int long float double boolean char<br>

<br>包装类型的 缓存机制--就是valueOf方法有缓存机制

<br>Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。<br>

<br>两个浮点类型的包装类没有实现缓存机制<br>



<br>应避免频繁的拆装箱<br>

<br>成员变量如果没有赋初始值，那么会自动以类型的默认值而赋值，但是成员变量没有（会编译时报错）<br>

<br>静态对象只会被赋一次内存,即使创建多个对象<br>

<br>char在java中占两个字节<br>

<br>构造方法不能被重写，但是可以被重载（参数不同）<br>

<br>面对对象三大特征 封装 继承 多态（多态表现为父类引用指向子类实例）<br>

<br>BigDecimal方法应该使用compareTo方法，因为bigDecimal的equals方法还会比较精度<br>

<br><br>
<br>stringBuilder和StringBuffer都是继承自abstractStringBuilder方法，采用字符数组保存字符串，并提供很多修改字符串的方法<br>

<br>String中的对象是不可变的，可以理解为常量，就是线程安全的，每次对String对象进行改变的时候都会生成一个新的String对象，然后指针指向新的对象并改变引用<br>

<br>StringBuffer对字符的操作加了同步锁或者对方法加了同步锁，所以是线程安全的<br>

<br>StringBuilder没有对方法加同步锁，所以是不安全的<br>

<br>每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险<br>

<br>String类型被final修饰，且是私有的，没有对外暴露方法，而且被final修饰不能被继承<br>

<br>+和+=是专门为String类重载过的运算符，字符串的+和+=实际上是通过StringBuilder的append方法<br>

<br>不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。

<br>

java9以后这个问题得到了解决，也就是意味着可以放心使用+进行拼接了




<br>String.intern(),就是返回这个对象在常量池里的引用？新建一个对象：常量池里的字符串<br>

<br>String是可以用== 来判断的，

<br>String str = "string";<br>
String newStr = str.intern();<br>
// true<br>
system.out.println(str == newStr)  
   &nbsp; &nbsp;<br>



<br>在编译期间可以确定的字符串，jvm会把他放到常量池中（在堆中）--常量折叠

<br>常量折叠不止能用在字符串，基本数据类型都有这种，字符串的加减乘除，以及string的+<br>

<br>平时写代码的时候，避免创建多个字符串对象的拼接，因为这样会重新创建对象，可以使用StringBuilder或者StringBuffer 不过，字符串用了final修饰之后，可以让编译器当作常量来处理，那么编译器在编译期间就能确定值，就也会对其优化，进行常量折叠<br>



<br><br>
<br>两个重要的Throwable子类：Exception 和 Error，简单来说就是可处理异常和无法处理的错误<br>

<br>受检异常和非受检异常（一个不catch不能通过编译，一个能编译）

<br>runtimeException 都称之为非受检异常

<br>空指针 NPE<br>

<br>数组越界<br>

<br>非法入参<br>

<br>类型转换错误<br>

<br>算术错误，0/1<br>

<br>NumberFormat<br>

<br>....<br>





<br>finally块中不要有return，否则会忽略掉try块的return，try或者catch块中有return语句的时候，return的语句会被暂存在一个本地变量中，然后执行finally块，然后返回return ，但是当finally中有return时，那就会把本地变量变成finally中的return了<br>

<br>finally块中的代码未必会执行

<br>执行过程中虚拟机直接终止<br>

<br>程序所在线程死亡<br>

<br>关闭CPU<br>



<br>每次抛异常要new一个对象抛出，不然如果定义了静态变量的话会导致异常栈信息错乱<br>

<br><br>
<br>泛型有三种

<br>泛型类<br>

<br>泛型接口<br>

<br>泛型方法<br>



<br>泛型不能用在异常处理的catch中，因为异常处理是运行时的JVM处理的，无法区别MyException和MyException的<br>

<br><br>反射是框架的灵魂，框架中都会有大量的反射机制，动态代理也依赖反射<br>通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行<br><br>注解的本质是一个实现了annotation的特殊接口<br><br><br>广义上他们都属于接口，SPI更像给开发者的接口，API则是给调用者的接口<br><img alt="img" src="\lib\media\1ebd1df862c34880bc26b9d494535b3dtplv-k3u1fbpfcp-watermark.png" referrerpolicy="no-referrer"><br><br>什么场景需要序列化和反序列化？<br>如果需要持久化Java对象，比如将对象保存在文件中或者网络传输java对象，都需要序列化<br><br>简单理解就是给你的对象穿装备，目的是拓展对象的功能<br>
<br>静态代理<br>

<br>动态代理 重点（尤其在框架中用的多）

<br>JDK动态代理机制：

核心是InvocationHandler接口和Proxy类



  创建一个代理之后要实现一个handler类，用proxy传建的类（Object强转），然后这个类就相当于穿了一层装备，每次调用方法实际是走的你实现的那个handler类里面的invoke方法，这样你可以在handler实现里面加上自己的处理<br>

<br>？？？序列化协议对应于 TCP/IP 4 层模型的哪一层？这个后续再看，不太懂<br><br>介绍<br>
Unsafe 是位于 sun.misc 包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升 Java 运行效率、增强 Java 语言底层资源操作能力方面起到了很大的作用。但由于 Unsafe 类使 Java 语言拥有了类似 C 语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用 Unsafe 类会使得程序出错的概率变大，使得 Java 这种安全的语言变得不再“安全”，因此对 Unsafe 的使用一定要慎重。

著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/basis/unsafe.html" target="_blank">https://javaguide.cn/java/basis/unsafe.html</a>
<br><br>
<br>条件编译，有局限，在通过判断常量之后会把确定的不会走的语句去掉再去编译<br>

<br>assert 底层逻辑就是if判断<br>

<br>数值字面量：不管是浮点数还是整数，java允许我们在数值中加入任意多个下划线，这些下划线不会对数字产生影响，目的就是为了方便阅读
  int i = 100_000_12;<br>

<br><br>java集合，也叫做容器，主要时两大类派生出来的，<br>
<br>一个是collection 用于存储单一的值，

<br>collection下面又分为三个大类

<br>List

<br>arrayList<br>

<br>vector<br>

<br>linkedList<br>



<br>set

<br>hashSet<br>

<br>linkedHashSet<br>

<br>TreeSet:(红黑树，自平衡的排序二叉树)<br>



<br>queue

<br>arryQueue<br>

<br>DelayQueue<br>

<br>PriorityQueue<br>







<br>一个是Map用来存贮键值对

<br>hashMap<br>

<br>linkedhashMap<br>

<br>Hashtable<br>

<br>TreeMap<br>



<br><br>
<br>arrayList里面不能储存基本类型的数据，，只能储存包装类，但是数组可以，（记：数组创建必须要确定大小）<br>

<br>arrayList的扩容机制

<br>无参构造Arraylist的时候，先是初始化一个空数组<br>

<br>然后真正赋值的时候，添加第一个数组的时候，数组容量扩大为10<br>



<br>ArrayList和Vector的区别，Vector很古老，线程安全（不是很推荐使用，线程安全可以用CopyOnWriteArrayList或者自己手动实现），arrayList属于新写法<br>

<br>ArrayList插入和删除元素的时间复杂度

<br>头部插入，由于需要将所有的元素都往后移动一个位置，所以时间复杂度是O（n）<br>

<br>尾部插入

<br>尾部插入分两种情况，一个是arrayList没有到达极限大小，那么复杂度就是O(1)<br>

<br>如果数组到达了极限大小，那么就要先做一个O（n)的操作把数组全体复制到一个大的数组中，再做一个O（1）的添加操作<br>

<br>指定位置插入，同头部插入O(n)<br>



<br>删除操作同理<br>



<br>LinkedList插入删除的时间复杂度

<br>头部插入/删除：只需要修改头结点的指针即可完成插入/删除操作，因此时间复杂度为 O(1)。
  尾部插入/删除：只需要修改尾结点的指针即可完成插入/删除操作，因此时间复杂度为 O(1)。
  指定位置插入/删除：需要先移动到指定位置，再修改指定节点的指针完成插入/删除，因此需要移动平均 n/2 个元素，时间复杂度为 O(n)。

  著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/collection/java-collection-questions-01.html" target="_blank">https://javaguide.cn/java/collection/java-collection-questions-01.html</a><br>



<br>LinkedList为什么不能实现randomAccess接口
  randomAccess接口是一个标记接口，他的作用是表面，实现他的类可以随机地址访问（就是通过索引快速访问元素），但是linkedList底层是链表，地址不连续，只能通过指针访问，不支持快速随机访问

所以其实，linkedList并没有在增删性能上优于arraylist，指定位置插入的删除复杂度也是O(n)


<br>随机访问时通过索引快速访问，数组天然支持快速随机访问，时间复杂度是O(1),而链表需要遍历到指定位置才能访问特定位置的元素，arrayList的底层就是数组，所以大多数场景，arrayList要比linkedList更好用<br><br>
<br>
comparable 和Comparator的区别：实现方法的参数不同

<br>
无序性和不可重复性的含义是什么

<br>无序性不是说就是随机排列，而是不是按照数组的索引顺序添加，是按照元素的hash值决定的

<br>不可重复性是指元素的判断相同时，元素的hashCode和equals方法不能不一致，要同时重写这两个方法<br>





<br><br>
<br>queue是单端队列，先进先出  
<br>deque是双端队列
<br><br>其实这俩都实现了Deque的接口<br>
<br>arrayDeque是基于可变长的数组和双指针来实现的，但是linkedList是基于链表来实现的<br>

<br>ArrayDeque不能存储null，但是linkedList可以<br>

<br>arrayDeque在插入时可能会有扩容，不过均摊后复杂度还是O（1）,linkedList不需要扩容，但是每次插入的时候都要申请新的堆空间，性能更慢

性能角度上，arrayDeque会更好


<br><br>作用是优先级更高的元素先出队，优先级用comparator来定义<br><br>阻塞队列，支持没有元素时一直阻塞到有元素，还支持当队列已满的时候会一直等到可以插入的时候再插入新元素<br>
<br>arrayBlockingQueue等  
<br>....了解即可
<br><br><br>
<br>hashMap是非线程安全的，但是hashTable是线程安全的，（但是hashTable快要被淘汰了，不要在代码里面使用他，想要线程安全可以用concurrentHashMap,concurrenthashmap里面的key不能为null）<br>

<br>hashMap是支持null 的key和value的，但是key只能有一个，hashtable会报空指针<br>

<br>hashmap 的默认大小是16，以后每次扩容都变成原来的2倍，hashtable默认大小是11,以后每次扩容都会变成原来的2n+1<br>

<br>如果创建时给定了大小，那么hashTable会直接使用给定大小，但是hashMap会把他扩充成2的幂次方（为了尽量把数据分配均匀，减少碰撞，碰撞会产生链表）<br>

<br>hashMap解决hash冲突时候（hash冲突会产生链表结构），当链表大于阈值（默认为8）就会先判断，if 当前数组长度小于64，那么就优先进行数组扩容，如果大于64则转链表为红黑树，而hashTable没有这个机制<br>

<br><br>
没啥大区别，Hash Set的底层就是基于HashMap实现的
<br><br>treeMap可以实现Comparator接口实现key排序<br><br>通过hashCode和equals方法<br><br><br>
<br>迭代器

<br>EntrySet<br>

<br>EntryKey<br>



<br>ForEach<br>

<br>Lambada<br>

<br>Stream<br>

<br><br>
<br>在使用 java.util.stream.Collectors 类的 toMap() 方法转为 Map 集合时，一定要注意当 value 为 null 时会抛 NPE 异常。<br>

<br>不要在foreach里面进行元素的remove/add操作，remove元素请用Iterator的方式，如果是并发操作，请加锁<br>

<br>使用Arrays.asList()的时候的注意事项

<br>第一这个数组不能是基本数据类型的，比如 int[] arr = {1,2,3}，Arrays.aslist(arr)会报错，要使用包装类型<br>

<br>使用集合的add,remove等方法会报错，因为Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类,这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。

  著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/collection/java-collection-precautions-for-use.html" target="_blank">https://javaguide.cn/java/collection/java-collection-precautions-for-use.html</a><br>



<br><br><br><br>
<br>线程比进程更小，但是同类的多个线程共享资源，而且每个线程都有自己的程序计数器和虚拟栈等<br>

<br>JAVA天生多线程，一个java程序的运行是main线程和多个其他线程同时运行<br>

<br>一个进程可以有多个线程，线程共享进程的堆和方法区，但是每个线程有自己的虚拟机栈，程序计数器和本地方法栈（私有的）

<br>虚拟机栈

<br>虚拟机栈相当于一个执行java的小空间<br>



<br>程序计数器

<br>程序计数器是记录当前线程的运行位置，方便切回来的时候，线程知道自己运行到哪了，方便实现代码的流程控制<br>



<br>本地方法栈

<br>和虚拟机栈类似，这个是用来执行native方法的<br>





<br><br>
<br>并发是指两个及以上的作业在同一时间段运行  
<br>并行是指两个及以上的作业同时运行
<br><br>
<br>初始<br>

<br>运行

<br>调用strat（）方法开始运行

初始和运行状态在JVM中都视作runnable状态




<br>阻塞

<br>当线程进入了synchronized 方法块，但是这个时候锁被其他线程占用，这个时候，线程就会进入到RUNNABLE状态<br>



<br>等待

<br>执行wait（）方法之后，就会进入等待状态，这个时候只能通过其他线程的notify才能回到运行时状态<br>



<br>超时等待

<br>等于在等待状态的基础上加了超时限制，比如sleep。等过了超时时间，会自动回到runnable状态<br>



<br>终止状态<br>

<br><br>什么是上下文：线程在执行过程中会有自己的运行过程和状态，就称之为上下文，上下文就是说是保存线程的执行状态，虚拟机栈，本地方法栈，以及程序计数器状态的一个统称<br>上下文切换发送的条件？：就是说线程sleep，或者报错，终止，时间片用完，都会切换，换到别的线程的上下文<br>频繁的切换会导致整体效率低下<br><br>
<br>什么是线程死锁

<br>举个例子，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态<br>

<br>代码为例

<br>public class DeadLockDemo {<br>
private static Object resource1 = new Object();//资源 1<br>
private static Object resource2 = new Object();//资源 2
  public static void main(String[] args) {
      new Thread(() -&gt; {
          synchronized (resource1) {
              System.out.println(Thread.currentThread() + "get resource1");
              try {
                  Thread.sleep(1000);
              } catch (InterruptedException e) {
                  e.printStackTrace();
              }
              System.out.println(Thread.currentThread() + "waiting get resource2");
              synchronized (resource2) {
                  System.out.println(Thread.currentThread() + "get resource2");
              }
          }
      }, "线程 1").start();

      new Thread(() -&gt; {
          synchronized (resource2) {
              System.out.println(Thread.currentThread() + "get resource2");
              try {
                  Thread.sleep(1000);
              } catch (InterruptedException e) {
                  e.printStackTrace();
              }
              System.out.println(Thread.currentThread() + "waiting get resource1");
              synchronized (resource1) {
                  System.out.println(Thread.currentThread() + "get resource1");
              }
          }
      }, "线程 2").start();
  }
复制
  }<br>

<br>Thread[线程 1,5,main]get resource1<br>
Thread[线程 2,5,main]get resource2<br>
Thread[线程 1,5,main]waiting get resource2<br>
Thread[线程 2,5,main]waiting get resource1<br>





<br>如何预防和避免线程死锁？

<br>预防（三点）

<br>第一是破坏请求与保持条件，比如一次性就申请所有的需要的资源，而不是一点一点申请<br>



<br>破坏不剥夺条件：占用部分资源的线程在进一步申请其他资源的时候，如果没有申请到，那就释放掉自己占用的资源<br>

<br>给申请资源的时候，排序，固定好释放和请求的顺序，按照某一顺序申请资源，释放也是按照顺序反向释放<br>

<br>避免

<br>借助算法（比如等线程一跑完，线程二才能申请到资源）<br>





<br><br>
<br>sleep()没有释放锁，但是wait方法释放了锁<br>

<br>wait()方法调用后，线程不会自动苏醒，要靠notify来唤醒，或者可以设置wait（long timeout），超时等待也会超时后苏醒，sleep会自动苏醒<br>

<br>sleep是Thread类的静态本地方法，而wait（）则是Object的本地方法<br>

<br><br>这个问题要从线程启动的流程来看<br>
<br>首先创建一个线程，new一个Thread以后，调用他的start方法，就会启动该线程，做一些准备工作，等到时间片分配到了以后，会自动执行他的run方法，这是真正的多线程工作，但是如果直接执行他的run方法的话，那这个线程就会被视为mian线程下的普通方法执行，而不是多线程工作了<br>

<br><br>
<br>
volatile关键字修饰的变量保证了变量的可视性，但是没有保证其原子性，就是volatile关键字修饰的的变量是共享的，每次使用都要去主内存去读取

<br>
synchrinized关键字是既能保证可视性，又能保证原子性

<br>
如何保证原子性（改进）？

<br>对于volitile变量，操作他的方法加synchronized关键字

<br>数字可以用automaticInteger来代替<br>



<br>用ReentrantLock改进<br>



<br>
Synchronized

<br>修饰实例方法<br>

<br>修饰静态方法<br>

<br>修饰代码块

静态synchronized方法和实例synchronized方法调用互斥吗？不互斥，因为一个是类的锁，一个是实例的锁


<br>尽量不要使用Synchronized(String)，因为有常量池这个东西，你一锁，就锁一片，影响其他线程<br>

<br>构造方法本身就属于线程安全，不能用Syncronized 修饰

Synchronized的底层原理，主要原理在于两个指令
一个是monitorenter，一个是monitorexit，

  <img alt="执行 monitorenter 获取锁" src="https://oss.javaguide.cn/github/javaguide/java/concurrent/synchronized-get-lock-code-block.png" referrerpolicy="no-referrer">
  <img alt="执行 monitorexit 释放锁" src="\lib\media\synchronized-release-lock-block.png" referrerpolicy="no-referrer"><br>



<br><br>volatile除了保证变量的可变性，还有一个重要作用就是防止JVM指令重排序<br>
 什么是指令重排序
 ​	为了提升执行速度/性能，计算机在执行程序代码的时候，会执行指令重排序，就是说，计算机执行程序的时候，不一定按照你代码写的顺序来
<br>
<br>双重检验锁方式实现单例模式

<br>public class SingleTon{<br>
private static volite SingleTon singleTon;
  private SingleTon(){
  }
  
  public static SingleTon getInstance(){
  	if(singleTon == null){
      	synchronized(SingleTon.class){
          	if(singleTon == null){
                  // 这一段实际上分三步执行    
              	singleTon = new singleTon()
              }
          }
      }
      return singleTon;
  }
  
复制
  }<br>

<br>singleTon = new singleTon()这一段实际分三步执行，

<br>为singleTon分配内存空间
<br>初始化singleTon
<br>将singleTon指向分配的地址

  但是由于JVM有指令重排的特性，这一步可能变成1-3-2，单线程环境下没有问题，多线程环境下就有问题出现了
  比如，线程A执行了1 3 然后时间片耗尽，这时候，线程2执行了这一段，发现singleTon不为null，就直接拿到了未初始化的singleTon对象，所以这个时候，volitile关键字就很有必要了<br>



<br><br>
<br>悲观锁：认为共享的资源每次都会被修改，所以每次申请资源的时候，都会上锁，其他想要访问的线程都会阻塞一直到上一个拿到资源的线程释放线程

<br>像java中的synchronized 和reentrantLock就是悲观锁的思想实现

<br>缺点，在高并发的场景下，频繁的阻塞会很浪费资源，增加性能开销，而且还可能有死锁的风险<br>





<br>乐观锁：认为线程是可以无限制的执行下去的，每次资源都不会被其他线程修改，但是在提交的时候会做一个验证，判断资源是否已经被其他线程修改过了，修改过了我就不给你修改了或者我就再重试一遍，重新拿资源修改一遍（版本号机制或者CAS算法）

<br>版本号机制：
  就是取资源的时候，资源上加一个version版本号，然后我修改完成以后，再看一下数据库里面这个版本号是不是跟我取的一样，一样就说明没别人动过，就改掉，顺便version加一，不一样就说明别人动过，那就驳回请求<br>

<br>CAS

CAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。

原子操作 即最小不可拆分的操作，也就是说操作一旦开始，就不能被打断，直到操作完成。

CAS 涉及到三个操作数：

<br>V：要更新的变量值(Var)<br>

<br>E：预期值(Expected)<br>

<br>N：拟写入的新值(New)<br>


当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。
举一个简单的例子：线程 A 要修改变量 i 的值为 6，i 原值为 1（V = 1，E=1，N=6，假设不存在 ABA 问题）。

<br>i 与 1 进行比较，如果相等， 则说明没被其他线程修改，可以被设置为 6 。
<br>i 与 1 进行比较，如果不相等，则说明被其他线程修改，当前线程放弃更新，CAS 操作失败。

当多个线程同时使用 CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。
Java 语言并没有直接实现 CAS，CAS 相关的实现是通过 C++ 内联汇编的形式实现的（JNI 调用）。因此， CAS 的具体实现和操作系统以及 CPU 都有关系。

著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html" target="_blank">https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html</a>




<br>乐观锁存在哪些问题？

<br>ABA问题：解决思路是CAS基础上加版本号或者时间戳<br>

<br>多写入场景频繁的重试或者失败会导致性能问题<br>



<br><br>ReentrantLock是一个实现了Lock接口的类，作用和Synchronized类似，是一个可重入且独占式的锁，但是更强大 (增加了，轮询，超时，中断，公平锁和非公平锁等高级功能)<br>
<br>公平锁

<br>锁被释放以后，先申请的线程先获得锁，但是会导致上下文频繁切换<br>



<br>非公平锁<br>

<br>锁释放后，可能随机线程获得锁，但是可能导致某些线程永远拿不到锁<br>

<br>可重入锁

<br>可重入锁也叫递归锁，这个可以解决部分死锁问题，简单例子就是一个线程拿到了一个对象的锁还没释放的时候，还能再获得该对象的锁，递归释放，目前JDK提供的LOCK实现类都是可重入式锁

<br>public class SynchronizedDemo {<br>
public synchronized void method1() {<br>
System.out.println("方法1");<br>
method2();<br>
}
  public synchronized void method2() {
      System.out.println("方法2");
  }
复制
  }<br>

<br>由于 synchronized锁是可重入的，同一个线程在调用method1() 时可以直接获得当前对象的锁，执行 method2() 的时候可以再次获取这个对象的锁，不会产生死锁问题。假如synchronized是不可重入锁的话，由于该对象的锁已被当前线程所持有且无法释放，这就导致线程在执行 method2()时获取锁失败，会出现死锁问题。<br>





<br>中断（获取锁的过程可中断）

<br>ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情<br>



<br><br>他是什么？为什么出现<br>
<br>首先，我们创建的变量在通常情况下都是能被所有线程访问修改的，但是这时候我们希望，能让线程拥有自己的专属本地变量，应该怎么办<br>

<br>ThreadLocal就出现了，用get(),set()或者独属于线程自己的本地变量，主要是避免线程竞争？？ 这个是不是就相当于变量锁一直不放?<br>

<br>原理解析：<br>

<br>
Thread 类里面有两个Map，一个是自身的map（ThreadLocalMap），一个是继承下来的map(inherientThreadLocalMap)，一开始，这两个都是null（ThreadLocalMap可以视作为ThreadLocal类定制实现的HashMap），我们在外面创建一个Thread的子类的时候，给他创建了一个成员变量ThreadLocl
 private static final ThreadLocal formatter = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat("yyyyMMdd HHmm"));
类似上面这样，那么线程重写（实现）run方法的时候，要在方法里调用这个formatter.get()或者set()的时候，就会初始化这个线程的这个定制的hashMap，然后拿值或者取值，所以实际上这个变量会存在线程中的ThreadLocalmaps里面，哪怕你给子类写了两个ThreadLocal，存还是存在一个ThreadLocalmap里面,key就是ThreadLocal对象，value就是set放的值
<br><img alt="ThreadLocal 数据结构" src="\lib\media\threadlocal-data-structure.png" referrerpolicy="no-referrer"><br><br>ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。<br>这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后最好手动调用remove()方法<br><br>著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/concurrent/java-concurrent-questions-03.html" target="_blank">https://javaguide.cn/java/concurrent/java-concurrent-questions-03.html</a><br><br>池化思想，线程池中的线程在完成任务后并不会被销毁，而是等待下一个任务<br>
<br>节约了资源的损耗<br>

<br>提升了线程的响应速度<br>

<br>提高线程可管理性<br>

<br><br>
<br>
构造函数创建
ThreadPoolExecutor来创建

<br>
Executor的框架工具类Executors来创建（不推荐，要开发的人更明确自己创建线程的意义）

<br><br>
<br>
不推荐Excutors 的方式创建线程，推荐构造方法创建，因为这样开发同学能更明确自己要什么样的线程池

<br>
阿里巴巴开发规范明确要求线程必须通过线程池来提供，不允许手动创建线程

<br>
线程池构造方法参数（7个）

<br>线程池大小 maxPoreSize

<br>当任务队列达到了最大值的时候，线程池允许的最大同时运行的线程数<br>



<br>核心线程数

<br>当任务队列没达到最大时，线程池允许的最大线程数<br>



<br>当线程池大于核心线程数的大小时，其他的非核心的线程存活的时间

<br>垃圾回收会对核心线程和非核心线程一视同仁，直到线程数量等于核心线程数为止

<br>时间单位<br>

<br>queue，队列存放任务的<br>





<br>拒绝策略

<br>就是当任务队列已经满了，而且线程池也已经满负荷了，这时候还加任务进来的拒绝策略<br>

<br>工厂（创建线程的工厂，一般默认就行）<br>





<br><br>
<br>
LinkedBlockIngQueue :无界的，所以你创建线程池的时候，永远不会满（实际容量为Integer.maxValue），那就创建一个核心线程数和最大一致的线程池

<br>
SynchronousQueue：同步的queue，同步的意思就是，我里面没有容量，一来任务我这就是满的，所以会一直需要最大线程数的线程池，可能会创建大量线程，导致OOM

<br>
DelayedWorkQueue： 这个特殊在于排序，按任务进来的延迟排序，延迟越低越先出栈

<br><img alt="图解线程池实现原理" src="https://oss.javaguide.cn/javaguide/%E5%9B%BE%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.png" referrerpolicy="no-referrer"><br><br>这个还是蛮重要的，因为普通的默认线程池名字不容易分辨，不带业务名称，看日志很成问题<br>
<br>自己实现工厂方法创建线程
<br>GUAVA的ThreadFactoryBuilder
<br><br>
<br>CPU密集型任务--》N+1（N是CPU核心数）

<br>就是密集的计算啊啥的<br>



<br>I/O密集型--》2N<br>

<br><br>PriorityBlockingQueue<br><br>详见美团技术团队的一些设置<br><br>
<br>
首先是考虑PriorityBlockingQueue 作为线程池的任务队列，因为这个是可以排序的，可以视为线程安全的PriorityQueue

<br>
然后由于这个任务队列可能是无界的，所以我们可以创建一个类继承PriorityBlockingQueue,重写他的offer方法，判断队列到达指定大小后就返回false，无法继续加进去

<br>
会可能存在某些任务进去以后，由于优先级的原因一直得不到线程，那么就可以继续修改一下这个子类，当一个任务超时多久就提升一个优先级之类的

<br><br>允许多个线程阻塞在一个地方<br><br><br>类比我们开发时候用到redis是为了解决程序处理速度和关系型数据库速度不匹配的问题<br>缓存就是为了解决CPU处理速度和内存存储速度不匹配的问题（硬盘的访问速度比较慢）<br><br>
<br>
原子性

<br>
可见性

<br>
有序性（volatile关键字可以禁止指令重排）

<br><br>笔记栏<br>
<br>
什么是IO ?

<br>
四个基类

<br>InputStream/Reader
<br>OutputStream/Writer




<br>
能做什么？

<br>

读取文件，写文件




<br>
关于字节流，字符流之类的方法比较多，不详细记录，主要重点感觉是flush方法，由于流是慢慢传输的，有时候，流在写入（只有写入流有这个方法）文件的时候，你不能说你觉得读完了就把流关闭了，你要在流关闭之前，flush一下，把流里面的东西清空，然后再关闭
public class test02 {<br>
public static void main(String[] args) {<br>
File file = new File("./xx.txt");<br>
try (FileOutputStream fileOutputStream = new FileOutputStream(file)) {<br>
fileOutputStream.write(new byte[1024]);<br>
fileOutputStream.flush();
    } catch (Exception e) {
        throw new RuntimeException(e);
    }
}
复制
}
上面的方法就可以得到1024个null字符的txt文件

<br>
字节缓冲流

<br>
是什么

<br>是一个包装类，创建方法就是new BufferedInputStream(inputStream)


<br>
做什么

<br>包装输入流，增强输入流能力的
<br>IO操作是非常耗费性能的操作，缓冲流能将数据一下子加载到缓冲区再慢慢操作，从而就能避免频繁的IO操作


<br>
什么特征？

<br>文件越大性能更好，缓存区的存在可以节约更多的性能




<br><br><br>
不改变对象的前提下，拓展其功能
<br>就是跟跟上面说的‘包装类’有类似的效果，需要的是装饰器需要跟原始类继承相同的抽象类或者实现相同的接口<br>例如：我们常见的BufferedInputStream(字节缓冲输入流)、DataInputStream 等等都是FilterInputStream 的子类，我们可以通过 BufferedInputStream（字节缓冲输入流）来增强 FileInputStream 的功能<br>用子类去增强原始类就既可以调原始类的方法，也可以调字节写的新方法了<br><br>
主要用于适配接口不兼容的情形
<br>举个例子，如何将字符流转换成字节流？<br>依靠 InputStream这个大类来转换,下面的代码把文件输入流（字符流）转换成了InputStreamReader（字节流），这里是因为InPutstream里面有一个StreamDecoder，专门用来转换的<br>// InputStreamReader 是适配器，FileInputStream 是被适配的类<br>
InputStreamReader isr = new InputStreamReader(new FileInputStream(fileName), "UTF-8");<br>
// BufferedReader 增强 InputStreamReader 的功能（装饰器模式）<br>
BufferedReader bufferedReader = new BufferedReader(isr);<br><br>
用于创建对象，就比如Files 类的 Files.newInputStream()方法
<br><br>
我理解就是，监听，在代码中，你定义了一个类，然后实现Watchable接口，然后去重写他的监听方法，比如监听文件的path类的监听方法里面采用的就是<a data-tooltip-position="top" aria-label="https://cloud.tencent.com/developer/article/2318134" rel="noopener" class="external-link" href="https://cloud.tencent.com/developer/article/2318134" target="_blank">守护线程</a>？采用轮询的方式去监听操作的文件的变化
<br><br>常见IO模型<br>
<br>
BIO

同步阻塞IO

在同步阻塞IO模型中，应用发起read之后，会一直阻塞，一直到内核把数据拷贝到用户空间里面，然后直到read返回，才会接触阻塞，这个同步的方式在大量用户大量读取的场景下，比较吃力
<img alt="图源：《深入拆解Tomcat &amp; Jetty》" src="\lib\media\6a9e704af49b4380bb686f0c96d33b81~tplv-k3u1fbpfcp-watermark.png" referrerpolicy="no-referrer">

<br>
NIO

NonBlocking IO 不阻塞IO

NIO属于多路复用模型，而不是同步非阻塞模型
<img alt="图源：《深入拆解Tomcat &amp; Jetty》" src="\lib\media\bb174e22dbe04bb79fe3fc126aed0c61~tplv-k3u1fbpfcp-watermark.png" referrerpolicy="no-referrer">
<img alt="img" src="\lib\media\88ff862764024c3b8567367df11df6ab~tplv-k3u1fbpfcp-watermark.png" referrerpolicy="no-referrer">
两者的差别就在于在准备数据阶段，同步不阻塞模型一直轮询采用read调用去判断数据有没有准备好，好了以后再阻塞的去拷贝数据
而多路复用模型则是直接select去查询系统是否准备好了，等准备好了，再阻塞的调用read

IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。
Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。

著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/io/io-model.html" target="_blank">https://javaguide.cn/java/io/io-model.html</a>

NIO,主要包括下面三个核心组件

<br>缓冲区

<br>NIO的读写数据都是通过缓冲区实现的，读操作的时候，把Channel中的数据写到buffer里面，写操作的时候，把Buffer中的数据写到Channel里面<br>

<br>NIO在读写数据的时候，都是通过缓冲区进行操作<br>



<br>Channel（通道）<br>

<br>Selector（选择器）<br>



<br>
AIO(Asynchronous I/O)

<br>AIO就是NIO2 ，属于异步IO模型，也就是完全不阻塞，基于回调机制实现，应用读取完会直接返回，等后台数据准备完成，操作系统会通知相对应的程序去进行操作<br><br><br><img alt="Java 运行时数据区域（JDK1.8 ）" src="\lib\media\java-runtime-data-areas-jdk1.8.png" referrerpolicy="no-referrer"><br>java 天生是多线程的语言，那么我们按照线程拥有的来划分可以得到<br>线程私有的：<br>
<br>程序计数器

<br>这个是因为线程总是在CPU分配的时间片上才是活的，有自己的上下文，计数器用来记录线程跑到哪里了，方便切回来的时候继续运行，控制代码的流程<br>



<br>虚拟机栈

<br>这个相当于一块小的虚拟机用来运行

<br>栈是JVM运行时核心，除了本地方法，其他所有的java方法的调用都是通过栈来完成的（和程序计数器配合）<br>



<br>每一次方法的调用，都会有一个对应的栈帧被压入栈中，调用结束后会被弹出<br>



<br>本地方法栈

<br>这个和虚拟机栈类似，但是是用来执行本地方法的<br>



<br>线程共享的：<br>
<br>堆

<br>堆是jvm管理内存中最大的一块内存，这一块的目的就是为了存放对象实例，几乎所有的对象实例和数组都在这里分配内存<br>

<br>为什么说几乎，因为JDK1.7开始，就已经开始默认开启逃逸分析，一些方法的的对象的引用没有被返回，或者没有被外面引用，那么这个对象的创建会在栈上<br>

<br>字符串常量池在堆上，为什么放堆上？：因为堆是GC管理的最大的区域，放堆中可以及时高效的回收，常量池里存放的直接是字符串对象，而不是引用<br>

<br>方法区

<br>JDK1.7的时候，堆内存被分为新生代，老生代，永久代，方法区就属于永久代<br>

<br>方法区是线程共享的一块逻辑区域，如何实现要看不同的虚拟机的实现<br>

<br>JDK1.8的时候，方法区（永久代）就被移除了，取而代之的是元空间<br>





<br>直接内存<br>

<br><br><br><br>
<br>
第一步 类加载检查

当虚拟机遇到一条新的new指令的时候，首先会去检查能不能从常量池找到对应对象的引用，然后再判断 new的这个类，是否在之前就被加载，解析和初始化过了，如果没有，那要执行相应的类加载过程


<br>
第二步 分配内存

在类加载完成之后，就会进入分配内存的过程中，这个大小在类加载完成后就可以确定，分配方式分为指针碰撞和空闲列表两种，具体选哪种要看堆内存是否规整，而堆内存是否规整要看垃圾回收装置是否带压缩整理功能（规整的收集器：parNew,serial，不规整的：CMS）


<br>指针碰撞

<br>如果内存是规整的前提下，用过的内存在一边，没用过的内存在另一边，中间会有个指针，只要把指针往没有用过的内存那边移动一个该类内存大小就可以了<br>



<br>空闲列表

<br>如果内存是不规整的，那么虚拟机会自己维护一个列表，里面记录了哪些内存块是可以用的，分配内存的时候，分配一个足够大的内存块给这个类就可以了<br>




内存分配的并发问题：
创建对象来说，虚拟机必须要保证创建过程是线程安全的，一般来说，虚拟机会通过两种方式来保证线程安全（因为堆是共享的，所以保证线程安全就很重要）

<br>TLAB：首先是虚拟机会为每个线程在Eden区域分配一块内存，创建对象的时候，优先在eden区域中创建，只有这一块内存不够的时候，再去堆中，用CAS+失败重试的方式去创建（乐观锁）

<br>eden区域，堆内存的一块地方，属于新生代<br>

<br>CAS +失败重试<br>





<br>
第三步 初始化零值

当内存分配完毕以后，虚拟机就会把对象里面的一些内存空间都初始化零值，比如，int 零值就是0 对象类零值就是null，这一部保证了 java的实例字段（成员变量）不需要初始化就能用
但是这不包含对象头


<br>
第四步 设置对象头

对象头就是一些信息，例如这个是哪个类的实例，或者对象的GC分代年龄等信息

Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。

著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/jvm/memory-area.html" target="_blank">https://javaguide.cn/java/jvm/memory-area.html</a>

<br>
第五步 init方法

实际上，在上一步的时候，虚拟机已经认为一个新的对象被创建好了，但是从程序的视角来看，这才刚刚开始，init就是构造方法之类的一些防范，给对象赋上真正的程序需要的值，变成一个程序可用的对象


<br><br>需要排查各种内存溢出问题，当系统过大，垃圾回收成为系统实现更高并发的瓶颈的时候，我们就要对这些进行必要的监控和调节<br><br>Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收<br>在jdk7及以前的版本里，堆内存通常分为如下的三块部分<br>
<br>新生代（eden和两个相等大小的survivor区）
<br>老生代
<br>永久代（JDK8以后被元空间取代，metaspace，元空间用的是直接内存）
<br><img alt="堆内存结构" src="\lib\media\hotspot-heap-structure.png" referrerpolicy="no-referrer"><br>上面的eden区和S0 S1属于新生代<br>Tenured属于老生代<br>下面就是永久层/元空间<br><br>
<br>创建对象有5步（看上面笔记）在分配内存的那一步的时候，一般是会在eden层进行，如果eden层内存空间不足了这时候再创建一个对象，那么垃圾回收就会执行一次minor GC
<br>minorGC: 就是会把这个eden里面的已经存在对象，找到里面活跃的对象，然后放到S1里面去，同时清理掉eden区和s1区的数据，然后对象年龄加一，一般15岁就会存放到老生代区<br>
大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁，cms是6），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数
-XX:MaxTenuringThreshold
来设置。
就是说对象每熬过一次minorGC就会长一岁（这个计算是动态的，根据对象年龄大小和占用动态涨年龄，说的浅显一点就是大概长了一岁）
<br>
<br>当进行minorGC以后，发现这个老的对象还是无法存入Survior区域，那么就会通过分配担保机制把这个老对象直接放到老年区
<br>后面创建的新的对象，如果能放eden区域的话，那么还是会在eden区域进行分配内存的
<br><br>
<br>
进入老年代有两种方式，一种是年龄到了，自动进入老年代

<br>
另一种是大对象（需要连续区域的大对象，一般有大数组，字符串之类的），会直接进入老年代

<br><br>
<br>分类：

<br>部分收集

<br>youngGC/minor GC 是用来回收新生代的<br>

<br>old GC /major GC 用来回收老年代的<br>

<br>mixed GC 用来回收新生代和老年代的<br>



<br>整堆收集

<br>full GC 收集整个java堆和方法区<br>





<br>什么时候进行这些回收？

<br>首先是yong GC 这个是创建对象的时候，eden区放不下新的对象的内存了，就会触发yong GC，这时候，eden区域存活的对象，会进入survior区，年龄也会增长，如果发现放不进去survior区，那就晋升到老年区去<br>

<br>那如果老年区也不够用咋办呢？那新生代的对象就放不进去老年区了，这时候，就不会触发yongGC了，会直接触发fullGC，回收掉整个java堆和方法区（判断对象死亡：就是不再被任何地方引用）<br>



<br>引用类型介绍（强，软弱无力）

<br>强引用

<br>大部分我们使用的都是强引用，java回收装置宁愿抛出outofmenory错误也不会回收强引用<br>



<br>软引用

<br>可有可无的引用，当内存不足 的时候，垃圾回收器就会回收他们<br>



<br>弱引用

<br>这个比软引用具有更短的生命周期，一旦被垃圾回收器扫到就一定会回收<br>



<br>虚引用

<br>这个不会影响对象的生命周期，几乎就等于没有任何引用，任何时候都可能会被回收<br>





<br>垃圾清理算法

<br>标记-清理算法
<br>复制算法
<br>标记-整理算法
<br>分代收集算法


<br><br><br>字节码就是 .class文件，是jvm可以理解的代码，是不同语言在java虚拟机上的重要桥梁，很多语言，包括 GRoovy，Scala，Kotlin 都是运行在Java虚拟机上的<br>Class文件通过ClassFile定义<br>ClassFile {<br>
u4             magic; //Class 文件的标志<br>
u2             minor_version;//Class 的小版本号<br>
u2             major_version;//Class 的大版本号<br>
u2             constant_pool_count;//常量池的数量<br>
cp_info        constant_pool[constant_pool_count-1];//常量池<br>
u2             access_flags;//Class 的访问标记<br>
u2             this_class;//当前类<br>
u2             super_class;//父类<br>
u2             interfaces_count;//接口数量<br>
u2             interfaces[interfaces_count];//一个类可以实现多个接口<br>
u2             fields_count;//字段数量<br>
field_info     fields[fields_count];//一个类可以有多个字段<br>
u2             methods_count;//方法数量<br>
method_info    methods[methods_count];//一个类可以有个多个方法<br>
u2             attributes_count;//此类的属性表中的属性数<br>
attribute_info attributes[attributes_count];//属性表集合<br>
}<br>在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。<br><br>著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/jvm/class-file-structure.html" target="_blank">https://javaguide.cn/java/jvm/class-file-structure.html</a><br><br><br>类从被加载一直到被卸载，可以简单概括成7个阶段<br>
<br>加载
<br>验证
<br>准备
<br>解析
<br>初始化
<br>使用
<br>卸载
<br>其中，验证-准备-解析可以统称为连接（Link）<br><img alt="一个类的完整生命周期" src="\lib\media\lifecycle-of-a-class.png" referrerpolicy="no-referrer"><br><br>类加载过程，就是如上图所示的，加载-连接-初始化<br>连接就是框里的三步（验证-准备-解析）<br>
<br>加载
  加载这一步主要是通过类加载器（ClassLoader）去确定的，用哪个类加载器是由 双亲委派模型确定的（不过我们也能打破双亲委派模型）
  每个java类都有指向他的ClassLoader，不过数组类除外，他们是JVM需要的时候自动创建的，数组类通过 getClassLoader获取他的ClassLoader 得到的和他内部元素的ClassLoader是一样的
  类加载的过程主要做三件事

<br>通过全类名获取定义此类的二进制字节流。（通过名字获取二进制字节流）
<br>将字节流所代表的静态存储结构转换为方法区的运行时数据结构。（然后把字节流的静态格式，转换成方法去动态的数据结构）
<br>在内存中生成一个代表该类的 Class 对象，作为方法区这些数据的访问入口。（然后生成一个Class入口）


<br>验证
  保证Class文件的字节流信息不会损害虚拟机<br>

<br>准备
  分配内存和准备类变量初始值<br>

<br>解析
  解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程<br>

<br>初始化
  初始化阶段是执行初始化方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。<br>

<br><br><br>首先<br>
<br>
每个java类（除了数组）都有一个引用指向加载他的Classloader

<br>
类加载器负责实现类的加载过程

<br>
数组类不是通过ClassLoader创建的，是JVM直接生成的

<br>简而言之，就是类加载器作用就是 把class文件转换到虚拟机里面生成一个class对象<br><br>
<br>JVM中类是动态加载的，JVM启动的时候不会加载所有的类，而是根据需要动态的加载类，对内存更加友好
<br>对JVM来说，相同的名称的类只会加载一次，加载过的类会存在ClassLoader里面，需要的时候直接返回
<br><br>jvm中内置了三个重要的Classloader<br>
<br>
BootstrapClassLoader
顶层加载器，通过getParent方法找这个加载器是null，因为这是C写的，java中没有对应的类

<br>
ExtensionClassLoader

<br>
AppClassLoader

<br><br>java类是由类加载器加载的，但是具体是哪个类加载器，则需要通过双亲委派模型了<br>
<br>ClassLoader使用委派机制<br>

<br>双亲委派模型要求除了最顶层的ClassLoader之外，每个ClassLoader都要有自己的父ClassLoader<br>

<br>在ClassLoader亲自去加载资源的之前，会委派父类Classloader去加载（比如加载的时候要判断这个类之前有没有加载过，就要一直找到最顶上的父类去查，然后查出来没有，我就依次向下，开始加载你这个类）<br>

<br><img alt="类加载器层次关系图" src="\lib\media\class-loader-parents-delegation-model.png" referrerpolicy="no-referrer"><br>
这里要注意的是，类加载器之间的父子关系不是以继承来实现的，而是组合 如下
public abstract class ClassLoader {<br>
...<br>
// 组合<br>
private final ClassLoader parent;<br>
protected ClassLoader(ClassLoader parent) {<br>
this(checkCreateClassLoader(), parent);<br>
}<br>
...<br>
}
在面向对象编程中，有一条非常经典的设计原则：组合优于继承，多用组合少用继承
<br>🌈 拓展一下：<br>JVM 判定两个 Java 类是否相同的具体规则：JVM 不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即使两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相同。<br><br>著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：<a rel="noopener" class="external-link" href="https://javaguide.cn/java/jvm/classloader.html" target="_blank">https://javaguide.cn/java/jvm/classloader.html</a><br><br>
<br>
可以保证java的稳定运行，避免类的重复加载，

<br>
二来，（java判断类相同的标准加上了加载器）也保证了java核心API不会被篡改

<br><br>首先是如何自定义一个类加载器<br>
<br>
写一个类，继承ClassLoader类，重写里面的findClass（String name），（比如，我们想加载一个D盘某个目录下的所有的类，那我们就findClass写成从D盘某个路径下的 name.class把他转换成class类这种）

<br>
但是如果想要打破双亲委派模型，那么就要重写，loadClass()方法了，就可以自定义怎么loadClass了，不需要一层层往上找了（比如tomcat服务器，为了优先加载web目录下的类，再加载其他目录下的类，就自己定义实现了webAppClassLoader这个类）

<br><br><br>主内存（Main Memory）：主内存是所有线程共享的内存区域，用于存储共享变量。当一个线程需要读取或修改共享变量时，它首先会从主内存中获取该变量的值，然后在自己的工作内存中进行操作，最后将结果写回主内存。<br>工作内存（Working Memory）：工作内存是每个线程私有的内存区域，用于存储主内存中共享变量的副本。线程对共享变量的所有操作都发生在工作内存中，然后将结果同步回主内存。这样可以提高程序的执行效率，避免每次操作共享变量都需要直接访问主内存。<br>内存间交互操作：JMM定义了八种操作，用于描述生成的class字节码指令对内存的读写要求。这些操作包括：lock（锁定）、unlock（解锁）、read（读取）、load（载入）、use（使用）、assign（赋值）、store（存储）和write（写入）。<br>原子性：原子性是指一个操作要么全部完成，要么全部不完成。JMM保证了基本类型的读取和写入操作的原子性，但对于复合操作（例如自增运算），JMM并没有保证其原子性。为了解决这个问题，可以使用synchronized关键字或显式锁（如ReentrantLock）来确保复合操作的原子性。<br>可见性：可见性是指当一个线程修改了共享变量的值，其他线程能够立即看到修改后的值。JMM通过volatile关键字和synchronized关键字来保证可见性。当一个共享变量被volatile修饰时，其他线程能够立即看到该变量的最新值；当一个共享变量在synchronized块中被修改时，其他线程在退出synchronized块后能够立即看到修改后的值。<br>有序性：有序性是指程序执行的顺序应该按照代码的先后顺序进行。然而，由于处理器和编译器的优化，代码的执行顺序可能会被重新排序。JMM通过happens-before规则来保证程序的有序性。如果一个操作happens-before另一个操作，那么第一个操作的结果对于第二个操作是可见的，而且第一个操作不会因为线程的执行而重排序。<br>总之，JMM内存模型定义了一组规则，用于确保多线程程序在各个虚拟机上运行时能够得到一致的结果。这些规则涉及原子性、可见性和有序性等方面，可以帮助开发者编写正确、可靠的多线程程序。<br><br>新建（New）：线程刚被创建，但是尚未启动。还没调用start()方法。<br>可运行（Runnable）：线程可以在Java虚拟机中运行的状态，可能正在运行自己代码，也可能没有，这取决于操作系统处理器。包括运行（running）和就绪（ready）两种状态。<br>阻塞（Blocked）：线程进入等待状态，线程因为某种原因，放弃了CPU的使用权。阻塞的情况分三种： A. 等待阻塞：运行的线程执行了wait()，JVM会把当前线程放入等待队列。 B. 同步阻塞：运行的线程在获取对象的同步锁时，如果该同步锁被其他线程占用了，JVM会把当前线程放入锁池中。 C. 其他阻塞：运行的线程执行sleep()、join()或者发出IO请求时，JVM会把当前线程设置为阻塞状态，当sleep()执行完，join()线程终止，IO处理完毕线程再次恢复。<br>等待（Waiting）：线程拥有了某个锁之后，调用了wait()方法，等待其他线程/锁拥有者调用notify/notifyAll以便该线程可以继续下一步操作。线程调用join()方法时，也会进入等待状态，等待被join的线程执行结束。<br>超时等待（Timed Waiting）：同等待状态，有几个方法有超时参数，调用他们将进入Timed Waiting状态。这一状态将一直保持到超时期满或者接收到唤醒通知。带有超时参数的常用方法有Thread.sleep、Object.wait。<br>终止（Terminated）：线程执行完毕，因为run()方法正常退出或者因为没有捕获的异常终止了run()方法而死亡。终止的线程不可再次恢复。<br><br>AQS（AbstractQueuedSynchronizer）是Java并发包java.util.concurrent中的一个抽象类，它提供了构建锁和其他同步组件的一个通用框架。AQS实现了基于队列的阻塞和唤醒机制，用于构建各种同步组件，如ReentrantLock、Semaphore、CountDownLatch等。<br>AQS内部维护了一个整数状态（state）和一个双向队列（CLH队列），用于实现同步组件的阻塞和唤醒操作。它定义了三个核心方法：tryAcquire、tryRelease和isHeldExclusively。这些方法需要子类根据具体同步组件的实现来提供。]]></description><link>2-领域\后端学习\java\java基础巩固.html</link><guid isPermaLink="false">2-领域/后端学习/Java/JAVA基础巩固.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="lib\media\1ebd1df862c34880bc26b9d494535b3dtplv-k3u1fbpfcp-watermark.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\1ebd1df862c34880bc26b9d494535b3dtplv-k3u1fbpfcp-watermark.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[mybatis 分页]]></title><description><![CDATA[ 
 刚开始分页，记录一下分页的小问题]]></description><link>2-领域\后端学习\java\mybatis-分页.html</link><guid isPermaLink="false">2-领域/后端学习/Java/mybatis 分页.md</guid><pubDate>Sun, 05 May 2024 18:32:04 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240430190241.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240430190241.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[复杂查询]]></title><description><![CDATA[ 
 <br><br>简而言之，套娃查询<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227231703.png" referrerpolicy="no-referrer"><br><br>举个例子<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240229233112.png" referrerpolicy="no-referrer"><br><br>实际上，我们可以把我们查出来的表当作一个虚拟的表写到 from 那块比如这样<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240229233355.png" referrerpolicy="no-referrer"><br>
但是这样写可能会导致查询看起来非常的复杂，我们有个更好的解决办法就是使用视图<br>
<br>等视图讲到的时候写一下链接 📅 2024-03-04 ✅ 2024-03-14
<br><a data-href="函数、视图、储存、触发器#视图" href="\2-领域\后端学习\mysql\基础\函数、视图、储存、触发器.html#视图" class="internal-link" target="_self" rel="noopener">函数、视图、储存、触发器 &gt; 视图</a><br><br>ALL 关键字的使用方式可以用 IN 来类比<br>
下面是大概的理解<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227232810.png" referrerpolicy="no-referrer"><br>
然后是能跑通的过程<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227232914.png" referrerpolicy="no-referrer"><br>-- 查询表中所有大于，客户3所拥有的最大发票，的所有发票
SELECT * From invoices WHERE invoice_total &gt;(
	SELECT max(invoice_total) FROM invoices 
	where client_id =3
);
-- 和上面一样结果，只是加了all关键字
SELECT * From invoices WHERE invoice_total &gt; ALL (
	SELECT invoice_total FROM invoices 
	where client_id =3
);
复制<br>总结：就是说 max 的聚合函数都可以用 all 进行替换，反之亦然<br><br>= any 可以和 in 关键字互换，达到相同的效果<br>
例如<br>select * from xxx where a in (1,2,5,25);
select * from xxx where a = any(1,2,5,25);
复制<br>注意 any 不能有空格和括号相隔<br>SELECT * FROM clients WHERE client_id = any(
SELECT client_id FROM invoices GROUP BY client_id HAVING COUNT(*) &gt;=2
)
复制<br>
<br>看到 p51 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=51&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=51&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">7- 相关子查询 | Correlated Subqueries_哔哩哔哩_bilibili</a> ✅ 2024-02-29
<br><br>同样是和 IN 关键字有关联<br>举个例子我们要查询所有拥有发票的客户要怎么查询？如下，使用了 in 关键字，但是这样有问题，如果我们查询的 invoice 数量很大那就可能造成一些性能问题，<br>-- 查询有invoice的client
select * from clients
where client_id in (
	SELECT DISTINCT client_id from invoices
)

-- 实际上可以理解成这样
select * from clients
where client_id in (
	1，2，2，3，5,8，4，5，45,9，84，5，6，87,1......
)

复制<br>解决方式可以是这样，使用exist<br>-- 替换成exist 运算符可以提升效果
SELECT * from clients c
where EXISTS(
	SELECT * FROM invoices where client_id = c.client_id
)
复制<br><br>我们学到了<br>
<br>子查询的使用方式
<br>相关关键字

<br> 这一章我们主要学到了子查询以及子查询相关的 3 个关键字，==这几个关键字最大的特点就是，都和 IN 关键字有或多或少的关系 ==

<br> ALL 关键字

<br> 作用是和 max 函数互换，实际上就是一种替代，用 in 的方式来理解，详见<a data-href="#ALL 关键字" href="\#ALL_关键字" class="internal-link" target="_self" rel="noopener">ALL 关键字</a>


<br> ANY 关键字

<br> 其实理解和 ALL 很像，这个可以用=any 替换掉 in 关键字


<br> EXIST 关键字

<br> 这个是比较实用的一个关键字，作用是替换掉 in 关键字，在大批量的数据面前，exist 会更高效






]]></description><link>2-领域\后端学习\mysql\基础\复杂查询.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/复杂查询.md</guid><pubDate>Thu, 28 Mar 2024 02:25:01 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227231703.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227231703.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[函数、视图、储存、触发器]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=55&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=55&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">【第七章】1- 数值函数 | Numeric Functions「MySQL的基本函数」_哔哩哔哩_bilibili</a><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240313225709.png" referrerpolicy="no-referrer"><br><br>mysql 里面很多函数，粗略过了一遍，大部分都是可以查到的，<br>
<br>现在有个问题是，通过 mysql 的函数运算出来或者拿出来用代码去运算哪个速度快？
<br><br>
<br>视图类似一个指针，指向数据，变成一个类似表的东西，你对他的所有操作都会映射到对应的数据，反过来一样
<br>为了防止视图的更新影响对应的表，可以在创建视图的时候，加上withCheckOption
<br>视图带来的抽象性保证了如果基础表改动，查询的稳定性，只需要修改视图，就能将修改基础表带来的大量的 sql 重写风险给排除掉
<br>
<br>mysql 未完待续 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=67&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=67&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">5- 视图的其他优点 | Other Benefits of Views_哔哩哔哩_bilibili</a> ✅ 2024-03-14
<br><br>
<br>大部分的 DBMS(database management system，縮寫：DBMS)对储存过程做了优化，大部分储存过程中的 sql 运行的更快<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314223701.png" referrerpolicy="no-referrer">
<br>创建存储过程<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314225047.png" referrerpolicy="no-referrer">
<br>删除可以加上 if exists，注意 drop 的话函数不用带括号 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314225735.png" referrerpolicy="no-referrer">
<br><br>
<br>看到 p 73 了 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=73&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=73&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">6- 带默认值的参数 | Parameters with Default Value_哔哩哔哩_bilibili</a> ✅ 2024-03-16
<br>
<br> 储存过程可以传参 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240314230104.png" referrerpolicy="no-referrer">参数是必填的，不填就会报错
<br><br>
<br>带逻辑的储存过程，不知道是什么原因，create or replace 语法好像不生效，可能是我记错了，目前这样是可以运行的，搜了一下，mysql 好像没这个语法，高斯 db 是有这个语法的，打个标记，后面再研究
<br>DROP procedure if exists get_clients_by_state;
create Procedure get_clients_by_state (state CHAR(2))
BEGIN
	IF state is NULL then 
		set state = 'CA';
	End if;
	
	SELECT * from clients c where c.state = state;
END;

call get_clients_by_state(NULL);
复制<br>这部分可以使用 IFNULL 函数来进行一些很酷的操作<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240316235041.png" referrerpolicy="no-referrer"><br>
IFNUll 函数就是如果参数为 null，那么我们就用后面的参数为值<br>
c.state = c.state 永远为 true ，那么就能查到全部了 ]]></description><link>2-领域\后端学习\mysql\基础\函数、视图、储存、触发器.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/函数、视图、储存、触发器.md</guid><pubDate>Wed, 27 Mar 2024 15:23:09 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240313225709.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240313225709.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[mysql查询]]></title><description><![CDATA[ 
 <br><br>-- 去重多余结果
select distinct state from customer
复制<br>
<br>未完待续，课程看完了 p 8  <a data-tooltip-position="top" aria-label="https://bilibili.com/video/BV1UE41147KC/?p=8&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://bilibili.com/video/BV1UE41147KC/?p=8&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">2- 选择子句 | The SELECT Clause_哔哩哔哩_bilibili</a>，注：新看完了8课 📅 2024-01-31 ✅ 2024-01-31
<br>-- 查询语法
select * from xxx
select * from xxx where  a&gt;b and cday &gt; '1990-01-01'

复制<br>
<br>运算符顺序 (+-*/)，乘除大于加减（可以括号改变顺序）
<br>逻辑符顺序（and, or） and 运算优先评估（可以括号改变顺序）
<br>时间可以直接写
<br>-- in语法
select * from Customer where state in ('va','la','ga')
-- between语法
select * from Customer where points between 1000 and 3000
select * from Customer where birthday between '1990-01-01' and '2000-01-01'

复制<br><br>
<br>可以用 % 来匹配任意长的字符，
<br>用 _ 匹配单个字符
<br>-- 模糊查询
select * from Customer where last_name like "%abc%" 
select * from Customer where last_name like "a____c" 
复制<br><br>一个新的关键字，正则 REGEXP<br>-- 下面两个表达等价
select * from customer where last_name like '%field%'
select * from customer where last_name regexp 'field'

复制<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240131212858.png" referrerpolicy="no-referrer"><br>关于正则<br>
<br>使用 ^xx 表示以 xx 开头 
<br>使用 xx$ 表示以 xx 结尾
<br>使用 a|b 表示或条件
<br>使用 [abcx]e 表示中括号里面的任意字母均可以出现在 e 前面，例如 ae、be、xe
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240131214544.png" referrerpolicy="no-referrer"><br>
例子：<br>--查询以field结尾或包含mac或以rose开头的
SELECT * from customer where last_name regexp 'field$|mac|^rose';
SELECT * from customers where last_name regexp '(field|mac)$';
复制<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240131214045.png" referrerpolicy="no-referrer"><br>-- 正则查法
SELECT * FROM customers WHERE LAST_NAME REGEXP '[a-h]e';
复制<br><br>SELECT * from customers where phone IS NULL
复制<br><br>order by 按顺序排序<br>-- 多列排序，先排前面的，再排后面的
SELECT  * FROM customers ORDER BY state desc, first_name desc;
-- 排序不需要在sql查询的列里
SELECT  first_name,last_name FROM customers ORDER BY state desc, first_name desc;
-- 可以用别名排序，或者加的一个列
SELECT  first_name,last_name, 10 as points FROM customers ORDER BY points, state desc;
-- 可以用列来排序，这里的1，2指的是查询的first_name last_name,但是尽量避免这种写法，防止前面查询的顺序会变换导致查询的顺序变化
SELECT  first_name,last_name, 10 as points FROM customers ORDER BY 1,2;
-- 他也可以是一个表达式
SELECT *,quantity * unit_price as total_price from order_items ORDER BY quantity * unit_price;
SELECT *,quantity * unit_price as total_price from order_items ORDER BY total_price;
复制<br>
<br>sql 看完 p16 下次看 p17 了 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=17&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=17&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">11- LIMIT子句 | The LIMIT Clause_哔哩哔哩_bilibili</a> 📅 2024-02-04 ✅ 2024-02-04
<br><br>-- 前面相当于offset 6  往后查三位
select * from customers limit 6,3
复制<br><br><br>内连接是 inner join 但是实际上，这个 inner 可有可无<br>-- 单join
select * from orders JOIN customers on orders.customer_id = customers.customer_id

复制<br>
<br>如果搜索的是两张表里面的共有列会报错：列不明确。例如 select customer_id，order_id from orders JOIN customers on orders.customer_id = customers.customer_id 正确的写法应该是给 customer_id 加上明确的表名
<br>可以给表赋别名，但是赋别名之后就必须要用别名了，否则报错，就像这样 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240204232449.png" referrerpolicy="no-referrer">
<br><br>USE sql_hr;
SELECT e.employee_id,e.first_name,m.first_name as manger FROM employees e JOIN employees m ON e.reports_to=m.employee_id;
复制<br>
<br>mysql 看 21 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=21&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=21&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">4- 多表连接 | Joining Multiple Tables_哔哩哔哩_bilibili</a> ⏫ 📅 2024-02-06 ✅ 2024-02-06
<br><br>mysql 外连接有两种，一个是 left join  一个是 right join<br>
外连接也是，存在 left outer join 和 right out join 但是和 inner join 一样，这个 outer 可有可无 ==<br>
区别在于，join 是内连接，查询到的数据不满足条件的，不会被返回<br>
==而 left join 或者 right join  会返回所有的左边（右边）的表的数据<br>
举个例子<br>SELECT c.customer_id,c.first_name,o.order_id from customers c join orders o on c.customer_id = o.customer_id order by c.customer_id;
复制<br>可以看到，单个 join 查询返回的数据，如果没有满足 on 后面的条件，那么就会不会出现在返回数据里面，那如果我们使用 left join 呢？<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240206224340.png" referrerpolicy="no-referrer"><br>
可以看到，leftjoin 返回了左边表的所有的数据，即便是不满足条件（没有 orderid 与之对应）<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240206224647.png" referrerpolicy="no-referrer"><br><br> 多表外连接比较复杂<br>-- 查找客户，订单以及对应的托运人
-- 下面的写法，会导致显示不完整,原因是join 的写法，我们join shipper这个表的时候，出现了内连接的写法，我们应该换成left join
SELECT
	c.customer_id,
	c.first_name,
	o.order_id 
FROM
	customers c
	LEFT JOIN orders o ON c.customer_id = o.customer_id
	JOIN shippers sh ON o.shipper_id = sh.shipper_id;
复制<br>这是上面错误写法的返回结果 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207143319.png" referrerpolicy="no-referrer"><br>
而这是正确写法的返回 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207143405.png" referrerpolicy="no-referrer"><br><br>下面是隐式连接的写法，但是一旦忘记写 where 语句，那么，会造成交叉连接 <a data-href="#交叉连接（cross join）" href="\#交叉连接（cross_join）" class="internal-link" target="_self" rel="noopener">交叉连接（cross join）</a>，推荐使用显式连接法，这样有问题会报错<br>-- 显式语法
SELECT * from orders o JOIN customers c ON o.customer_id = c.customer_id;

-- 隐式连接
SELECT * from orders o,customers c WHERE o.customer_id = c.customer_id;
-- 会造成交叉连接，会出现异常多的数据查询
SELECT * from orders o,customers c ;
复制<br>
<br>sql 未完待续，下次看 P25-p35 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC?p=25&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC?p=25&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">8- 多表外连接 | Outer Join Between Multiple Tables</a> 📅 2024-02-07 ✅ 2024-02-07
<br><br>我们可以用简洁的写法 USING 关键字代替 join 后面的 ON 条件 <a data-href="#JOIN 关键字" href="\#JOIN_关键字" class="internal-link" target="_self" rel="noopener">JOIN 关键字</a>, 写法如下，如果 on 后面的条件是一样的字段名称，那么我们就能用 using 来直接表达，如果后面有多个条件，我们也可以在 using 后面的括号里面用逗号隔开<br>SELECT
	c.customer_id,
	c.first_name,
	o.order_id 
FROM
	customers c
	LEFT JOIN orders o 
	-- ON c.customer_id = o.customer_id
	USING (customer_id)
	LEFT JOIN shippers sh ON o.shipper_id = sh.shipper_id;
复制<br>但是注意<br>能在条件列名不同时候使用吗？
USING 关键字只能在对应表的列名称完全一样时使用
<br><br>自然连接指的就是，根据你 join 的两个表的相同名字的字段，数据库会自己看着办<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207150842.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207150944.png" referrerpolicy="no-referrer"><br>
但是这种连接会出现很多意外，不推荐使用，但是自己写可以写<br><br>
<br>什么是笛卡尔积--数据库<br>
交叉连接会出现笛卡尔积的形式<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207151653.png" referrerpolicy="no-referrer">
<br>实际的应用场景应该是你需要一些表数据的所有排列组合的时候才会用到 cross join<br>
交叉连接还有一种隐式写法，不需要显式的写出 cross join 如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207151953.png" referrerpolicy="no-referrer"><br>
去掉 cross join 改为 from 后面多写几个表名就行了 <a data-href="#隐式连接" href="\#隐式连接" class="internal-link" target="_self" rel="noopener">隐式连接</a><br><br>只需要给查询的数据前面加上数据库的名字就可以了<br><br>目前看是就是join多加了一个条件，需要在 where 语句后面进行连接上 and 就行<br><br>注意
你需要返回的列的数量一定要一致，否则就会报错，且列名是基于第一段查询的<br>
Unions 关键字是用来联合多个查询条件的<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240207153921.png" referrerpolicy="no-referrer">
<br>
<br>实际上只看到了 P 30 明天再看,要看到45p 📅 2024-02-08 ✅ 2024-02-08
<br><br>对以上学习进行一个小总结<br>
<br>在模糊查询中，% 表示任意长的字符，而 _ 表示单个字符
<br>正则关键字 regexp 可以直接替代模糊查询的 like 关键字，另外就是正则查询的一些写法，简单来说就是

<br>使用 ^xx 表示以 xx 开头 
<br>使用 xx$ 表示以 xx 结尾
<br>使用 a|b 表示或条件
<br>使用 [abcx]e 表示中括号里面的任意字母均可以出现在 e 前面，例如 ae、be、xe


<br>查询 null 的时候要用 is null 用= null 是搞不出来的，这个我在工作中犯过错
<br>order by 的排序能力很强，不仅仅能排一个列 <a data-href="#查询排序" href="\#查询排序" class="internal-link" target="_self" rel="noopener">查询排序</a>

<br>如果排两个条件，就先排前面的条件，后排后面的条件，而且条件不需要出现在查询的列里面
<br>他的排序条件甚至不需要是表里面的列，你 select 一个常数，用别名都能用来排序
<br>可以用 order by 1,2 这种语法来表示使用查询中的第一第二列作为排序条件


<br>limit 关键字的用法，除了 limit 6 这种单纯的限制返回数量，还可以做到 offerset 的效果，如 limit 3,10 ，相当于往后数三位后再限制返回 10 个
<br>JOIN 关键字，分为内连接 inner join 和两个外连接 left outer join 和 right outer join (inner 和 outer 都可以省略)，内连接和外连接最大的不同就是 <a data-href="#JOIN 关键字" href="\#JOIN_关键字" class="internal-link" target="_self" rel="noopener">JOIN 关键字</a>

<br>内连接如果没有满足的条件，就会不返回，
<br>外连接，以左连接为例，会返回所有的左边的表的信息，以及满足 on 后面条件的右边的表的信息, 外连接要注意多表外连接一些问题推荐只用 left join 
<br>隐式连接，直接省略了 join 的关键字，直接 select 两张表，加上 where 语句就能实现 join 的同样的效果，但是要注意可能会出现交叉查询的问题哦<a data-href="#交叉连接（cross join）" href="\#交叉连接（cross_join）" class="internal-link" target="_self" rel="noopener">交叉连接（cross join）</a>
<br>Using 关键字，用来替换 join 后面的 on 的条件的，这里可以在 USING 后面的括号里面加上逗号来表示多个条件
<br>自然连接，这个是交给 mysql 自己来判断怎么连接数据，一般是会根据表里面的相同的字段名来判断需要查询什么
<br>交叉连接，笛卡尔积的形式的查询，场景一般是用来查询所有的排列组合


<br>UNIONS 关键字，用来连接多个查询的关键字，但是要注意，查询的拼接要求查询的字段数量要一致，而且，字段的名称由第一个查询来确定的
<br>
<br>看完 p30 再做一遍总结，到时候刚好第三章看完 📅 2024-02-08 ✅ 2024-02-08
]]></description><link>2-领域\后端学习\mysql\基础\mysql查询.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/mysql查询.md</guid><pubDate>Thu, 28 Mar 2024 02:25:01 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240131212858.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240131212858.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[mysql的更新和删除]]></title><description><![CDATA[ 
 <br><br>
<br>varchar 和 char 的区别，varchar 代表了变化，如果在 mysql 的列中，字段是 varchar （50）但是你存储进去的字符串大小只有 5，那么 mysql 就会用直接存储这个 5 字符串大小的字符串，但是如果你的列属性是 CHAR(50), 那么 MySQL 会再插入 45 个字符来满足 CHAR（50）
<br>默认值，不给值会得到默认值
<br>not null 属性
<br>对于 mysql 来说，单引号包裹的字符串和双引号包裹的没区别
<br><br><br>-- 默认值插入，单引号和双引号没有区别
INSERT INTO customers
VALUES
	(
		DEFAULT,
		'john',
		'Smith',
		"1990-01-01",
		NULL,
		'address',
		'city',
		'CA',
		DEFAULT);
-- 或者你就直接insert into后面加列名，省略默认值就行
复制<br><br>-- value 后面逗号隔开就行了
insert into shippers (name)
values 
('shippers1'),
('shippers2'),
('shippers3')
复制<br><br>使用 mysql 里面内置的函数 LAST_INSERT_ID()，可以获取上一次我们插入的 id <br>SELECT*FROM orders;-- 模拟一次添加，1号顾客下单
INSERT INTO orders (customer_id,order_date,STATUS) VALUES (1,'2019-01-02',1);
SELECT LAST_INSERT_ID();
-- 使用内置函数LAST_INSERT_ID() 进行插入
INSERT INTO order_items VALUES (LAST_INSERT_ID(),1,1,2.95),(LAST_INSERT_ID(),2,1,3.95)
复制<br><br>创建表复制
我们可以很简单的用下面的语句创建一个表复制
-- 创建表复制
create table orders_archived as select * from orders;
复制
但是，需要注意的是，用这个方式创建表复制的时候，会丢失主键和自增的列属性<br>
另外就是我们创建了这张表之后，因为表的字段完全一致，后面我们可以使用一些类似下面的操作来插入表 <a data-href="#子查询" href="\#子查询" class="internal-link" target="_self" rel="noopener">子查询</a>
INSERT into orders_archived
SELECT * from orders where order_date &lt;'2019-01-01';
复制
<br><br>实际上就是插入或者更新的时候，插入的信息用 select 方式获取<br>
例如<br>UPDATE invoices 
SET payment_total = invoice_total * 0.5,
payment_date = due_date 
WHERE
	client_id = (
	SELECT
		client_id 
	FROM
		clients 
WHERE
	NAME = 'Myworks')
复制<br>注意上面的查询如果子查询的 select 是返回多个的话，我们就要用 update set xxx in  了，就是加上 in a 关键字<br><br>这里我们学到了 sql 语法中的更新操作，有以下几个部分需要注意<br>
<br>我们可以使用 DEFAULT 关键字表示插入默认的值
<br>我们可以用 LAST_INSERT_ID() 表示上一次插入的 id
<br>子查询，这个可以用到很多地方，就包括创建表复制
]]></description><link>2-领域\后端学习\mysql\基础\mysql的更新和删除.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/mysql的更新和删除.md</guid><pubDate>Thu, 28 Mar 2024 02:25:01 GMT</pubDate></item><item><title><![CDATA[mysql的执行流程]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240424013731.png" referrerpolicy="no-referrer"><br>简单分两块来说（server 层和储存引擎层）<br>
<br>Server 层负责建立连接、分析和执行 SQL。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
<br>存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。
<br><br>如果你在 Linux 操作系统里要使用 MySQL，那你第一步肯定是要先连接 MySQL 服务，然后才能执行 SQL 语句，普遍我们都是使用下面这条命令进行连接：<br># -h 指定 MySQL 服务得 IP 地址，如果是连接本地的 MySQL服务，可以不用这个参数；
# -u 指定用户名，管理员角色名为 root；
# -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里面输入密码
mysql -h$ip -u$user -p
复制<br>连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的，如果 MySQL 服务并没有启动，则会收到如下的报错：<br><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E8%BF%9E%E6%8E%A5%E9%94%99%E8%AF%AF.png" referrerpolicy="no-referrer"><br>如果 MySQL 服务正常运行，完成 TCP 连接的建立后，连接器就要开始验证你的用户名和密码，如果用户名或密码不对，就收到一个"Access denied for user"的错误，然后客户端程序结束执行。<br><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/%E5%AF%86%E7%A0%81%E9%94%99%E8%AF%AF.png" referrerpolicy="no-referrer"><br>如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断。<br>所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。<br>
<br>select 语句执行的时候发生了什么，未完待续
]]></description><link>2-领域\后端学习\mysql\基础\mysql的执行流程.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/mysql的执行流程.md</guid><pubDate>Mon, 29 Apr 2024 07:08:29 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240424013731.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240424013731.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[mysql之数据汇总]]></title><description><![CDATA[ 
 <br><br>一些计算函数<br>--  max()
--  min()
--  AVG()
--  SUM()
--  COUNT()
SELECT min(invoice_total) from invoices;
-- distinct 去重
SELECT
	max( invoice_total ) AS highest,
	min( invoice_total ) AS lowest,
	AVG( invoice_total ) AS average,
	SUM( invoice_total ) AS total,
	COUNT( DISTINCT client_id ) AS total_records 
FROM
	invoices 
WHERE
	invoice_date &gt; '2019-07-01';
复制<br>
<br>mysql 看到了 p41, 稍微休息一下，整理一下，然后看完到 45 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC?p=41&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC?p=41&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">【第五章】1- 聚合函数 | Aggregate Functions「汇总数据」_哔哩哔哩_bilibili</a> 📅 2024-02-18 ✅ 2024-02-18
<br><br><br>-- 查询表中所有数据的汇总
SELECT sum(invoice_total) as total_sales from invoices;
-- 查询表中所有数据的汇总,但是是按照client_id 一致的来汇总
SELECT client_id,sum(invoice_total) as total_sales from invoices GROUP BY client_id;
复制<br>默认情况下，表中数据是按照 group by 的列来进行排序的，但是可以使用 order by 来进行二次排序 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218225306.png" referrerpolicy="no-referrer"><br>语句执行过程
FROM&gt;WHERE&gt;GROUP BY &gt;SELECT &gt;ORDER BY<br>
所以 GROUP BY 语句永远要在 where 语句之后，例如下面的<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218225637.png" referrerpolicy="no-referrer">
<br><br>多列分组就是 GROUP BY 多列，这样做的意义就是将查询结果按照两列的排列组合进行分组<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218230429.png" referrerpolicy="no-referrer"><br>-- 按照state city的独一无二的排列进行聚合，配合sum进行汇总查询
SELECT state,city,sum(invoice_total) as total_sales from invoices i
JOIN clients USING (client_id)
GROUP BY state,city 
ORDER BY total_sales DESC;
复制<br>
<br> 
小练习
按照时间和方法来聚合搜索当天的账单，并按照时间排序
select date, pm. name as method, sum (amount) total_payments from payments 
join payment_methods pm  where payment_method = payment_method_id
GROUP BY date, payment_method
order by date;
复制


<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218231428.png" referrerpolicy="no-referrer"><br><br>多列分组的作用在于，会找到多列的所有的独一无二的排列做为分组依据，比如说，你想指定某个城市里面烧烤店的营业额，城市为一列，烧烤店为一列，你就会得到所有的存在的数据的排列组合<br><br>having 语句是用来干什么的？
是用来在 groupBy 之后进行筛选的
<br>
<br>mysql 看到 p43 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=43&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=43&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">3- HAVING子句 | The HAVING Clause_哔哩哔哩_bilibili</a> 📅 2024-02-22 ✅ 2024-02-22<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222232036.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222232721.png" referrerpolicy="no-referrer">
<br>SELECT  client_id, 
        sum(invoice_total) as total_sales,
        count(*) as number_of_invoices from invoices
GROUP BY 
    client_id
HAVING total_sales &gt;500 AND number_of_invoices&lt;6;
复制<br>
<br> 可以多条件排序，and 连接
<br> 但是 having 的条件必须在 select 里面存在，不能是别的字段，跟 where 不一样，where 可以使用任何列作为条件
<br><br>
<br>MySQL 44 课往后 <a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1UE41147KC/?p=44&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1UE41147KC/?p=44&amp;spm_id_from=pageDriver&amp;vd_source=eb319c6e317591be75da0554d1d79e3a" target="_blank">4- ROLLUP运算符 | The ROLLUP Operator_哔哩哔哩_bilibili</a> 📅 2024-02-23 ✅ 2024-02-27
功能：
获得一个汇总行


<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240222233505.png" referrerpolicy="no-referrer"><br>
<br> rollup 运算符号只能用于聚合值的列，比如这里的 sum 列
<br> 注意这个未必能用在其他的 sql，这个应该是 mysql 独有的
<br><br>
<br>
 首先是聚合函数，内置函数，协助查询，例如 sum min max

<br>
 GroupBy 关键字 

<br> group by 是用来分组的，一般来说都需要配合聚合函数使用
<br> group by 语句永远要在 where 后面，因为 where 先执行
<br> groupby 可以分组多列


<br>
 Having 关键字，这个充当的一个效果就是聚合函数后的 where 的作用，因为 where 没有办法过滤聚合函数的那一列，所以 having 就出现了，同样可以多列做条件

<br>
 注意这个关键字只能用 select 里面出现的条件作为条件，但是 where 可以任意列作为条件（只是 having 当 where 使用的场景下）类似这种，select 里面没有 total 的话就会报错 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227234107.png" referrerpolicy="no-referrer">

<br>
 补充上一条，内置的聚合函数是可以不用写在 select 列里面的如下两个<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240227233915.png" referrerpolicy="no-referrer">



<br>
 rollup 只要在聚合函数后面加上 rollup 就能获得一个汇总的列

]]></description><link>2-领域\后端学习\mysql\基础\mysql之数据汇总.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/基础/mysql之数据汇总.md</guid><pubDate>Thu, 28 Mar 2024 02:25:01 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218225306.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240218225306.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[索引]]></title><description><![CDATA[ 
 <br><br>索引实际上可以理解成目录，一种方便快速查询的目录。<br>
但是说到索引，就需要引出一些适合索引查询的数据结构，索引实现方式有很多种，一些特定的数据结构能更好的实现索引的功能，下面简单介绍三种：<br>
<br>哈希表

<br>哈希表是一种以 key-value 的形式存储数据的结构
<br> 其优势在于插入的时候很快，同 id 就是往后加链表，所以插入新数据很快
<br> 劣势在于因为不是有序的，所以做区间查询还是要遍历所有的 key，速度很慢
<br> 所以哈希表这种结构只适合于等值查询的场景，比如一些 nosql 引擎


<br>有序数组

<br>而有序数组在等值查询和区间查询的表现都非常优秀
<br>在等值查询的时候（就是用 key 查 value）可以直接二分法快速查到
<br>区间查询更是只要选定区间就能查询
<br> 但是，其缺点在于新增数据的时候，要整体移动插入数据后面的所有数据的位置，新增很慢
<br> 所以有序数组只适用于静态存储引擎，比如 2017 年某个城市人口信息这类不会再变动的数据


<br>二叉树

<br>二叉树的结构如图<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303163256.png" referrerpolicy="no-referrer">
<br>他的特点是节点是中间节点，也就是子节点，小于右边的叶子节点，大于左边的叶子节点，他的查询复杂度是 o(logN)，更具体的看<a data-href="平衡多路查找树（B树）" href="\2-领域\后端学习\java\平衡多路查找树（b树）.html" class="internal-link" target="_self" rel="noopener">平衡多路查找树（B树）</a>
<br>树可以二叉，也可以多叉，这个多叉是值每个节点有多个叶子节点，二叉树是搜索效率最高的数据结构
<br> 但是实际上大多数的数据库存储不使用二叉树，原因在于索引不止存在内存中，还要写到磁盘上

<br> 关于这点你可以想想一下，一颗一百万节点的平衡二叉树，树高 20 ，而机械硬盘时代，从磁盘随机读取一个数据块需要 10ms 的寻址时间，而 20 块就是 20 个 10ms，读取的效率过于低下 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303164041.png" referrerpolicy="no-referrer">


<br>所以实际上我们使用的是 N 叉树，这里的 N 取决于数据块的大小，以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。


<br>在 mysql 中，索引是在储存引擎层实现的，没有固定的索引标准，而 mysql 中用的最广泛的就是innoDB<br><br>在 innoDB 中，表是根据主键顺序以索引的形式存放的，这种存储方式成为索引组织表，而 InnoDB 采用了 B+树的模型，所以数据都是存放在 B+树中的<a data-href="平衡多路查找树（B树）#B+树" href="\2-领域\后端学习\java\平衡多路查找树（b树）.html#B+树" class="internal-link" target="_self" rel="noopener">平衡多路查找树（B树） &gt; B+树</a><br>每一个索引在 innoDB 里面对应着一颗 B+树<br><br>创建一个表，字段 k 上有索引<br>create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB;
复制<br>假设里面储存了多个数据（&gt;5）其中五个数据（R1-&gt;R5）如下，&nbsp;(100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，那么这两颗树示意图如下 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303194609.png" referrerpolicy="no-referrer"><br>
从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引<br>
<br>主键索引的叶子节点存的是整行数据，在 InnoDB 里面也被称为聚簇索引
<br>非主键索引的叶子节点内容是主键的值，在 innoDB 里面非主键索引也被成为二级索引<br>
由上我们就可以知道
<br>
<br> 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
<br> 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。<br>
所以我们在应用中要尽量使用主键查询
<br><br>B+树为了维护索引的有序性，在新插入值的时候需要做必要的维护<br>
所谓维护，就是指在插入数据的时候做出必要的维护而保证整体的数据结构的操作<br><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303194609.png" referrerpolicy="no-referrer"><br>
以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。<br>下面整段摘抄<a data-tooltip-position="top" aria-label="https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/MySQL%e5%ae%9e%e6%88%9845%e8%ae%b2/04%20%20%e6%b7%b1%e5%85%a5%e6%b5%85%e5%87%ba%e7%b4%a2%e5%bc%95%ef%bc%88%e4%b8%8a%ef%bc%89.md" rel="noopener" class="external-link" href="https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/MySQL%e5%ae%9e%e6%88%9845%e8%ae%b2/04%20%20%e6%b7%b1%e5%85%a5%e6%b5%85%e5%87%ba%e7%b4%a2%e5%bc%95%ef%bc%88%e4%b8%8a%ef%bc%89.md" target="_blank">04 深入浅出索引（上）</a><br>
而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。<br>
除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。<br>
当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。<br>
自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。
插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。
也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。
而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。
除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？
由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整型（bigint）则是 8 个字节。
显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。
所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。
<br>还有一个需要讨论的案例，有没有什么业务场景适用于业务字段直接做主键的呢？
比如有的业务场景是这样要求的<br>
<br>
1. 只有一个索引<br>
2. 该索引必须是唯一索引
这就是典型的 KV 场景，这时候我们就没有其他的索引储存 id 值从而出现上面的那种情况（指占用空间问题），所以不用考虑其他节点的叶子节点大小问题，而我们又有尽量使用主键查询的原则，直接将业务字段设置为主键，可以避免每次查询需要搜索两颗树
<br><br><br>我们先可以看一下这个问题<br>先创建一个表<br>create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; 
insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
复制<br>得到表结构如下<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303203705.png" referrerpolicy="no-referrer"><br>现在我要执行一个 sql 语句<br>	select * from T where k between 3 and 5
复制<br>需要执行几次树的搜索操作，会扫描多少行？
现在，我们一起来看看这条 SQL 查询语句的执行流程：

<br>在 k 索引树上找到 k=3 的记录，取得 ID = 300；
<br>再到 ID 索引树查到 ID=300 对应的 R3；
<br>在 k 索引树取下一个值 k=5，取得 ID=500；
<br>再回到 ID 索引树查到 ID=500 对应的 R4；
<br>在 k 索引树取下一个值 k=6，不满足条件，循环结束。

在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。
需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2
<br>那我们有没有办法经过索引的优化从而避免回表的过程呢？<br><br>如果我们执行的语句是 <br>select ID from T where k between 3 and 5;
复制<br>只需要查 id 值，而 id 的值已经在 k 索引树上了，因此我们可以直接提供查询结果，不需要回表，这里索引 K 已经"覆盖"了我们的查询需求，所以称之为覆盖索引<br>覆盖索引是一个常用的性能优化手段<br>所以联合索引的意义就出来了，举个例子<br>我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？<br>如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。<br>当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。<br><br>在真实的搜索场景中，我们有很多其他的场景，比如身份证和房屋啊，或者其他的，那如果要为每一种查询都设计一个索引又是不是太多了？尽管可能某个搜索频次不高，但是总不能全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？<br>但是实际上，B+树的索引结构，是可以利用索引的"最左前缀"，来定位记录<br>如下图<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303233102.png" referrerpolicy="no-referrer"><br>
可以看到是以姓名，年龄做的联合索引，可以看到索引是有顺序的，按照索引定义里面出现的字段排序的<br>当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。<br>如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是”where name like ‘张 %’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。<br>可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。<br>所以根据这个最左原则，我们需要合理的安排索引的创建，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。<br><br>还是以市民表的联合索引为例<br>select * from tuser where name like '张 %' and age=10 and ismale=1;
复制<br>这里根据上面的最左匹配原则我们可以知道，这个语句在搜索的过程中，只能用“张”找到第一个满足添加的记录 ID3, 然后我们就需要判断其他条件是否满足，<br>
<br>
在 mysql5.6 之前只能通过从 ID3 这个数据开始回表，然后一个个对比从主键索引上找出来的数据，进行字段的对比，找到 age= 10 并且 ismale =1 的数据

<br> 在 5.6 以前，查到的数据并不会对比联合索引中的 age 的值, 只是按顺序把 name 第一个字是张取出来回表。所以回表四次
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303234359.png" referrerpolicy="no-referrer">


<br>
在 mysql5.6 以后，出现了索引的下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

<br> 5.6 以后引入的索引下推优化，会开始看联合索引里面的 age 是否等于 10，对于不等于的值就直接跳过，也就是只需要回表 2 次
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303234410.png" referrerpolicy="no-referrer">


<br><br>
<br>本次我们学习到了索引模型，以及较为重要的二叉树-&gt; N 叉树的模型
<br>mysql 最多使用的引擎是 innoDB，里面的索引模型是 B+树
<br>在 InnoDB 中，每一个索引都是以一颗树的形式存储的，

<br>主键索引又叫群簇索引，他是储存整行数据的
<br>其他索引（非主键索引）里面储存的是 id 值

<br> 这里要注意的是 id 值在其他索引里面是占空间的，id 值过长的话，其他索引里面存储的空间就会变大，造成性能上的浪费，所以我们推荐自增形式的索引
<br>在 innoDB 里面也被称为 2 级索引
<br> 通过非主键索引搜索的时候，会先遍历这个对应的索引树，找到对应的 id，然后再遍历 id 索引树，这个称之为回表，所以我们推荐尽量使用主键索引




<br>某些业务场景 KV 场景，可以直接用业务字段做主键
<br>然后是索引的优化部分，一共三个知识点<br>
<br>覆盖索引

<br>即合适的联合索引是能够减少回表的次数的


<br>最左匹配原则

<br>这里的最左前缀原则是，操作数据库的时候，并不需要完全满足索引的所有字段，只要满足最左前缀原则，照样是可以利用到索引来加速检索的
<br>这里就考验架构师的构建索引的能力了


<br>索引下推

<br>指的是，在 mysql5.6 以后，可以根据联合索引里面的其他值进行索引下推（换句话说就是看得见联合索引的其他值了，之前都是视而不见的）减少了回表的次数


]]></description><link>2-领域\后端学习\mysql\原理\索引.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/原理/索引.md</guid><pubDate>Tue, 02 Apr 2024 14:41:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303163256.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240303163256.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[count关键字]]></title><description><![CDATA[ 
 <br><br>直接给结论 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312221044.png" referrerpolicy="no-referrer"><br><br>
<br> count 其实就可以理解成一个用来统计不为 null 的字段有多少个的一个函数
<br>如下，<br>select count(name) from t_order;
复制<br>这句话的意思就是统计这张表里面，name 字段不为 null 的记录有多少个<br>那么 count（1）呢？<br>select count(1) from t_order;
复制<br>1 其实就永远不为 null 的，所以这个意思就变成了，使得 1 不为 null 的字段有多少个，因为 1 永远不为 null，实际上是常数，所以，实际上这查询的是表里面所有记录的个数<br>
实际上，数字啥的常量都是一样的<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312221804.png" referrerpolicy="no-referrer"><br><br>
<br>Mysql 的 server 层是啥
<br>执行过程就是一个循环的读取过程，在通过 count 函数统计多少个记录的时候，<br>
<br>Mysql 的 server 层会维护一个叫 count 的变量
<br>server 层会循环向 innoDB 读取记录，如果 count 函数指定的参数不为 null，那么就会将 count 加一，最后返回 
<br>count (id) 的执行过程大致分为两种情况，索引看<a data-href="索引" href="\2-领域\后端学习\mysql\原理\索引.html" class="internal-link" target="_self" rel="noopener">索引</a><br>
<br>只有聚簇索引，也就是只有主键索引

<br> 如果只有主键索引的话，那么 server 层查询就会去查主键索引，循环遍历主键索引<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312223337.png" referrerpolicy="no-referrer">


<br>有二级索引，也就是非主键索引

<br> 那么InnoDB 循环遍历的对象就不是聚簇索引，而是二级索引<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312223437.png" referrerpolicy="no-referrer">


<br>原因在于，mysql 的索引，每个索引都是一颗 B+树 <a data-href="索引#InnoDB 的索引模型" href="\2-领域\后端学习\mysql\原理\索引.html#InnoDB_的索引模型" class="internal-link" target="_self" rel="noopener">索引 &gt; InnoDB 的索引模型</a>，主键索引里面储存的是当前行，而二级索引储存的是 id 值，所以二级索引更小，遍历二级索引的 IO 成本更低<br><br>与上面 count（主键）类似，也是 server 层循环查询对应的字段，但是相对于 count（主键），count（1）里面的数是常数，所以不需要判断数据是否为空，所以，执行过程不会读取任何字段的值，这就解释了为什么 count（1）的效率略高于 count（主键）<br>其余和主键索引类似，有二级索引遍历二级索引，没有就遍历主键索引<br><br>看到&nbsp;*&nbsp;这个字符的时候，是不是大家觉得是读取记录中的所有字段值？<br>对于&nbsp;selete *&nbsp;这条语句来说是这个意思，但是在 count(*) 中并不是这个意思。<br>count(*) 其实等于 count(0)，也就是说，当你使用 count(*) 时，MySQL 会将&nbsp;*&nbsp;参数转化为参数 0 来处理。<br>而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用key_len 最小的二级索引进行扫描。<br>只有当没有二级索引的时候，才会采用主键索引来进行统计。<br><br>这个查询方式的效率是最慢的<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312224537.png" referrerpolicy="no-referrer"><br>
可以看到，这是全表扫描的方式查的，（type 是 ALL 就说明是全表扫描 ）<br><br>
<br>Mysql 的另一个引擎 MyISAM 查询 count（*）会更快，时间复杂度只有 O（1）原因在于他维护了一个 meta 信息，储存了 row_count 的值，由表级锁来保证一致性
<br>而 InnoDB 是支持事务的，同一时刻的多个查询，由于多版本并发控制（MVCC）, innoDB 也不知道需要返回多少数据，所以只能循环遍历
<br>如果都带了 where 进行过滤，那两个查询 count 的方式就没区别了，都要扫描全表
<br><br>简单来说，使用 explain 语句来代替 count（*）就能得到近似值，因为 explain 语句不需要真正的查询，只是估计<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312225820.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312225829.png" referrerpolicy="no-referrer"><br>或者专门出一张表来维护所有的记录，每次插入删除的时候，都要操作这张表来记录对应的数据，这个比较麻烦<br><br>结合实际工作，我理解是这样<br>
<br> 首先是，如果需要计数，写在 sql 里面的话，尽量给表配上一个二级索引
<br> 如果一定要查某个字段不为 null 的个数的话，就给这个字段加上一个二级索引
]]></description><link>2-领域\后端学习\mysql\原理\count关键字.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/原理/count关键字.md</guid><pubDate>Fri, 29 Mar 2024 15:02:47 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312221044.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240312221044.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[orderby是怎么运作的]]></title><description><![CDATA[ 
 <br><br>我们在查询中经常会使用到关键字 order by 用来排序，那么他的底层原理是什么样的呢？<br>
我们可以举个例子来说明<br>首先创建一张表<br>CREATE TABLE 't' 
( 'id' int(11) NOT NULL, 
'city' varchar(16) NOT NULL, 
'name' varchar(16) NOT NULL, 
'age' int(11) NOT NULL, 
'addr' varchar(128) DEFAULT NULL, 
PRIMARY KEY ('id'), 
KEY 'city' ('city') ) ENGINE=InnoDB;
复制<br>这样就创建了一张城市姓名表，表中的主键是"id"列，该列的值是唯一的。同时还创建了一个"city"列的索引。<br>我们的 sql 这样写，利用了 city 的索引，避免了全表扫描<br>select city,name,age from t where city='杭州' order by name limit 1000 ;
复制<br>
<br> 使用 explain 命令查看这个语句的执行情况
<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240304234707.png" referrerpolicy="no-referrer"><br>主要看 extra 里面的字段<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240304234818.png" referrerpolicy="no-referrer"><br>
这个 Using filesort 表示的就是需要排序，Mysql 会给每个线程分配一块内存用于排序称之为，sort_buffer<br>下面是 city 的索引示意图<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240304235828.png" referrerpolicy="no-referrer"><br>
可以看到满足 city = '杭州' 的是 IDX 到 ID(X+N) 这些记录<br>通常情况下，语句执行流程如下<br>
<br>初始化 sort_buffer，确定放入 name、city、age 这三个字段；
<br>从索引 city 找到第一个满足 city=‘杭州’条件的主键 id，也就是图中的 ID_X；
<br>到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；
<br>从索引 city 取下一个记录的主键 id；
<br>重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
<br>对 sort_buffer 中的数据按照字段 name 做快速排序；
<br>按照排序结果取前 1000 行返回给客户端。
<br>暂且就把这个排序称之为全字段排序 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240305000721.png" referrerpolicy="no-referrer"><br><br>上面我们看到我们是在 sort_buffer 中按照 name 进行排序的，这个排序的动作，可能在内存中完成，也可能需要使用外部排序，这个取决于，排序所需的内存和参数 sort_buffer_size <br>sort_buffer_size 就是 mysql 为排序开辟的内存（sort_buffer）的大小，这个看需要排序的数据量<br>
<br> 如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。
<br> 但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。
<br>可以通过下面的方式确定一个排序语句是否使用了查询<br>/* 打开 optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on';
/* @a 保存 Innodb_rows_read 的初始值 */
select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 执行语句 */
select city, name,age from t where city='杭州' order by name limit 1000;
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G 
/* @b 保存 Innodb_rows_read 的当前值 */
select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 计算 Innodb_rows_read 差值 */
select @b-@a;

复制<br>
<br>未完待续 orderby, 没时间看，跳过，看后面的，grouBY 也跳过了，后面再看
]]></description><link>2-领域\后端学习\mysql\原理\orderby是怎么运作的.html</link><guid isPermaLink="false">2-领域/后端学习/Mysql/原理/orderby是怎么运作的.md</guid><pubDate>Thu, 28 Mar 2024 02:25:00 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240304234707.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240304234707.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[IO多路复用]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://juejin.cn/post/6882984260672847879" rel="noopener" class="external-link" href="https://juejin.cn/post/6882984260672847879" target="_blank">多路复用</a><br>简单来说就是，有I/O触发的时候，就会产生通知，收到了通知，再去处理通知对应的事件<br>
多路就是指的网络连接，复用就是说用的同一个线程<br>
<br>IO多路复用是一种同步IO模型，实现一个线程可以监听多个文件句柄
<br>一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；
<br>没有文件句柄就绪就会阻塞应用程序，交出CPU。
<br><br>服务器端采用单线程通过&nbsp;select/poll/epoll&nbsp;等系统调用获取 fd 列表，遍历有事件的 fd 进行&nbsp;accept/recv/send&nbsp;，使其能支持更多的并发连接请求。<br><br>
<br>select
<br>poll
<br>epoll
<br><br>首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用 就是为了解决这个问题而出现的。<br>针对IO多路复用，redis做了一层包装，叫Reactor模型<br>
本质就是监听各种事件，当事件发生的时候，就将事件分发给不同的处理器<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106221343.png" referrerpolicy="no-referrer">]]></description><link>2-领域\后端学习\redis\知识点\io多路复用.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/IO多路复用.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106221343.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106221343.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis 跳表]]></title><description><![CDATA[ 
 <br><br>
<br>是什么<br>
- 有序的多索引链表
<br>场景<br>
- ZSET
<br>特点

<br>有序；查询性能高


<br>数据结构<br>
<br>
<img alt="image-20231220001631362.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231220001631362.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240122230212.png" referrerpolicy="no-referrer">
<br>查找的话，先查二级索引，超过了要查的索引（比如查8，然后这时候跳到了10，那就跳回去从6往一级索引找），再往下查一级索引，一直定位到要查找的节点<br>插入和查询类似<br><br>跳表的表头结构<br>typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    //节点数量
    unsigned long length;
    int level;
} zskiplist;
复制<br>
<br>查询节点总数的复杂度<br>
看表头的length就可以知道，储存了节点总数，那么复杂度就是O(1) 
<br>跳跃列表的平均查找和插入时间复杂度都是O(logn) <a data-tooltip-position="top" aria-label="^39e5b0" data-href="#^39e5b0" href="\#^39e5b0" class="internal-link" target="_self" rel="noopener">插入数据复杂度</a>
<br>
标准的跳表有如下的限制：(Redis不是标准的跳表)

<br>里面的值（score分数）不能重复  一个节点两个值，一个分数，一个成员值  | redis里面的可以重复，如果分数重复，那么排序就按照成员的字典序进行排序
<br>只有向前指针，没有回退指针 | redis里面加了回退指针

<br><br>先说结论，插入数据平均复杂度是O(logN)，首先是有序集合无论是查找还是增加元素，都要定位到数据位置，所以跳表首先要进行定位，定位的方式跟查询类似，都是上级索引往下找找到下级索引，最坏情况下就是一直没有找到对应的索引，一直到最底层的索引才找到，复杂度是logN<br>
这里可以看层数<a data-href="#^75fe36" href="\#^75fe36" class="internal-link" target="_self" rel="noopener">^75fe36</a><br><br><img alt="image-20231220231836307.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231220231836307.png" referrerpolicy="no-referrer"><br><br> 层高的确定是比较随机的，能否增加一层的概率是25%，而Redis5.0最大层数限制是64，7.0是32层<br>
]]></description><link>2-领域\后端学习\redis\知识点\redis-跳表.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis 跳表.md</guid><pubDate>Tue, 20 Feb 2024 15:41:07 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231220001631362.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/image-20231220001631362.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis Hash]]></title><description><![CDATA[<a class="tag" href="?query=tag:RedisHashTable" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#RedisHashTable</a> 
 <br><br>
<br>
是什么
字典，无序<br>
key field都是String的hash表，储存在Redis中
他的底层是zipList 或者hashTable都是无序的

<br>
怎么操作

<br><img alt="image-20231219013243753" src="\lib\media\image-20231219013243753.png" referrerpolicy="no-referrer"><br>
<br>
hash对象编码

<br>
zipList（压缩列表）<a data-href="Redis List底层以及操作#^2bb21b" href="\2-领域\后端学习\redis\知识点\redis-list底层以及操作.html#^2bb21b" class="internal-link" target="_self" rel="noopener">Redis List底层以及操作 &gt; ^2bb21b</a> 	

<br>查询长度的时候，有俩字节的zllen，所以小于65535的时候为O(1),大于65535的时候是遍历的o(n)
<br>查询某一个key的时候必须要遍历，所以复杂度为o（n） 

如果hash对象保存的所有值和键的长度都小于64字节，并且总的对象元素个数小于512个就用ziplist，否则就是hashTable


对应到Hash里面的话，实际就是将field-value当作里面的entry放到ziplist里面，如下图，两两一组<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106171015.png" referrerpolicy="no-referrer"> 

<br>Hashtable<br>
<a href=".?query=tag:RedisHashTable" class="tag" target="_blank" rel="noopener">#RedisHashTable</a><br>
- hashTable在无序集合的Set里面也有应用<a data-href="Redis Set的操作以及底层#^35d9bd" href="\2-领域\后端学习\redis\知识点\redis-set的操作以及底层.html#^35d9bd" class="internal-link" target="_self" rel="noopener">Redis Set的操作以及底层 &gt; ^35d9bd</a><br>
- 和Set中的HashTable中不同的是，在Hash里面的hashTable有对应的值的，而set后面始终为null<br>
-<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106171735.png" referrerpolicy="no-referrer">


]]></description><link>2-领域\后端学习\redis\知识点\redis-hash.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis Hash.md</guid><pubDate>Tue, 20 Feb 2024 15:41:13 GMT</pubDate><enclosure url="lib\media\image-20231219013243753.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231219013243753.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis hashTable 底层以及操作]]></title><description><![CDATA[ 
 <br><br>
<br>
是什么

<br>
hash表，特点是O(1)的查找效率

<br>
数据结构

<br>
<img alt="image-20231219234355756" src="\lib\media\image-20231219234355756.png" referrerpolicy="no-referrer">

<br>
总的结构是dictht，里面有table、size、sizemask、used

<br>
table里面指向我们的dictEntry，里面是保存的信息

<br>
size是表示数组的大小

<br>
sizemask(也叫hash掩码)为size-1，使用于插入数据的时候，用与操作和插入值的hash值判断插入数据的位置

<br>插入数据的时候，先是通过hash函数计算出key的哈希值，然后与hash掩码做运算得到索引值，索引值就是这个数据在HashTable中的储存位置 


<br>
used表示的是使用了几个，比如上图只有0 2 有数据，1对应的值为null，所以used 为2

<br>
1 2后面是链表，出现hash冲突的时候会有链表结构





<br>// from Redis 5.0.52 
typedef struct dictht {
  dictEntry **table;
  unsigned long size;
  unsigned long sizemark;
  //键值对数量
  unsigned long used;
 } dictht;
复制<br><br>
<br>渐进式扩容

<br>

<br>新表h1和旧表h0


<br>渐进式扩容 
<br>从h0-&gt;h1扩容
<br>负载因子-扩容和缩容
<br>h1的空间是原表used的2倍的2次方幂<br>
-hashTable的扩容方式比较特殊，为了实现渐进式扩容，Redis中实际没有之间把dicthth暴露给上层，而是又做了一层封装 
<br>封装层如下图 dict

<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106183838.png" referrerpolicy="no-referrer"><br>
里面包含了两个hashTable结构，分别是 "ht[0]" 和 "ht[1]",这个指向的才是真正的表结构dictht


<br>如何触发扩容？

<br>每次往字典里面添加元素的时候，都会检查一下负载因子，一旦需要扩容就会触发渐进式扩容，这里提到了新的知识点，<a data-tooltip-position="top" aria-label="^5c3450" data-href="#^5c3450" href="\#^5c3450" class="internal-link" target="_self" rel="noopener">扩容时机</a>


<br>渐进式扩容流程（大致流程）<br>
1. 首先式为新的HashTable1 分配空间，新表的大小取原表的两倍大小向上取2的N次方幂），比如原来的表大小是500，那么新表就是取1000的向上取整的最接近的2次方幂1024<br>
2. 然后就是将封装的dict里面的rehashidx从静默状态（-1）变成0，这就代表着rehash工作正式开始<br>
3. 然后就是把旧数据迁移到新表，而且同时，每次又触发了增删改查的操作的时候，程序也会顺带迁移表0上的数据，随着迁移的进行，最终，数据会全部迁移到表【1】上，这个时候，就换名字，将ht[1]和ht[0]指针对象互换，同时把偏移索引l的值设为-1，表示Rehash操作已完成。这个事情也是在Rehash函数做的，每次迁移完一个元素，会检查下是否完成了整个迁移：<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106190503.png" referrerpolicy="no-referrer"><br>



<br><br>
<br>负载因子

<br>

<br>Redis提出了一个负载因子的概念，负载因子表示目前Redis HASHTABLE的负载情况，是游刃有余，还是不堪重负了。我们设负载因子为k，那么k=ht[0].used/ht[0].size，也就是使用空间和总空间大小的比例。Redis会根据负载因子的情况来扩容：

<br>负载因子大于等于1，说明此时空间已经非常紧张。新数据是在链表上叠加的，越来越多的数据其实无法在0(1)时间复杂度找到，还需要遍历一次链表，如果此时服务器没有执行BGSAVE或BGREWRITEAOF这两个命令，就会发生扩容。复制命令对Redis的影响我们后面在原理篇再讲。 
<br>负载因子大于5，这时候说明HASHTABLE真的已经不堪重负了，此时即使是有复制命令在进行，也要进行Rehash扩容。






]]></description><link>2-领域\后端学习\redis\知识点\redis-hashtable-底层以及操作.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis hashTable 底层以及操作.md</guid><pubDate>Tue, 20 Feb 2024 15:41:18 GMT</pubDate><enclosure url="lib\media\image-20231219234355756.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231219234355756.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis List底层以及操作]]></title><description><![CDATA[ 
 <br><br>
<br>是什么：list
<br>怎么用：

<br>创建 lpush rpush（分别是左边开始往里push数据，和右边开始往里push数据）
<br>查询 llen（查有多少个） lrange（lrange key start stop 查看start到stop为角标的元素，如果是负数，就从后面开始查）
<br>更新 lpush rpush lpop（从左边开始pop） rpop（同理） lrem（从左边开始remove）
<br>删除 DEl UNLINK


<br><img alt="image-20231217222008380" src="\lib\media\image-20231217222008380.png" referrerpolicy="no-referrer"><br><br>list 对象编码：<br>
<br>
ziplist 

<br>要满足条件采用ziplist编码：

<br>列表对象保存的所有字符串长度都小于64字节
<br>列表对象元素个数少于512个




<br>
linkedlist

<br>
quickList(综合了上面的两个实现，双向链表，内部是ziplist)目前5.0.5全是由quickList实现
linkedlist 表头定义了len，所以查询长度的时间复杂度是0（1）

<br>
list由  结构头（header）{里面有结构 zllen记录了节点的数量，但是如果节点大于65535,那就需要遍历了，因为zllen是2字节的，只能存这么多}   和 数据字段组成

<br><br>- 是什么：内存紧凑的列表，适合小数据量
- 场景：list set，小数据量
- 优点：内存占用小
- 缺点是，更新的时候，里面的所有数据都要移动，性能成本高
复制<br>外面是表头，数据指针指向的就是ziplist实际的部分了<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105002312.png" referrerpolicy="no-referrer"><br><img alt="image-20231217232526448" src="\lib\media\image-20231217232526448.png" referrerpolicy="no-referrer"><br>zlend表示list的终结 二进制的8个1组成，代表255<br>zllen 为两个字节长度，一般查数据的的时候，zllen里面可以存 2^16次方减一位数，超过了就要遍历，所以一般情况下，查询节点个数的时间复杂度为O(1),超过一定阈值，则查询节点的复杂度就变成O(n) <br>entry表示的是实际的数据，<br>
entry 里面结构： prevlen encoding entry-data,分别表示，记录的前面entry的长度、编码、实际数据
<br>
<br>连锁更新
<br>
连锁更新就涉及到上面这个prevlen了，这里记录了上一个节点的数据长度，如果前一个节点长度小于254字节，那么prev就用1字节长的空间保存这个长度，否则就用5字节的空间保存这个长度
这里就可能触发连锁更新，即prevlen更新长度以后，使得本节点长度上涨了4位，再使得后面的节点再更新prevlen
<br><img alt="image-20231219010107838" src="\lib\media\image-20231219010107838.png" referrerpolicy="no-referrer"><br>
<br>
怎么解决连锁更新

<br>
listpack
结构变成了 



<br><img alt="image-20231219010259819" src="\lib\media\image-20231219010259819.png" referrerpolicy="no-referrer"><br>encoding-type是编码类型<br>element-data是数据内容<br>element-tot-len存储整个节点除它自身之外的长度<br>
这样，可以通过 element-tot-len 来计算节点长度，从而不用 prevlen 来保存前一个节点的长度<br>zipList的更新<br>
由于ziplist底层是偏向数组的结构，里面的entry为了节约内存都是相邻的，而且整个ziplist的内存是统一分配的，所以插入数据的时候，主要有两件事<br>
1. 为整个ziplist重新分配空间，主要是为了给新节点分配空间（这一步如果ziplist刚好没有多余空间，那么我们就要把ziplist整个迁移到别的地方去）<br>
2. 插入节点,插入的数据后面的节点都要往后移动，腾出空间给新节点<br>
<br>
所以，ziplist在数据较多的时候（没达到转换成linkedList的标准前），会复制较多的空间，导致很多内存复制 <br><br>
如果不满足zipList的条件 <a data-href="#^5f2ba2" href="\#^5f2ba2" class="internal-link" target="_self" rel="noopener">^5f2ba2</a>，则使用linkedList 的编码
<br>格式如下<br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105002807.png" referrerpolicy="no-referrer"><br>而且，linkedList里面的表头定义了链表包含节点数量的字段 len<br>
结构如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105005306.png" referrerpolicy="no-referrer"><br>
所以在linkedList的编码下，查询节点个数的复杂度是O(1) <br><br>上面两个list都各有缺点，ziplist 在数据插入稍多的时候插入数据会导致很多的内存复制（原因）<a data-href="#^228531" href="\#^228531" class="internal-link" target="_self" rel="noopener">^228531</a> ，linkedList表稍微大一点，节点数量就很多，会导致不少的内存占用，所以，在 3.2版本以后，就引入了quickList，他是集二者优点之数据结构<br>他的数据结构如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105004503.png" referrerpolicy="no-referrer"><br>以linkedList的结构，节点里面储存的是ziplist，当数据量比较少的时候，他只有一个节点，相当于一个ziplist，数据很多的时候则是同时拥有了ziplist和linkedlist的优点<br>
1. 数据上节约内存<br>
2. 更新提高更新效率]]></description><link>2-领域\后端学习\redis\知识点\redis-list底层以及操作.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis List底层以及操作.md</guid><pubDate>Tue, 20 Feb 2024 15:41:21 GMT</pubDate><enclosure url="lib\media\image-20231217222008380.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231217222008380.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis Set的操作以及底层]]></title><description><![CDATA[<a class="tag" href="?query=tag:RedisHashTable" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#RedisHashTable</a> 
 <br><br>
<br>
是什么 
无序的集合

<br>
场景 
去重场景

<br>
怎么用

<br>创建

<br>SADD 


<br>查询

<br>SISMEMBR : 查询是否为成员
<br>SMEMBRS : 返回集合中所有成员的列表 
<br>SCCARD： 返回集合中成员个数
<br>SSCAN : 遍历，后面跟下标和count，还可以用match进行模糊查询
<br>SINTER : 以前面的集合判断交集
<br>SUNION：并集
<br>SDIFF：different 多个set的去重，以前面的为基准


<br>更新

<br>SADD
<br>SREM：删除


<br>删除

<br>DEL




<br><br>Set的编码方式：<br>
1. intSet<br>
2. HashTable <br><img alt="image-20231219012529951" src="\lib\media\image-20231219012529951.png" referrerpolicy="no-referrer"><br>
<br>hashTable 的特点，就是能在O(1)的复杂度下查找到需要的数据
<br>intset更节约内存 

<br>如果集群元素都是整数，且元素数量不超过512个，就采用inset编码，


<br>intSet 结构如下图，结构紧凑，内存占用少，但是需要查询的时候，要二分查询, 也就是时间复杂度是 O（logN）<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105010432.png" referrerpolicy="no-referrer"><br><a href=".?query=tag:RedisHashTable" class="tag" target="_blank" rel="noopener">#RedisHashTable</a> <a data-href="Redis hashTable 底层以及操作" href="\2-领域\后端学习\redis\知识点\redis-hashtable-底层以及操作.html" class="internal-link" target="_self" rel="noopener">Redis hashTable 底层以及操作</a><br>如果不满足intSet的条件，就要用HashTable，结构如下图，hashTable查询元素性能很高，O(1)时间就能查到一个数据是否存在<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240105010603.png" referrerpolicy="no-referrer"> ]]></description><link>2-领域\后端学习\redis\知识点\redis-set的操作以及底层.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis Set的操作以及底层.md</guid><pubDate>Tue, 20 Feb 2024 15:41:26 GMT</pubDate><enclosure url="lib\media\image-20231219012529951.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231219012529951.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis ZSET]]></title><description><![CDATA[<a class="tag" href="?query=tag:RedisHashTable" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#RedisHashTable</a> <a class="tag" href="?query=tag:Redis跳表" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Redis跳表</a> 
 <br><br>
<br>
是什么
有序集合，也叫SortedSet ,存储的分数是抽象的分数，任何指标都能抽象成分数

<br>
场景
排行榜

<br>
怎么用

<br>
<img alt="image-20231220232953541" src="\lib\media\image-20231220232953541.png" referrerpolicy="no-referrer">

<br>简单看一些增删改查的场景<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106211735.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106212228.png" referrerpolicy="no-referrer"><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240106212304.png" referrerpolicy="no-referrer"> <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240122233906.png" referrerpolicy="no-referrer"><br>
<br>底层结构 

<br>一个是ziplist  <a data-tooltip-position="top" aria-label="Redis List底层以及操作 > ^2bb21b" data-href="Redis List底层以及操作#^2bb21b" href="\2-领域\后端学习\redis\知识点\redis-list底层以及操作.html#^2bb21b" class="internal-link" target="_self" rel="noopener">ziplist</a>
<br>一个是skiplist（跳表） <a data-href="Redis 跳表" href="\2-领域\后端学习\redis\知识点\redis-跳表.html" class="internal-link" target="_self" rel="noopener">Redis 跳表</a> + hashTable


<br><img alt="image-20231221002451815" src="\lib\media\image-20231221002451815.png" referrerpolicy="no-referrer"><br>只有上面的两个条件同时满足才会用ziplist<br>而第二种结构如下(跳表和HashTable配合使用)<br>
为什么配合使用？<br>
<a href=".?query=tag:RedisHashTable" class="tag" target="_blank" rel="noopener">#RedisHashTable</a> <a href=".?query=tag:Redis跳表" class="tag" target="_blank" rel="noopener">#Redis跳表</a><br>
当Zset要根据成员来找分数的时候，使用字典实现，时间复杂度就是O(1)，当Zset执行范围操作的时候，比如ZRANK、ZRANGE，就用原本有序的跳表来实现<a data-href="Redis 跳表" href="\2-领域\后端学习\redis\知识点\redis-跳表.html" class="internal-link" target="_self" rel="noopener">Redis 跳表</a><br>
<br>
<img alt="image-20231221002636300" src="\lib\media\image-20231221002636300.png" referrerpolicy="no-referrer"> <br>上面的跳表提供了跳表范围查询等能力，hashtable 则是提供了 O(1)复杂度的查询分数的能力<br>这里的搭配是指，使用 hashTable 储存 member 值对应的 score 值，如果只要得到对应的 score 值，只需要 zcore key member 直接快速得到 score 值，而 skiplist 是有序的排列，所以能快速的按照 score 值进行查找，删除，插入的操作，搭配使用就比如先通过 hashTable 得到 score 值，然后在 skiplist 中查找 score 区间元素]]></description><link>2-领域\后端学习\redis\知识点\redis-zset.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis ZSet.md</guid><pubDate>Tue, 20 Feb 2024 15:41:29 GMT</pubDate><enclosure url="lib\media\image-20231220232953541.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231220232953541.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis持久化]]></title><description><![CDATA[ 
 <br><br>持久化策略，两个策略<br>
<br>RDB快照（整点切片），通过切片还原数据
<br>AOF日志：记录日志，可以通过命令恢复
<br>差别<br>
​相同数据量下，RDB更小，因为RDB记录二进制紧凑型数据库<br>
​AOF记录了每条日志，RDB是间隔时间记录一次，AOF恢复数据通常更为完整，ADB更快，<br>
AOF对性能影响比较大，AOF和RDB是可以同时开启的，官网更推荐AOF <br><br>
<br><img alt="image-20231226001133134" src="\lib\media\image-20231226001133134.png" referrerpolicy="no-referrer"><br>

<br>先判断AOF是否存在，而且不会降级（如果开启了AOF，AOF文件不存在也不会去找RDB），aof文件不存在就会启动空库，有aof只会用aof<br><br>​	dump.rdb -- 快照文件<br>
<br>
怎么开启

<br>
save 900 1（save min times）

<br>
900 秒里面有一次就会开启一次 RDB

<br>
也可以使用 sava 命令来主动使用持久化
  <img alt="image-20231226234407734" src="\lib\media\image-20231226234407734.png" referrerpolicy="no-referrer">
  后台持久化，会fork一个子进程开始复制

<br>
文件名字 ： dump.rdb

<br>
存放位置：看redis



<br><br>
<br>Fork 一个子进程专门来做 RDB 持久化
<br>子进程写入数据到临时的 RDB 文件
<br>写完之后用新的 RDB 文件替换旧的 RDB 文件
<br><br>如果用的 save 的命令。那么就会阻塞主进程，如果用的 bgsave 命令，那么用的就是从主进程 fork 的一个子线程，用子进程去开始复制，那么这里就有问题，就是子进程在执行的过程中，主进程还能修改数据吗？如果阻塞岂不是跟 save 命令没什么区别，还是会主动阻塞进程？<br>答案是 主进程当然可以写入数据，依然可以继续执行操作命令<br>
执行 bgsave 命令的时候，会通过 fork（）创建子进程（注意是进程不是线程），子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，方便子进程去复制数据 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125145632.png" referrerpolicy="no-referrer"><br>
但是当发生内存修改的时候，这块被修改的物理内存就会被复制一份，如下 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240125150053.png" referrerpolicy="no-referrer"> 子进程指向就变成了新的物理内存，然后主进程继续自己的写入，子进程继续自己的复制，发生了写时复制后，RDB 快照保存的是原本的内存数据，而主线程刚修改的数据，是没办法在这一时间写入 RDB 文件的，只能交由下一次的 bgsave 快照。<br>那么极端情况下，如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。<br>
所以，针对写操作多的场景，我们要留意下快照过程中内存的变化，防止内存被占满了<br>写时复制：写的时候就开始复制，谁写谁复制，fork的过程中如果有新的数据，那就这个数据自己多写一份放到线程里面<br><br>
<br>
RDB本质是什么<br>
​	本质是二进制形式的快照，直接保存二进制的数据到磁盘，后续通过加载RDB文件恢复数据<br>


<br>
如何触发RDB 

<br>RDB可以通过配置定时触发，触发时使用的是后台持久化方式
<br>也可以主动用命令触发，save和bgsave，save底层用的是阻塞式持久化，bgsave用的是后台持久化
<br>最后如果redis正常关闭，是会触发阻塞式持久化的


<br>
RDB对主流程有什么影响 

<br>用命令执行阻塞式持久化的时候，由主进程进行RDB快照保存，会阻塞主进程
<br>当执行后台持久化时，由fork出来的子进程来进行RDB快照保存

<br>数据量大的时候，会导致fork主进程操作比较耗时，从而阻塞主进程（耗时会阻塞主进程吗？进程之间阻塞？）
<br>由于采用了写时复制，如果在fork期间有大量写入，就会导致主线程多拷贝一份数据，消耗大量额外内存




<br><br>
<br>
是什么<br>
- 记录了对应操作的命令，有自己的一套协议记录（但是只记录写操作，读操作是没有意义的）

<br>
怎么开启<br>
-  也是redis的配置文件，里面有一个appendonly no 改成yes<br>
-  写入的文件名字就是 appendfilename “xxxx”

<br>
写入？<br>
- 流程：请求到来-&gt;处理请求-&gt; 写入AOF文件 

<br>其中写入 AOF文件又有三个步骤<br>
- 命令追加（写入aof_buf）<br>
- 将数据写入到AOF缓存中，实际上就是把日志写到一个sds数据<br>
- 文件写入（write 到内核缓冲区）<br>
- 将上一步aof_buf里面的数据里面对应的数据刷入磁盘缓冲区，那么我们什么时候刷入呢？<br>
- 事实上有四个时机刷入，会调用一个叫flushAppendOnlyFile的函数，将数据写入系统缓冲区<br>
1. 处理完时机，等待下一次事件到来之前，这就是在 beforeSleep中<br>
2. 周期函数serverCron中<br>
3. 服务器正常退出的时候<br>
4. 通过配置指令关闭AOF功能的时候<br>
- 文件同步（fsync到磁盘）<br>
- 刷盘策略（当我们设置的刷盘策略（appendfsync）不同，刷盘的情况也不同）<br>
1. always 每次请求都刷盘，非常慢，但是很安全，性能开销很大，而且这玩意是直接主线程刷盘的<br>
2. everySec 每秒刷一次，足够快了，但是可能丢失一秒的数据<br>
3. appendfsync no 不主动刷盘，让操作系统自己刷，一半linux30秒刷一次，对性能影响比较小，但是如果崩溃，可能会丢失比较多的数据 


<br>
aof重写（优化aof里面的重复数据，比如set a1 1 然后覆盖了 set a1 2,就会合成一个） 

<br>为什么要重写<br>
-因为AOF是不断写入的，这样带来一个问题就是AOF文件会变得非常大，无限制的膨胀，针对这个问题，Redis采用了重写的方式来解决问题
<br>怎么做？<br>
- <img alt="image-20231227001640372" src="\lib\media\image-20231227001640372.png" referrerpolicy="no-referrer"> 

<br>当redis发现AOF文件过大的时候，就会在后台Fork一个子进程（进程不是线程，Redis是单线程的），专门针对AOF进行重写，简单来说就是合并一些重复的操作，比如set同一个key之类的
<br>重写的过程中，Redis不但将新的操作记录在原有的AOF缓冲区<a data-tooltip-position="top" aria-label="^0a9ef9" data-href="#^0a9ef9" href="\#^0a9ef9" class="internal-link" target="_self" rel="noopener">看里面的命令追加那一块</a>，而且同时会记录在AOF的重写缓冲区，一旦新的AOF重写好了，Redis就会把缓冲区的直接追加到新的文件里面，然后用新文件替换旧文件


<br>什么时候重写?<br>
-配置决定的，默认情况下是，超过64M的情况下，相比上次重写时的数据大一倍，则触发重写，很明显，最后实际上还是在周期函数来检查和触发的。


<br><br>​aof不是默认开启，rdb是默认开启的<br>aof对主流程有什么影响？<br>
1. 如果我们使用的是 always 的策略，Redis执行命令之后，就会需要主线程进行write+fsync的操作<a data-href="#^852a5e" href="\#^852a5e" class="internal-link" target="_self" rel="noopener">^852a5e</a>，导致主线程处理其他请求会变得很慢<br>
2. 如果策略用的是everySec，如果后台线程上一轮的fsync没有完成，会导致我们本轮主线程执行write被阻塞<br>
3. AOF重写是由fork出的子进程进行的，类似于上面提到的风险，fork子进程这个操作有可能阻塞主进程 <br><br>
<br>
aof的不足

<br>文件体积大，加载速度慢
<br>重写性能差，重写流程复杂，代码难度高


<br>
混合持久化（redis配置文件中打开） 

<br>混合部署实际上发生在AOF的重写阶段
<br>是什么？

<br>重写发生时候，将当前状态保存为RDB二进制内容，把这个RDB的内容写入到新的AOF文件中，然后跟前面重写流程一样会把缓存区的内容追加进来，最后代替原有的AOF文件<a data-tooltip-position="top" aria-label="^a8ab5a" data-href="#^a8ab5a" href="\#^a8ab5a" class="internal-link" target="_self" rel="noopener">重写流程</a>


<br>解决了什么问题？

<br>优化了重写的流程，RDB是快照 <a data-tooltip-position="top" aria-label="^1010b3" data-href="#^1010b3" href="\#^1010b3" class="internal-link" target="_self" rel="noopener">RDB本质</a>，使得重写更快，大大降低了AOF重写的性能损耗


<br>怎么开启？

<br>打开redis配置文件，5.0之后是默认打开的


<br>实际流程是什么样子？<br>
-<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107194100.png" referrerpolicy="no-referrer"><br>
前面说过，如果同时开启了RDB和AOF，那么优先会去找AOF文件  <a data-tooltip-position="top" aria-label="^7ad171" data-href="#^7ad171" href="\#^7ad171" class="internal-link" target="_self" rel="noopener">加载策略</a>，那么这里如果开启了混合持久化，也会去优先找混合持久化的文件


<br>
优化方案

<br><img alt="image-20231227003741271" src="\lib\media\image-20231227003741271.png" referrerpolicy="no-referrer"><br>优化之一,下面去掉了重写缓冲区，去掉了子线程合入两个文件，直接两个文件放一起，由manifest清单（7.0的优化)<br><img alt="image-20231227004031822" src="\lib\media\image-20231227004031822.png" referrerpolicy="no-referrer">]]></description><link>2-领域\后端学习\redis\知识点\redis持久化.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis持久化.md</guid><pubDate>Tue, 20 Feb 2024 15:41:33 GMT</pubDate><enclosure url="lib\media\image-20231226001133134.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231226001133134.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis单线程，还是多线程？]]></title><description><![CDATA[ 
 <br>核心处理逻辑一直都是单线程，但是有其他模块比如UNLINK 之类的异步删除操作用的多线程，而6.0以后 网络I/O解包都是多线程<br><br>6.0后有多线程的IO<br>
<br>
为什么用单线程 

<br>快，节约成本  
<br>redis的性能瓶颈在于IO而不是cpu
<br>一旦引入多线程，复杂性会大大提升，redis就要实现线程安全，做各种优化，包括上下文切的额外成本（内核成本）


<br>
为什么单线程还这么快<br>
redis单线程的性能很好，在普通机器上能有每秒十多万的读性能，几万的写性能 

<br>首先redis是内存数据库，其大部分操作在内存上完成，内存操作本身就特别快
<br>redis有高效的数据结构
<br><a data-href="IO多路复用#Redis对多路复用的使用" href="\2-领域\后端学习\redis\知识点\io多路复用.html#Redis对多路复用的使用" class="internal-link" target="_self" rel="noopener">IO多路复用 &gt; Redis对多路复用的使用</a>，使得其能在IO操作中能并发的处理大量的客户端请求，实现高吞吐量


<br><br>
<br>
读取数据

<br>accept 请求发给redis
<br>recv redis准备好了以后返回，就可以开始下一步
<br>parse 解析数据


<br>
处理数据
获取一个key

<br>
发送返回包
send

<br>
<br>redis的读性能能达到10W/s
<br>写也能有七八万/s
]]></description><link>2-领域\后端学习\redis\知识点\redis单线程，还是多线程？.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis单线程，还是多线程？.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate></item><item><title><![CDATA[Redis内存淘汰策略]]></title><description><![CDATA[ 
 <br><br><br>​	<img alt="image-20231222075727013" src="\lib\media\image-20231222075727013.png" referrerpolicy="no-referrer"><br>不开启淘汰策略内存满了就拒绝新的数据<br><br><br>
淘汰最近最久未使用的数据，latest Recently Used
<br>成本：双链表，巨大的内存损耗<br>redis选择近似LRU算法<br><img alt="image-20231222080207919" src="\lib\media\image-20231222080207919.png" referrerpolicy="no-referrer"><br>LRU的不足<br> 他是用最近访问时间去淘汰内存的而忽略的频率，有时候会出现经常访问的一个数据和偶尔才被访问的数据在一起，反而经常访问的数据被淘汰了,所以有新的淘汰算法  LFU<br><img alt="image-20231222080456083" src="\lib\media\image-20231222080456083.png" referrerpolicy="no-referrer"><br><img alt="image-20231222080551943" src="\lib\media\image-20231222080551943.png" referrerpolicy="no-referrer"><br><br><img alt="image-20231222080735687" src="\lib\media\image-20231222080735687.png" referrerpolicy="no-referrer"><br>在淘汰时候，他会去检测一下上次的访问和最近访问的时间的衰减再去淘汰<br><br>内存回收什么时候发起<br>​	每次读写的时候，都会调用ProcessCommand函数，processCommand又会调用freememoryIfNeed，满了就会尝试释放内存<br><br><img alt="image-20231222081538376" src="\lib\media\image-20231222081538376.png" referrerpolicy="no-referrer"><br>dict(字典中存储数据)<br><img alt="image-20231222081708058" src="\lib\media\image-20231222081708058.png" referrerpolicy="no-referrer"><br>  过期时间戳<br><img alt="image-20231222081807472" src="\lib\media\image-20231222081807472.png" referrerpolicy="no-referrer">]]></description><link>2-领域\后端学习\redis\知识点\redis内存淘汰策略.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/Redis内存淘汰策略.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="lib\media\image-20231222075727013.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231222075727013.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[RedisString的编码以及操作]]></title><description><![CDATA[ 
 <br><br>
<br>
是什么：字符串

<br>
场景：Json，二进制，数字字符串

<br>
限制，最大 512 M

<br>
怎么用：

<br>
创建：set  setnx (数据已经存在就不会覆盖)

<br>
查询: get mget（获取一批）

<br>
更新: set

<br>
删除: del 

<br>
Set  strniuniu ex 10 (设置 string 的同时加上过期时间)
<img alt="image-20231217210935567" src="\lib\media\image-20231217210935567.png" referrerpolicy="no-referrer">



<br><br>String 的操作几乎是最常用的了，所以必须要掌握，多写几个例子吧<br>
首先就是最重要的 Set 命令<br>String 的创建和更新都用到 Set<br>127.0.0.1：6379 &gt; SET straaa cat
ok
复制<br>除了上面的简单写法，Set 命令还有扩展，<br>
1. EX second: 设置键值的过期时间为多少秒<br>
2. PX millisecond: 设置键值过期时间为多少毫秒<br>
3. NX: 只在键不存在的时候才对键进行设置操作，SET key value NX 效果等同于 SETNX key value 😀<br>
4. XX: 只在键值存在时候，才对键值进行设置操作 <br>下面是一些简单的代码展示<br>127.0.0.1：6379 &gt;SET straaa fish NX
null
127.0.0.1：6379 &gt;SET straa fish 
OK
127.0.0.1:6379&gt; set straaa fish
OK
127.0.0.1:6379&gt; set straaa cat nx
(nil)
127.0.0.1:6379&gt; setnx straa cat
(integer) 1
127.0.0.1:6379&gt; setnx straa cat
(integer)0
127.0.0.1:6379&gt;
复制<br><br>​	string 对象编码有三种<br>&gt; redis字符串对象是由redisObject和sdshdr两部分组成的
复制<br>
<br>
INT
  数字之类的用 INT，占空间小（只针对整形）

<br>
EMBSTR
字符串长度比较小的时候用这个编码，有点是，可以一次性分配内存，字节头和内容是在一起的

缺点在于，如果重新分配空间，整体都需要再分配，所以 EMBSTR 被设置为只读，任何写操作以后，EMBSTR 都会变成 RAW 格式，因为写操作的字符都会被认为是易变的，易变的就用 raw 更方便更新


<br>
RAW
sds和RedisObject是分开的，使用在长的字符串上，字节头和内容是分开的（有个阈值，超过阈值就用这个来存储字符串，目前比较新版本是 44（5.0.5））

<br><img alt="image-20231217211908559" src="app:\\a4c6d72d640b09303e9906b3e15645db826c\C:\\Users\92502\AppData\Roaming\Typora\typora-user-images\image-20231217211908559.png" referrerpolicy="no-referrer"><br><br>​	<img alt="image-20231217212109893" src="\lib\media\image-20231217212109893.png" referrerpolicy="no-referrer"><br>
另外要注意的是，Redis 中的 SDS 分为 sdshdr 8, sdshdr 16, sdshdr 32, sdshdr 64, 字段属性一样，但是对应不同大小的字符串, 以 sdshdr8 为例，这里的 8 代表可以用 8 个 bite 记录字符串长度 8 位可以表示的数字范围是 ±2 的 8 次方（包括负数）然后一个 char 是8比特大小，即一个字节，sdshar8表示的是 char 的长度，也就是说 embstr 只能用 sdshdr 8 (最合适是的，也有特殊的 sdshdr 5 啥的，就是特殊情况了)<br>// from Redis 7.0.8
struct __attribute__ ((__packed__)) sdshdr8{
    unit8_t len;/*used*/
    unit8_t alloc;/*excluding the header and null terminator*/
    unsigned char flags; /* 3 Lsb of type, 5 unused bits*/
    char buf[]
};
复制<br><br><br>​	浮点型在 redis 中怎么表示的呢，因为 int 只针对整形，所以，浮点型会转换为对应的字符串，比如 3.14--&gt;"3.14"，然后根据长短（阈值 44（5.0.5）），用 embstr 或者 raw，long 以内的整数就以 INT 编码存储<br><br>​	一个 redis 字符串最大为 512 MB，5.0.5 里面源码也是直接写死的<br><br>
<br>Redis 是 c 语言写的，sds 就是对 C 语言字符串的封装，自带返回字符串长度，而不需要 c 语言一样遍历才能得到长度，
<br>而且因为自带长度，所以不需要再以 /0 作为结束判断，二进制安全，
<br>有预留空间方便扩展，节约性能
<br>​		<br><br>
<br>基础操作部分，get  set  setnx，ex 加过期时间（set 会覆盖或者擦除过期时间）
<br>字符串结构
<br>场景：缓存
<br>编码格式：int, embstr, raw（embstr 的只读）
<br>SDSHDR：raw 的数据指针、优点、预留空间
]]></description><link>2-领域\后端学习\redis\知识点\redisstring的编码以及操作.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/知识点/RedisString的编码以及操作.md</guid><pubDate>Tue, 20 Feb 2024 11:34:09 GMT</pubDate><enclosure url="lib\media\image-20231217210935567.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\image-20231217210935567.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[多机部署]]></title><description><![CDATA[ 
 <br>
多机部署详解：解决机器出现意外情况的手段
<br>
<br>这一部分和分布式锁重合度很高，做好连接 ✅ 2024-01-15
<br><br>Note
Redis 主从的操作是异步的
<br>为了解决可能发生的机器宕机问题，我们首先想到的是主从模式，即给 Redis 加上从节点，当 Master 挂掉了，可以启动从节点取而代之<br>要明确两个问题<br>
1. Master 和 Slave 之间数据怎么同步 <a data-href="#Slave 数据同步" href="\#Slave_数据同步" class="internal-link" target="_self" rel="noopener">Slave 数据同步</a><br>
2. Slave 什么情况下可以升级为 Master<br><br>
<br>有机会实际操作一下两个 redis 的主从方式的数据同步<br>
首先是上例子
<br>
<br>将 6380 设置为 6379 的从节点 127.0.0.1:6380&gt;slaveof 127.0.0.1 6379 
<br>执行 info replication 能看到已经成为了从节点
<br>127.0.0.1:6380&gt; info replication
# Replication3 
role:slav
master_host:127.0.0.1
master_port:6379
master_link_status:up
master_last_io_seconds_ago:3
master_sync_in_progress:0
slave_repl_offset:774
slave_priority:100
slave_read_only:1
connected_slaves:0
master_rep1id:89ebfa88d67fb7fe206d3a6180fcfe5f030f173a
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:774
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:747
repl_backlog_histlen:28
复制<br>
<br>然后我们通过一个 key 来验证，主节点 set 的 key 从节点能查到，说明设置成功，并且历史数据也会复制到从节点，用 keys * 命令可以查之前的所有 key
<br>127.0.0.1:6380&gt; GET skey
(nil)
127.0.0.1:6379&gt; SET skey abc
OK
127.0.0.1:6380&gt; GET skey
"abc"
复制<br>
<br>如果不想要这个从节点，可以用 SLAVEOF no one 去掉
<br>127.0.0.1:6380&gt;SLAVE0F no one
OK
复制<br>这个主从之间到底怎么同步的可以看看 <a data-href="数据同步技术" href="\2-领域\后端学习\redis\redis场景\数据同步技术.html" class="internal-link" target="_self" rel="noopener">数据同步技术</a><br>但是主从节点的切换需要我们从业务层来做切换，如果主节点出现问题，那么我们要去代码里面人工进行切换，似乎显得不那么方便，所以 Redis 也注意到了这个问题，就出现了进阶版的处理方案---哨兵模式<br><br>更详细的包括如何配置、节点的主观宕机、 SDOWN 状态以及纪元之类的可以看这个 <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/354720754" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/354720754" target="_blank">Redis专题：深入解读哨兵模式 - 知乎</a><br>
哨兵是什么？<br>
哨兵（Sentinel），顾名思义，就是具有监测能力、故障转义能力的服务或者说方案，redis 的哨兵本质还是 redis 进程，也可以说是一个运行着哨兵进程的东西（主机、容器）<br>先直接看演示三个哨兵节点，三个数据节点 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240115230610.png" referrerpolicy="no-referrer"><br>
运行完哨兵和主从之后，在哨兵的客户端运行命令查询主节点<br>127.0.0.1:26379&gt;9SENTINEL get-master-addr-by-name mymaster
1）"127.0.0.1"
2）"6379"
复制<br>然后模拟主节点宕机，切换到主节点使用 sleep 命令, 然后再查询主节点就会发生切换（原本的主节点变成了其他节点的从节点）<br>debug sleep 30
复制<br><br>上面提到了，哨兵实际上还是一个 redis 进程，只是工作模式不同，哨兵以监控节点状态及执行故障转移为主要工作，哨兵总是以固定的频率去发现节点、故障检测，然后在检测到主节点故障时以安全的方式执行故障转移，确保集群的高可用性，那么哨兵是如何工作的呢<br>一般情况下，哨兵节点每隔10秒（故障转移时每隔1秒）向主从节点发送 INFO 命令，以此获取主从节点的信息。第一次执行时，哨兵仅知道我们给出的主节点信息，通过对主节点执行 INFO 命令就可以获取其从节点列表。如此周期性执行，就可以不断发现新加入的节点<br>主管下线和客观下线<br>
简单来说，主观下线就是一个哨兵判断一个 redis 节点下线了，当 Sentinel 将一个主节点判断为主观下线之后，为了确认这个主服务器是否真的下线了，它会向同样监视这一主服务器的其他 Sentinel 进行询问，看它们是否也认为主服务器已经进入了下线状态。当 Sentinel 从其他 Sentinel 那里接收到足够数量的已下线判断之后，Sentinel 就会将从服务器判定为客观下线，并对主服务器执行故障转移操作。就设计到 <a data-tooltip-position="top" aria-label="^3290a9" data-href="#^3290a9" href="\#^3290a9" class="internal-link" target="_self" rel="noopener">选主策略</a><br>哨兵的选主策略是什么？<br>
当一个主节点被判断为客观下线时，监控这个主节点的所有 Sentinel 会进行协商，选举一个 Leader 对下线的主节点执行故障转移操作，然后由哨兵的 leader 从 Redis 节点中选择一个 Redis 节点作为主节点, 但是选择主节点是有策略的<br>
1. 会过滤故障的节点<br>
2. 优先选择 slave-priority 最大的从节点作为主节点，如果不存在则继续<br>
3. 选择复制偏移量最大的的从节点。如果存在相同，则继续（数据偏移量记录写了多少数据，主服务器会把偏移量同步给从服务器，当主从偏移量一致，则数据完全同步）<br>
4. 选择 runId 最小的从节点（Redis 每次启动的时候生成随机的 runid 作为 Redis 的标识） <br>哨兵如何推选出哨兵 leader<br>
故障检测是多个 Sentinel 同时执行的，也就是说可能多个 Sentinel 在相近的时间内都判定主节点客观宕机了，因此 Leader 的选举过程在 Sentinel 集群内可能是同步进行的。所以，Sentinel 需要在集群内进行“拉票” 简单来说想要成为 leader：<br>
1. 越老的资历（配置纪元，就是经历了几次故障转义的次数，而且先发现的节点会把自己纪元先加一）<br>
2. 就是越早拉票（Redis 是单线程，所以有先来后到）<br>
3. 然后选票大于 50% （防止脑裂，出现两个主节点）<br>
4. 获得的投票必须大于法定投票人数（这是一个设置，quorum，比如你设置的是 3，但是只有两个人投票时，将不会产生哨兵leader）]]></description><link>2-领域\后端学习\redis\redis场景\多机部署.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/多机部署.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240115230610.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240115230610.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[分布式锁（很重要）]]></title><description><![CDATA[ 
 <br><br>首先，锁可以理解为针对某项资源使用权限的管理，通常用来控制共享资源, 而分布式锁，顾名思义，是在分布式环境下的锁，用来管理多台不同机器上的线程，竞争同一项资源<br><br>
<br>互斥性<br>
锁的目的是保证同一时刻只有一个线程持有锁，所以要保证互斥
<br>安全性<br>
需要避免锁因为异常的出现而永远不被释放，需要有兜底的释放锁的能力，并保证后续其他竞争者也能加锁
<br>对称性<br>
同一个锁，加锁解锁的必须是同一个对象，不能解锁别人的锁
<br>可靠性<br>
需要能处理一定程度的异常，而且要有容灾的能力
<br><br><br>最简化版本就是直接使用 Redis 的 setnx 命令，这个命令如果 key 存在，那么会返回 0，不会影响该数据<br>setnx key value
复制<br>基于这个特性，我们就可以利用这个方式加锁解锁了<br><br><img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240111221311.png" referrerpolicy="no-referrer"><br>
通过 setnx 进行锁的操作<br>但是这么做解决了互斥性的问题，但是安全性和可靠性还有对称性该怎么解决？<a data-href="#分布式锁应该具备哪些特性？" href="\#分布式锁应该具备哪些特性？" class="internal-link" target="_self" rel="noopener">分布式锁应该具备哪些特性？</a><br><br>安全性当然是支持过期时间了，这样可以保证一旦一个线程持有锁时间太长，就会自动解锁，保证服务能继续进行下去，但是这时候又引入了一个新的问题，怎么加过期时间，虽然 Redis 提供了 setNx 方法 <a data-tooltip-position="top" aria-label="RedisString的编码以及操作 > ^244caf" data-href="RedisString的编码以及操作#^244caf" href="\2-领域\后端学习\redis\知识点\redisstring的编码以及操作.html#^244caf" class="internal-link" target="_self" rel="noopener">set的一些操作</a>，但是 setnx 和 expire 不具备原子性，也就是说，如果是 setnx key value 然后再 expire key 100 这种操作可能在中间（服务器宕机）就断掉了，所以 Redis 官方也提供了原子性的写法 set key value nx ex seconds <br>set key value nx ex seconds 
复制<br>另外还有就是 watchDog 的机制，通过监听锁和业务的流程来给锁续期，保证业务流程不会在活没干完的时候，锁就过期了, 这个可以根据业务判断是否加上，（关于同样是做分布式锁的 zookeeper 有心跳监听的能力，在这个上面就更进一步了）<br>
<br>zookeeper 的创建分布式锁过程有时间就需要补充
<br>etcd 的分布式锁怎么操作，什么流程
<br>watchDog 的机制是什么机制--时间轮
<br>锁的可重入性、读写锁、公平锁
<br>以及一个 singleFight 了解一下
<br><br>上面的解决了安全性的问题，保证了一个锁不会被长时间占有，但是还存在一个重要问题，这个锁可能会被别的进程解锁<br>
例如：由于业务流程比较长，或者网络延迟 GC 卡顿等原因，导致锁过期，而业务还会继续进行。这时候，业务 B 已经拿到了锁，准备去执行，这个时候服务 A 恢复过来并做完了业务，就会释放锁，而 B 却还在继续执行。那就是说，A 把 B 的锁强行释放掉了<br>
所以我们需要进一步的解决方案，那就是设置 ownerId ，即每个人只能释放自己的锁，遵循谁申请谁释放，这里注解一下，key 还是那个 lock 的 key，但是 value 变成了判断依据，申请后还要判断里面的数据是不是自己的，否则不能解锁<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240111230557.png" referrerpolicy="no-referrer"><br>
这样，每次获取锁的删除锁的操作，都检查锁的名称对不对，不然不删除，这就解决了对称性的问题，但是实际上并不完善<br>
因为很明显，你看删除锁的流程，获取锁-&gt;返回值-&gt;删除锁，这三步不是原子操作，那么在分布式的环境下就有一个大问题，你获取的时候，锁是你自己的，但是你删除锁的时候，其实是其他人的锁，因为这些操作不具有原子性，所以我们就引入了 Redis 的特性 <a data-tooltip-position="top" aria-label="https://www.runoob.com/lua/lua-tutorial.html" rel="noopener" class="external-link" href="https://www.runoob.com/lua/lua-tutorial.html" target="_blank">LUA</a>, 使用 LUA 脚本，我们可以获得原子性的操作的能力<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240111231447.png" referrerpolicy="no-referrer"><br>
<br>Reids 分布式锁还没写完，未完待续 ✅ 2024-01-12
<br><br>
<br>那么上面的其实就解决了三个重要特性，互斥性，安全性，对称性，还有最后一个，可靠性，也就是说，需要解决的是，针对一些异常场景，网络波动、服务器挂掉、业务执行时间过长，要怎么解决, 一般来说，有两种大的解决办法

<br><a data-href="#主从容灾" href="\#主从容灾" class="internal-link" target="_self" rel="noopener">主从容灾</a> 
<br><a data-href="#多机部署" href="\#多机部署" class="internal-link" target="_self" rel="noopener">多机部署</a> 


<br>下面简单介绍一下<br><br>更详细的另一篇笔记有提到 <a data-href="多机部署#主从模式" href="\2-领域\后端学习\redis\redis场景\多机部署.html#主从模式" class="internal-link" target="_self" rel="noopener">多机部署 &gt; 主从模式</a><br>
主从容灾就是最简单的方式，即为 Redis 配置从节点，从节点挂了就用主节点顶上去，Redis 有了成熟的解决方式，就是哨兵模式<br>
1. 哨兵模式就是说，哨兵总事以固定的频率去发现节点、故障检测，然后在检测到主节点故障时以安全的方式执行故障转移，确保集群的高可用性<br>
2. 一般情况下，哨兵节点每隔 10 秒（故障转移时每隔 1 秒）向主从节点发送 INFO 命令，以此获取主从节点的信息。第一次执行时，哨兵仅知道我们给出的主节点信息，通过对主节点执行 INFO 命令就可以获取其从节点列表。如此周期性执行，就可以不断发现新加入的节点<br>
3. 因为哨兵模式有同步的延迟，很可能使得数据丢失，也可能导致分布式锁失效<br>
4. 具体的可以看这篇   <a data-href="多机部署#主从的进阶！哨兵模式" href="\2-领域\后端学习\redis\redis场景\多机部署.html#主从的进阶！哨兵模式" class="internal-link" target="_self" rel="noopener">多机部署 &gt; 主从的进阶！哨兵模式</a><br>
<br>可以到时候再把哨兵模式的流程整理成一个文档 ✅ 2024-01-15
<br><br>相对于哨兵模式，多级部署是更可靠的方法，多机部署对一致性的实现更高一点，比如 Redis 的 RedLock 算法，大概的思路就是多个机器，通常是奇数个，达到一半以上同意加锁才算加锁成功，这里是有原因的，由于主从的操作是异步的，也就是说，当客户端请求加锁的时候（set key），主服务会<br>流程<br>
1. 首先是假设有五个 Redis 主节点，基本保证不会同时宕机<br>
2. 客户端会同时向五个 Redis 申请锁，只要超过一半的 Redis 返回成功，就说明获取到了锁，超过一半的 Redis 失败则要发送解锁命令（没申请到就自己把锁解除了，防止锁卡在手上），失败的原因可能有很多，比如网络问题，或者有个 Redis 干脆就宕机了，但是只要超过一半的 Redis 节点成功的返回了数据，那就表示获取锁成功，线程就能继续<br>
3. 有一点需要注意的是，由于要向多个 Redis 发送消息，所以锁剩余时间要减去请求时间，如果剩余时间为 0，那么也是获取锁失败，比如你拿过来一个锁的时候，Redis 返回给你的时间是 30 s, 但是你请求时间花了一秒，所以到手的时候锁实际剩余剩余时间是 29 s, 这个时间为 0 说明锁过期了，或者网络问题导致你申请卡在手上，那么就属于获取锁失败，走失败的逻辑，也就是解锁，重新请求或者就不请求了，看业务实际情况<br>
4. 使用完成之后，需要同时向五个 Redis 发送解锁请求 <br><br>有了 Redlock 就一定能保证分布式锁的可靠性嘛？<br>
结论是不行，分布式系统的三大困境，简称（NPC），所以没有完全可靠的分布式锁<br>NPC 是什么？<br>
1. N: networkDelay（网络延迟）当分布式锁获得返回包时间过长，就是上面提到的一个问题  <a data-tooltip-position="top" aria-label="^edb40e" data-href="#^edb40e" href="\#^edb40e" class="internal-link" target="_self" rel="noopener">看第三点</a> Redlock 我们有解决方案是减去了请求时间的，一定程度上是可以解决这个问题<br>
2. P: Process Pause （进程暂停）比如发生了 GC，获取锁的线程开始处理流程，但是处于 GC 执行中，导致锁超时（超时后其他的线程就可以拿到锁了，但是这个时候，线程并不知道自己的锁过期了，因为他处于 GC 流程之前是有剩余时间的），这样会导致一个问题就是等 GC 结束，会有两个线程拿到同一个分布式锁 lock（尽管里面的 value 不一样了），然后他们就会都走完业务流程，见下图 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240114192725.png" referrerpolicy="no-referrer"> 分布式锁就失效了<br>
3. C: Clock Drift（时钟漂移）可以理解成不同机器里面的时间不一致，导致锁瞬间过期或者其他情况 <a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/183843997" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/183843997" target="_blank">分布式系统：与时间有关的故事 - 知乎</a>，也会导致上面的情况，让 A 和 B 线程拥有相同的锁，同时执行业务流程，破坏了互斥性<br><br>
<br>四大特性：互斥性、安全性、对称性、可靠性 <a data-tooltip-position="top" aria-label="^34901e" data-href="#^34901e" href="\#^34901e" class="internal-link" target="_self" rel="noopener">四大特性</a>
<br>可靠性可以两个方面来看，一个集群一个多机部署
<br>分布式锁没有绝对可靠的
]]></description><link>2-领域\后端学习\redis\redis场景\分布式锁（很重要）.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/分布式锁（很重要）.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240111221311.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240111221311.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[缓存一致性怎么保证]]></title><description><![CDATA[ 
 <br>
缓存和数据库是两个存储数据的地方，那就可能出现缓存中的数据和实际库里面的数据对不上的情况，那我们应该怎么保证他们的一致性呢，以 mysql、Redis、和最常用的 <a data-tooltip-position="top" aria-label="Redis缓存基础 > Cache Aside" data-href="Redis缓存基础#Cache Aside" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a> 为例
<br><br><a data-tooltip-position="top" aria-label="https://xiaolincoding.com/redis/architecture/mysql_redis_consistency.html#%E5%85%88%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE%E5%BA%93-%E8%BF%98%E6%98%AF%E5%85%88%E5%88%A0%E9%99%A4%E7%BC%93%E5%AD%98" rel="noopener" class="external-link" href="https://xiaolincoding.com/redis/architecture/mysql_redis_consistency.html#%E5%85%88%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE%E5%BA%93-%E8%BF%98%E6%98%AF%E5%85%88%E5%88%A0%E9%99%A4%E7%BC%93%E5%AD%98" target="_blank">先删除缓存还是更新数据库后再删除缓存</a><br>是由于线程并发引起的，<br>
对于Cache Aside 来说，先更新数据库，然后删缓存，可能会有A线程读取完数据库后再将数据写入缓存的过程中，出现了一个写操作的线程，他更新完数据库并删除完了缓存之后，线程A才将查到的旧数据写入缓存，就会出现不一致的现象<br>
但是这个几乎不可能出现，因为缓存的写入比数据库写入快的多，A写回数据的时间远远小于B先写数据库，然后删除缓存的时间<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107231137.png" referrerpolicy="no-referrer"><br><br>举个例子，比如在多线程的环境，<br>
Thread 1 更新 mysql 为 5-&gt;thread 2 更新 mysql 为 3-&gt;  （但是这个时候，因为时间片的原因，两个线程更新缓存的顺序反过来了） thread 2 更新缓存为 3 -&gt;thread 1 更新缓存为 5，最终正确的数据因为时序性被覆盖了。 <br><br><br>使用 redis 的过期期间, mysql 更新的时候，redis 不做任何处理，等缓存时间过期自动失效，然后再从 mysql 拉取缓存<br><br>优点<br>
1. 开发成本低，Redis 原生接口，好实现<br>
2. 管理成本低，出问题概率小<br>缺点<br>
比较依赖过期时间的设置，过期时间设置太长容易导致数据对不上，太短又容易出现数据库压力，需要对业务有强力的判断<br><br>这个就是旁路缓存 <a data-href="Redis缓存基础#Cache Aside" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#Cache_Aside" class="internal-link" target="_self" rel="noopener">Redis缓存基础 &gt; Cache Aside</a> 的写了（<a data-tooltip-position="top" aria-label="Redis缓存基础 > ^67a07b" data-href="Redis缓存基础#^67a07b" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#^67a07b" class="internal-link" target="_self" rel="noopener">Cache Aside的写流程</a>），如果是新增，那就直接插入数据库后写入 redis 缓存，如果是更新，那就更新数据库后尝试删除对应的缓存<br>优点在于<br>
1. 能更好的保证一致性<br>
2. 实现成本更低<br>缺点<br>
硬要说缺点就是，可能会带来性能上的一些损耗，以及如果删除失败，那就相当于方向一<a data-href="#只更新数据库，不管 Redis" href="\#只更新数据库，不管_Redis" class="internal-link" target="_self" rel="noopener">只更新数据库，不管 Redis</a><br><br>这种就是我们的 <a data-href="Redis缓存基础#Write Through" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#Write_Through" class="internal-link" target="_self" rel="noopener">Redis缓存基础 &gt; Write Through</a> 模式用的方式了<br>
详细的来说就是<br>
把我们搭建的消费服务作为 mysql 的一个 slave，订阅 mysql 的 binlog 日志，解析日志内容，再更新到 redis。此方案和业务完全解耦，redis 的更新对业务方透明，可以减少心智成本。<br>优点<br>
- 和业务完全解耦，在更新 mysql 时候，不用额外操作<br>
- 无时序性问题，可靠性强<br>缺点<br>
- 引入了消息队列这种比较重的组件，还需要单独搭建一个同步服务，维护需要成本<br>
- 同步服务不能崩，一崩那 redis 会在很长时间里面都是旧数据]]></description><link>2-领域\后端学习\redis\redis场景\缓存一致性怎么保证.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/缓存一致性怎么保证.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107231137.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107231137.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[缓存异常场景]]></title><description><![CDATA[ 
 <br>
在使用缓存的过程中，我们会遇到各种各样的问题，要了解问题，避免问题，不行再解决问题
<br><br>缓存穿透的本质<br>
我们的缓存模式，是会在查不到缓存数据的时候，去查 db，然后把 db 的数据写入缓存里面，这个查询 db 这一步就可能被人利用给我们造成危害<br>
由于查不到就去查 DB，所以如果有大批量的查询全部来查询这个不存在的 key，那就相当于没有缓存，所有的请求都是去 db 查询的，流量够大就会导致 db 挂掉<br>解决方案<br>
1. 我们的数据可能有 id 或者其他的一些用户信息，用于判断，比如我们可以直接拦截所有 id 小于 0 的请求（id 为-1 的很可能就是攻击者）<br>
2. 如果发生了在缓存里面没有取到的数据并且在数据库里面也没有取到，那我们可以通过设置 key-value 为 key-null 来防止用户反复攻击同一个 key，但是设置过期时间不能太长，30 s 差不多<br>
3. 可以考虑使用<a data-href="#布隆过滤器" href="\#布隆过滤器" class="internal-link" target="_self" rel="noopener">布隆过滤器</a> <br><br>是什么？<br>
Loomfilter 就类似于一个 hashset，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断某个 key 是否存在于某容器，不存在就直接返回。布隆过滤器的关键就在于 hash 算法和容器大小，是一种巧妙的概率模型，可以用来判断一个东西 一定不存在或者可能存在<br>原理：<br>
当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点（offset），把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了：如果这些点有任何一个 0，则被检元素一定不在；如果都是 1，则被检元素很可能在。这就是布隆过滤器的基本思想。<br>详解:<br>
简单来说，我们存储数据是二进制的，数据不是 0 就是 1，默认是 0，那我们就用 0 表示数据不村在，1 表示数据存在，而布隆过滤器本身，就是一个很长的二进制和一些列随机映射函数，当有数据进入的时候，就会经历下面的流程<br>
1. 通过 k 个哈希函数计算，返回 k 个哈希值<br>
2. 将 k 个哈希值对应映射到 k 个二进制数组下标<br>
3. 然后把 K 个下标对应的二进制数据变成 1<br>
也就是说，存进来的数据布隆过滤器先放好，然后再来数据的话，我先检查下我 K 个 hash 算法能不能找到对应的节点，并且其中的二进制数据是不是 1，如果有 0 存在那就说明数据不存在了<br>优缺点<br>
优点在于，他用的二进制，那么空间消耗都会特别小，缺点就是存在误判，只能判断可能存在，而且不好删除，hash 存在 hash 冲突，很可能多个合成了一个<br><br>是什么？<br>
数据中可能存在缓存中没有但是数据库中有的数据（缓存数据到期可能会出现这个情况），这时候一般情况下，我们都会去数据库查询，但是这个时候，如果用户并发量很大，大量的请求同时去读取数据库，也会导致数据库挂掉，简单来说，就是热点数据失效的一瞬间，还没来得及重新产生，就有海量数据直达数据库<br>解决方案<br>
1. 首先就是热点数据可以续期，持续的访问可以让数据不断的续期，避免因为过期导致击穿<br>
2. 缓存失效，重建缓存加互斥锁，加锁保护数据库，当线程发现缓存不存在的时候，就会尝试加锁，只有加到锁的线程可以去查询数据库，然后重建缓存，避免高流量 <br><br>是什么？<br>
指的是大量的应用请求因为异常，无法在 Redis 缓存中进行处理，就会像雪崩一样，直接打到数据库，简单来说就是缓存中有大量的数据到了过期时间，而查询量巨大，就会导致数据库压力过大甚至宕机<br>与<a data-href="#缓存击穿" href="\#缓存击穿" class="internal-link" target="_self" rel="noopener">缓存击穿</a>的区别在于，击穿是说一条热点数据没有及时的重建缓存导致的，而雪崩更加彻底，大批量数据失去缓存导致请求直接请求到数据库<br>解决方案<br>
1. 缓存数据的过期时间可以设置成随机，防止大量数据同时过期<br>
2. 跟击穿的第二条解决方法类似，加锁 <a data-tooltip-position="top" aria-label="^40fc0e" data-href="#^40fc0e" href="\#^40fc0e" class="internal-link" target="_self" rel="noopener">第二点的锁</a><br>
]]></description><link>2-领域\后端学习\redis\redis场景\缓存异常场景.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/缓存异常场景.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate></item><item><title><![CDATA[缓存应用]]></title><description><![CDATA[ 
 <br>
<br>
本地缓存和远端缓存

<br>
本地缓存可以和远程缓存一起配合使用，多级缓存 本地缓存包括，cafeeine，

<br>
缓存预热

<br>
压测，本地缓存比 redis 好

]]></description><link>2-领域\后端学习\redis\redis场景\缓存应用.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/缓存应用.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate></item><item><title><![CDATA[秒杀和限流]]></title><description><![CDATA[ 
 <br>
<br>秒杀场景、和限流场景记得补充
]]></description><link>2-领域\后端学习\redis\redis场景\秒杀和限流.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/秒杀和限流.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate></item><item><title><![CDATA[事务]]></title><description><![CDATA[ 
 <br>事务是什么？<br>
多个操作被视为一个整体，就是事务，事务通常具有原子性<br><br>Redis 原生有 Multi 命令，可以开启事务，实际上是由 MULTI、EXEC、DISCARD、WATCH 这四个命令配合完成的<br>怎么操作？<br>
1. Multi 即开启事务<br>
127.0.0.1:6379&gt;multi<br>
OK<br>
2. 此时进行的事务都会到 Redis 队列里面, 但是不会真正执行<br>
127.0.0.1:6379 (TX))&gt;set k1 aa<br>
QUEUED<br>
127.0.0.1:6379 (TX))&gt;set k2 bb<br>
QUEUED<br>
3. 实际执行<br>
127.0.0.1:6379&gt;exec<br>
1) OK<br>
2) OK<br>
4. 这个时候 get 命令就能查到这俩了<br>
127.0.0.1:6379&gt;get k1<br>
"aa"<br>
127.0.0.1:6379&gt;get k2<br>
"bb"<br>
5. 放弃执行事务就是<br>
127.0.0.1:6379&gt;DISCARD<br>
OK<br>
127.0.0.1:6379&gt;get k1<br>
(nil)<br>
6. WATCH 命令<br>
1. Watch 用来提前来观察数据，具体来说，它用于监视一个 (或多个）key，如果在事务执行之前这个 (或这些) key 被其他命令所改动，那么事务将被打断<br>
2. 一般的操作是：在multi 命令之前使用 watch 命令监控某些键值对，然后使用 multi 命令开启事务，执行各类对数据结构进行操作的命令，这些命令会进入先入先出队列中<br>
3. 当 redis 使用 exec 命令执行事务的时候，首先会比较对被 watch 的键值对有没有发生变化，如果产生变化回滚事务，没有变化执行事务中的命令，无论事务执行与否，最终都会取消执行事务之前的 watch 命令<br>
4. 总结：就是说，如果你这个 key 在你操作的时候，被别人改了，那你就回滚事务<br>
5. 例子如下（前提是你先 Watch 了） <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240114225819.png" referrerpolicy="no-referrer"> 可以看到最终是没有修改成功<br>那么 MULTI 有原子性吗？答案是没有，他只是通过单线程的特性，让其他操作切不进来，如果中途发生了什么，或者自己有问题还是可能只做了一半，甚至中间报错，后面还是能成功，这是很捞的一件事<br><br>
Lua 是一种用标准 C 语言编写的轻量的脚本语言，陈其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。可以看到 Lua 的目标就是为应用程序提供扩展
<br>使用的例子<br>127.0.0.1:6379&gt;eval "return {redis.call('set','ka', 'a'), redis.call('set','kb', 'b')}" 0
1) ok
2) ok
复制<br><br>先提出几个问题<br>
<br>lua 事务失败会怎么做？
<br>相比 multi 的优势在哪？
<br>有哪些场景？
<br>首先第一点，失败会怎么做？<br>
不会，会中断，不会进行后面的步骤，也就是不能保证完全的原子性，也说明了不能完全依赖分布式锁<br>第二优势就多了<br>
<br>lua 可以编写 if else 来写逻辑
<br>事务中间如果失败，会中断后续执行
<br>方便，不需要 multi 的 Watch，直接开始就行
<br>场景就是<a data-href="分布式锁（很重要）" href="\2-领域\后端学习\redis\redis场景\分布式锁（很重要）.html" class="internal-link" target="_self" rel="noopener">分布式锁（很重要）</a>和秒杀
]]></description><link>2-领域\后端学习\redis\redis场景\事务.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/事务.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240114225819.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240114225819.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据同步技术]]></title><description><![CDATA[ 
 <br>我们可以通过 slaveof 来设置主从，但是 Slave 和 Master 的数据是如何同步的呢？我们从流程一步步看<br>首先是成为 Slave 节点<br>
通过 laveof 命令或者 replicateof (两个命令是一样的，只是改了名)，就能成为 Master 的 Slave, 成功之后，Slave 会记录下 Master 的 IP 和端口<br>建立连接<br>
1. Slave 每隔一秒都会检测一下，要不要和 Master 建立连接<br>
2. 创建 Socket 连接<br>/* Check if we should connect to a MASTER*/
if (server.repl_state == REPL_STATE_cONNECT）{
	serverLog(LL_NoTICE,"Connecting to MASTER%s:%d",
		server.masterhost, server.masterport);
	if (connectWithMaster() == C_OK) {
		serverLog(LL_NoTICE,"MASTER &lt;-&gt; REPLICA Sync started");
	}
}
复制<br>在上面的代码中的 connectWithMaster 中，如果连接成功，就会为 Socket 建立一个专门处理复制事项的文件事件处理器，来负责后续的复制工作。对于 Master 而言，接收到从节点的 socket 连接后，会为该 socket 创建相应的客户端记录，也就是说，将从节点看作一个普通的客户端，后面的复制其实是以客户端-服务器请求回复的模式来进行。<br>建立连接完成之后，就进入数据同步状态 Slave 会主动向 Master 发送 psync 命令，首次新添加的 Slave 会触发全量复制，其余时候是增量复制<br>
增量复制即将缓冲区中记录的操作发给 Slave，Slave 复刻这些操作进行同步。<br>
全量复制是说 Master 先生成一个类似 RDB 文件的内存数据快照，然后将快照发给 Slave，Slave 直接将快照加载到内存。]]></description><link>2-领域\后端学习\redis\redis场景\数据同步技术.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/数据同步技术.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate></item><item><title><![CDATA[消息队列]]></title><description><![CDATA[ 
 <br>
<br>可以补充的看一下视频课，后续补充吧，感觉不是很重要
<br><br>消息队列我们经常用到，但是消息队列是什么呢？<br>
1. 定义：顾名思义，就是传递消息的队列，<br>
2. 特性：有队列的先入先出的特性，<br>
3. 适用场景：一般用于异步流程、消息分发、流量削锋等问题，可以通过消息队列实现高性能、高可用、高扩展的架构<br>
4. 有哪些常用的消息队列：业界比较出名的消息队列中间件有 ActiveMQ、RabbitMQ、ZeroMQ、Kafka、MetaMQ、RocketMQ 等，这些队列通常具备可靠性、高性能等特点。]]></description><link>2-领域\后端学习\redis\redis场景\消息队列.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/消息队列.md</guid><pubDate>Wed, 21 Feb 2024 07:50:59 GMT</pubDate></item><item><title><![CDATA[Redis缓存基础]]></title><description><![CDATA[ 
 <br>
由于Redis性能高效，通常可以用作数据库储存的缓存，常见的就有给Mysql的热点数据做缓存的玩法，可以很大程度的提升系统的吞吐量
<br>缓存是什么？<br>
- 缓存分为两种，服务器缓存和客户端缓存<br>
- 服务器缓存就是指服务端将数据存入Redis，这样在访问DB之后，可以将从DB得到的数据缓存起来，下次访问就从缓存拿，而不走DB<br>
- 客户端缓存就是客户端自己把之前储存的结果放在缓存里面，下次再请求时候就能直接拿到结果<br><br>

<br>Cache Aside Pattern 旁路缓存模式
<br>Read Through Cache Pattern：读穿透模式
<br>Write Through Cache Pattern：写穿透模式
<br>Write Behind Pattern：又叫Write Back，异步缓存写入模式

<br><br>是什么？<br>
旁路缓存模式，即最常见的模式，应用的服务会直接把缓存当作数据库的旁路，并直接和缓存进行交互，读先读缓存，写先写内存<br>读流程<br>
流程如下图<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107224814.png" referrerpolicy="no-referrer"><br>
简单来说，优先访问缓存，缓存没有再去查db，然后更新缓存，再返回数据<br>写流程<br>
流程如下<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107224939.png" referrerpolicy="no-referrer"><br>
写操作的时候，如果是新增，那就是写入数据库然后更新 Redis 缓存，那如果是更新，就有点需要注意了，更新的时候优先写到数据库里面，然后尝试直接删除缓存，那么为什么不更新呢？因为更新相比删除，会更容易出现时序性的问题<a data-href="缓存一致性怎么保证" href="\2-领域\后端学习\redis\redis场景\缓存一致性怎么保证.html" class="internal-link" target="_self" rel="noopener">缓存一致性怎么保证</a>，还有个需要注意的地方是，为什么说是尝试删除，因为删除是可能失败的，如果失败我们就会选择忽略，因为还有过期时间兜底 <br>适用场景<br>
由于写操作的时候直接删除缓存，那么很显然这个更适合于写操作少的场景，读多写少<br>缺点<br>
缺陷在于，可能会出现缓存和数据库不一致的情况 <a data-href="#缓存和数据库不一致的问题" href="\#缓存和数据库不一致的问题" class="internal-link" target="_self" rel="noopener">缓存和数据库不一致的问题</a><br>
<br><a data-href="缓存一致性怎么保证" href="\2-领域\后端学习\redis\redis场景\缓存一致性怎么保证.html" class="internal-link" target="_self" rel="noopener">缓存一致性怎么保证</a><br><br>是什么？<br>
读穿透模式，就是Redis起了一个服务，由这个服务呢去访问数据库和缓存，服务根据自身情况去判断怎么做，类似于Service Dao层的管理模式，跟<a data-href="#Cache Aside" href="\#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a>不一样的就在于，他是将具体的逻辑封装起来了，跟<a data-href="#Cache Aside" href="\#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a>相比，业务代码会更简洁一点，但是缺点在于，多进行了一次服务的调用，缓存命中的时候，性能不如<a data-href="#Cache Aside" href="\#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240108203039.png" referrerpolicy="no-referrer"><br><br>是什么？<br>
相对于 <a data-href="#Cache Aside" href="\#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a> 应用程序需要维护两个数据，一个缓存一个数据库，操作比较麻烦，所以Write Through相当于做了一个封装，封装了一个单独的服务，应用程序就只有一个访问源，服务自身维护自己的访问逻辑<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107231718.png" referrerpolicy="no-referrer"><br>
他的特点在于<br>
- 缓存的及时性更高，写入时候就加载了缓存里面，当然可能会有时序问题<br>
- 不能忍受数据丢失，和数据不一致，当然Cache Aside也是这样<br>怎么用？<br>
这个模式是有配合的，有Write-Through 一般都配合Read-through 一起使用，<br>使用场景<br>
一般是银行场景用的多<br>优缺点<br>
优点在于缓存及时，数据写入就加入了缓存，但是缺点也在于此，由于写入时同时写入了缓存，所以会导致缓存到后面变的很大，需要考虑缓存的时效性，让缓存及时过期<br><br>是什么？<br>
和<a data-href="#Write Through" href="\#Write_Through" class="internal-link" target="_self" rel="noopener">Write Through</a>一样都会在写入的时候，更新数据库且更新缓存，但是并不是立即写入数据库，当数据写入时，先写缓存，然后异步把数据一起写入数据库，异步写操作时其最大的特点，<a data-href="#数据库的写" href="\#数据库的写" class="internal-link" target="_self" rel="noopener">数据库的写</a><br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240108204434.png" referrerpolicy="no-referrer"><br><br>数据库的异步写入可以用多种方式完成，下面两种是可以结合的<br>
-  一个是收集写操作，等到某一个时间点的时候（比如就是数据库负载低的时候）再一起写入<br>
- 一个就是合并几个写操作，然后一起批量写入，即集中写入<br>异步写入可以极大的降低数据库的负担，但是代价是安全性不够，比如你先写入了缓存里面，准备异步写入的时候，服务器崩了，那数据就消失了<br><br>怎么设置缓存？<br>
1. 一般来说，Redis缓存用的就是Set命令，Set key value 如果业务需要，后面还能加一个ex当过期时间，set key value ex 10 <br><br>
<br>四种模式怎么选择？<br>
<a data-href="#Cache Aside" href="\#Cache_Aside" class="internal-link" target="_self" rel="noopener">Cache Aside</a>是最常用的，其他的在业务开发的时候很少会用到
<br>几个缓存的场景总结

<br>Cache Aside 读是先读缓存再读数据库并更新缓存，写的时候则是直接删除缓存
<br>Read Through 类似于封装了一层Service的Cache Aside，业务上更简洁，性能却不如Cache Aside
<br>Write Through 也是封装了服务，但是逻辑上变了，缓存和内存的同时写入，带来了及时性（可能有时序性问题），也带来了巨大的缓存量
<br>Write-Behind 异步写的操作是特点，能够降低数据库负担，但是安全性不足


<br>如何保证缓存一致性呢？--&gt; 见<a data-href="缓存一致性怎么保证" href="\2-领域\后端学习\redis\redis场景\缓存一致性怎么保证.html" class="internal-link" target="_self" rel="noopener">缓存一致性怎么保证</a>
]]></description><link>2-领域\后端学习\redis\redis场景\redis缓存基础.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/Redis缓存基础.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107224814.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240107224814.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis集群]]></title><description><![CDATA[ 
 <br><br>对 Redis 集群来说，集群就是多个 Redis 合作解决问题，一起对外提供服务，但对外看起来是一个单机服务的方案<br><br>我们可以从例子实际出发<br><br>首先创建一个目录，redisCluster ，里面创建三个子目录 7000，7001，7002，这些目录都 copy 一份 redis.conf 配置，在其中修改下面 3 个字段：<br>
<br>cluster-enabled，设置为 yes，这样才能成为集群的一份子 
<br>port，修改为为对应数字，比如 7000 目录 port 就改为 7000
<br>dir 改为./，表示数据放在当前目录下
<br># Normal Redis instances can't be part of a Redis Cluster; only no
# started as cluster nodes can. In order to start a Redis instance
# cluster node enable the cluster support uncommenting the folLowi
#
cluster-enabled yes
# Accept connections on the specified port, default is 6379 （IANA
# If port θ is specified Redis will not Listen on a TcP socket.
port 7000
# Note that you must specify a directory here, not a file name.
dir ./
复制<br>然后在 7000 7001 7002 目录下，使用如下命令<br>redis-serverredis.conf
复制<br>执行之后目录结构如下<br>|--- 7000
|	|---appendonly.aof
|	|---nodes.conf
|	└─--redis.conf
|	
|--- 7001
|	|---appendonly.aof
|	|---nodes.conf
|	└─--redis.conf
└─-- 7002
	|---appendonly.aof
	|---nodes.conf
	└─--redis.conf

复制<br>接着使用命令 redis-cli -p 7000 连接上 7000 节点，然后 cluster nodes 查看节点，只有一个节点在集群中<br>127.0.0.1:7000&gt; cluster nodes
f48115efb65569d0740b29a708e6de9e1ae439b6:7000@17000myse1f,master
复制<br><br>组建集群的命令是 cluster meet<br>127.0.0.1:7000&gt;clustermeet 127.0.0.1 7001
OK
127.0.0.1:7000&gt;cluster meet 127.0.0.1 7002
OK
复制<br>这样就连接上了 7001 和 7002 了，再在 7000 的客户端查询 cluster nodes<br>127.0.0.1:7000&gt; cluster nodes
f07a9325cd3bc65bab81ba80246552308bd019dd 127.0.0.1:7001@17001 master - 0 1678499786966 1 connected
f48115efb65569d0740b29a708e6de9e1ae439b6 127.0.0.1:7000@17001 myse1f,master - 0 167849978500 0 connected
5a12b48f3fb9d057a0da4bb32fee07b6912e7034 127.0.0.1:7002@17001 master -0 1678499785956 2 connected
复制<br>但是这时候还需要做一步 分配 hash 槽 详见<a data-href="RedisHash槽" href="\2-领域\后端学习\redis\redis场景\redishash槽.html" class="internal-link" target="_self" rel="noopener">RedisHash槽</a><br>
连接到集群后，我们还需要为每个节点分配责任区间，也就是每个节点对应的数据区间<br>
<br>Redis hash 槽要连接到这块 ✅ 2024-01-18
<br>redis-cli -p 7000 cluster addslots {0..5461}
redis-cli -p 7001 cluster addslots {5462..10922}
redis-cli -p 7002 cluster addslots {10923..16383}
复制<br>然后再查询 cluster nodes 就能看到到各自的数据区间了<br>127.0.0.1:7000&gt; cluster nodes
f07a9325cd3bc65bab81ba80246552308bd019dd 127.0.0.1:7001@17001 master - 0 1678499786966 1 connected 5462-10922
f48115efb65569d0740b29a708e6de9e1ae439b6 127.0.0.1:7000@17001 myse1f,master - 0 167849978500 0 connected 0-5461
5a12b48f3fb9d057a0da4bb32fee07b6912e7034 127.0.0.1:7002@17001 master -0 1678499785956 2 connected 10923-16383
复制<br><br>直接操作 7000 的客户端就行，如果不在对应的 slot，就会让你 MOVED，Redis 怎么判断出 MOVED 错误的可以看这里 <a data-href="RedisHash槽#储存槽的位置" href="\2-领域\后端学习\redis\redis场景\redishash槽.html#储存槽的位置" class="internal-link" target="_self" rel="noopener">RedisHash槽 &gt; 储存槽的位置</a><br>
GPT 解释: 在 Redis 集群环境中，"MOVED"错误是一个常见的响应，它告诉客户端请求的键 (key)存在于另一个不同的节点。Redis 集群通过分片（sharding）来实现数据的水平扩展，这种方式将所有数据分散存储在多个节点上，每个节点负责维护一部分键空间。Redis 集群将所有的键空间分成了 16384 个槽（slots)，每个键通过 HASH_SLOT 算法映射到一个槽，每个节点负责一部分槽的管理。当客户端尝试访问一个键时，它会根据该键计算出相应的槽，并将请求发送到负责该槽的节点。如果客户端尝试访问的键不在当前节点负责的槽内，那么节点会返回一个"MOVED"错误，并指示正确负责该键的节点和槽。"MOVED"错误的格式通常如下：MOVED &lt;slot&gt; &lt;host&gt;:&lt;port&gt;&lt;slot&gt; 是键映射到的槽的编号。&lt;host&gt; 是负责该槽的节点的 IP 地址。&lt;port&gt; 是负责该槽的节点的端口号。<br>127.0.0.1:7000&gt;get bigfish
(error) M0VED 12574 127.0.0.1:7002
127.0.0.1:7000&gt;get bigcat
(error) M0VED 10548 127.0.0.1:7001
127.0.0.1:7000&gt;get bigdog
(error)M0VED11769127.0.0.1:7002
127.0.0.1:7000&gt;get bigmouse
(error) M0VED 11002 127.0.0.1:7002
127.0.0.1:7000&gt;get bigmonkey
(error)MOVED9888127.0.0.1:7001
127.0.0.1:7000&gt;get bigdragon
(error)MOVED10224127.0.0.1:7001
127.0.0.1:7000&gt;get bigsnake
(nil)
127.0.0.1:7000&gt; set bigsnake snake
OK
127.0.0.1:7000&gt; get bigsnake
复制<br>具体的操作如上<br><br>集群的操作如果想要新添加节点的话应该怎么做呢？hash 槽已经分配完的情况下，怎么获取新的 hash 槽呢？<br>
<br>未完待续 ✅ 2024-01-18<br>
第一步先起一个 Redis, 然后加入集群
<br>127.0.0.1:7000&gt;clustermeet 127.0.0.1 7003
OK
复制<br>然而此时，新加入的节点还没有负责任何槽，使用 cluster nodes 可以看到<br>127.0.0.1:7000&gt; cluster nodes
f07a9325cd3bc65bab81ba80246552308bd019dd 127.0.0.1:7001@17001 master - 0 1678499786966 1 connected 5462-10922
f48115efb65569d0740b29a708e6de9e1ae439b6 127.0.0.1:7000@17001 myse1f,master - 0 167849978500 0 connected 0-5461
5a12b48f3fb9d057a0da4bb32fee07b6912e7064 127.0.0.1:7003@17001 master -0 1678499786400 0 connected 
5a12b48f3fb9d057a0da4bb32fee07b6912e7034 127.0.0.1:7002@17001 master -0 1678499785956 2 connected 10923-16383
复制<br>他没有负责任何槽位（这里以实际操作返回为准，只要知道他没返回槽位就行了）<br>第二步，分配一些槽位给这个节点命令为 redis-cli --cluster rehard 127.0.0.1:7000<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118231943.png" referrerpolicy="no-referrer"><br>
这里的意思是，从 7000 和 7001转移了一共 100 个 slot 给 7003<br>
于是从 7000 和 7001，分别拿了 50 个 slots 给 7003，这里分配的就是节点各自按编号最小的 50 个左右 slot，比如 7000，就是 0-50。<br>
完成之后 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118232427.png" referrerpolicy="no-referrer"><br><br>集群是什么？
	集群是指多个 Redis 共同工作，分配不同的 hash 槽的区域，但是对外总体是暴露一个单独的服务的方案
<br>怎么访问集群
访问的话，就是随便找一台已经在集群里面的客户端，直接用就行了，get 不到的话会有 moveD 报错，告诉你去找哪个集群，使用 cluster node 查看集群
<br>Redis 用的是一致性的 Hash 算法吗
是 Hash 槽
]]></description><link>2-领域\后端学习\redis\redis场景\redis集群.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/Redis集群.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118231943.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118231943.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[RedisHash槽]]></title><description><![CDATA[ 
 <br><br>Redis 是用 Hash 槽来管理数据分片的，一个 Redis Cluster 包含 16384 个 Hash 槽，即 2^14 次方，存储在集群中的每一个 key 都能通过关联 Hash 算法关联到某个槽，每个节点负责一部分的槽，查数据也要一一对应<br>首先看代码定义 <img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118234129.png" referrerpolicy="no-referrer"> 其中 slot 数组表示某个节点的对应关系，而且这里要注意的是，分配 Slot 的过程不是说会让 slot 数组分成若干的部分，而是每个 Redis 都有一整个 slot，这个数组是负责记录对应的槽位是不是自己负责的，如果为 1 表明是自己负责<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118234502.png" referrerpolicy="no-referrer"><br>这是位图，所以可以做到在 o (1) 复杂度的情况下，查到某个槽是不是由该节点负责<br>
<br>可以看看位图的解释
<br><br>如果客户端请求错了节点，节点还会友情返回你这个数据应该从哪个节点获得（MOVED）。从这里我们能看出除了记录了自己负责了哪些槽，还需要知道某个槽所在位置。这是怎么实现的呢？首先，在分配槽之后，节点会向集群中其它节点放送消息，告知它负责了哪些槽。<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118234732.png" referrerpolicy="no-referrer"> 然后节点会维护一个 ClusterState 结构<br>typedef struct clusterState{
	...
	clusterNode *myself;/*This node*/
	clusterNode *slot[CLUSTER_SLOTS];
	...
}clusterState
复制<br>这里解释一下，这个结构有两个 Node，一个是 Solts ，重点是 slots ,slots 里面存储着一个节点，结构如下，举个例子，slots[0] 里面的值是一个节点，这个节点的 key-val 是 0-val，而这个 val 就是对应的储存该槽的 Redis, 所以能很快的查到对应槽的Redis<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118235254.png" referrerpolicy="no-referrer">]]></description><link>2-领域\后端学习\redis\redis场景\redishash槽.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/Redis场景/RedisHash槽.md</guid><pubDate>Tue, 20 Feb 2024 11:34:10 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118234129.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240118234129.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Redis学习]]></title><description><![CDATA[<a class="tag" href="?query=tag:Redis操作" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Redis操作</a> <a class="tag" href="?query=tag:RedisHashTable" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#RedisHashTable</a> <a class="tag" href="?query=tag:Redis单线程" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Redis单线程</a> 
 <br><br>
<br>推荐5.05
<br>先对象
<br>再执行流程
<br>持久化
<br>场景
<br>集群
<br>自测题<br>
<a href=".?query=tag:Redis操作" class="tag" target="_blank" rel="noopener">#Redis操作</a> 
<br><br>
<br>Redis为什么需要持久化<br>
- 持久化是说把数据存到磁盘中，redis虽然大多数时候是缓存场景，但是也不想重启以后缓存都不存在了，如果缓存一下消失了，那么数据就一下打到内存里面了，而且，Redis本身也可以做存储，这就更需要持久化了
<br>RDB和AOF的本质是什么

<br>一个是存储二进制文件，一个是记录命令的日志


<br><br><br><br>
首先String的底层编码有三种

<br>
int （记录数字的）

<br>
embStr 
字符串长度比较小的时候用这种，这种格式的优点在于，字节头和内容是在一起的，内容紧凑，占空间小，但是缺点是，他是被设置成只读的，一旦有任何改动，就会变成raw格式

<br>
raw
超过一定阈值就会变成raw的格式，5.5的话是44，占空间


那么可知，浮点型的数据不是int，所以，会用raw或者embstr来表示，具体用哪个编码要看字符串长度
<br><br><br>​	首先，redis的底层字符串的实现有三种<br>
<br>int
<br>raw
<br>embstr
<br>int就是存储一个整型，就可以用一个long去存储这个数据，int超过long的话 ，就看阈值，会变成embstr或者raw的格式，目前较新的版本是44（5.0.5）<br>embStr是两部分放在一起同时分配内存，而raw则是分开分配内存，所以embstr占用内存更小<br>embstr他的优点是紧凑，但是他被设置成只读的样式，一旦发生改变，数据格式就会改成raw的格式<br>raw的话，就是两部分内存分开，方便修改数据<br>而embstr和raw则是两部分结构，redisObj和sdsHdr这俩部分，redisObj就是用来说明数据是不是string、解码方式是raw还是embstr的，而sdshdr就是封装的数据层，提供了原生c语言达不到的能力<br>其中sdshdr的部分就是封装了一层，里面直接存储了len（长度）、alloc（就是分配的空间）、以及数据，这样他查询字符串长度不需要遍历，直接O(1)的复杂度，而且判断字符串也不用用 /0去判断了，更安全，然后他还有分配了冗余空间，当字符串小于1M的时候，冗余空间就是字符串的大小，当字符串大于1M的时候，冗余空间就是1M<br><br>redis采用的jemalloc作为内存分配器的，然后因为redisObj和sdshdr里面的一些结构还有结尾的\0符号占据了一些字节，本身这个分配器是以64个字节为单位分配数据的，然后占据的部分刚好有64-44=20，所以44长度加上刚好一个内存单元<br><br>原本呢，redis是用C语言写的，C语言里面的字符串是用/0结尾的char数组来表示的，但是这样的话，会有很多问题，包括安全、更改长度以及什么修改字符串啥之类的问题，所以redis封装了一下，<br>sds里面自带了len，长度查询就是O（1）的复杂度，而且判断字符串结尾也不需要用/0来判断了<br>然后里面分配了冗余内存，方便修改，规则就是 min（len 1M）<br><br>list是双端操作对象，头尾都能pop和push<br><br>底层编码有两种，一种是ziplist、一种是linkedlist<br>ziplist结构更紧凑，占用空间小，数据直接是直接放在一起的，优点是占内存小，缺点是数据比较多的时候，修改会复制很多的空间<br>linkedList结构更松散一些，数据之间是链表的形式，优点是修改方便，缺点是本身占用内存比较大<br>所以在3.7版本以后，出现了quickList，综合了两者的优点，所有的list全部由quickList实现<br>​	他是以链表形式存储ziplist，数据少的情况他就是一个ziplist，数据多的话，由会变成linkeslist的结构，里面存储的是ziplist而不是单个数据了<br><br>ziplist是一个双端的链表，而且是占据一块完整的空间，一般的双端链表就是数据之间的位置差距比较大，但是ziplist会把数据放在连续的内存里面成为一个整体<br><br>ziplist在头部定义了一个zllen用于记录数据，是两个字节的大小，就是超过了两个字节2^16-1的大小，那么就会失效，需要遍历,复制度就是O（n）<br>
一个字节多大？<br>
一个字节等于8bit,为16位，所以数字最大就是2的16次方-1位，65535
<br><a data-href="Redis List底层以及操作#^1f1cf4" href="\2-领域\后端学习\redis\知识点\redis-list底层以及操作.html#^1f1cf4" class="internal-link" target="_self" rel="noopener">Redis List底层以及操作 &gt; ^1f1cf4</a><br><br>LInkedList里面的时间复杂度也是O（1），他的表头里面就包含了节点的数量<br>
<a data-tooltip-position="top" aria-label="Redis List底层以及操作 > ^c4c8df" data-href="Redis List底层以及操作#^c4c8df" href="\2-领域\后端学习\redis\知识点\redis-list底层以及操作.html#^c4c8df" class="internal-link" target="_self" rel="noopener">linkedList的查询时间复杂度</a><br><br>采用了整数和字典的方式，其中，如果集群数据都是整数，且整数数量不超过512个，那么就会采用intSet的方式，否则就会采用hashTable的方式编码，hashTable编码查询效率很高，能在O(1)的时间内找出一个数据是否存在<br>
<a data-href="Redis Set的操作以及底层#^c089ec" href="\2-领域\后端学习\redis\知识点\redis-set的操作以及底层.html#^c089ec" class="internal-link" target="_self" rel="noopener">Redis Set的操作以及底层 &gt; ^c089ec</a><br><br>从Set的底层编码来看，Set的底层有两种，一个是IntSet,一个是HashTable，前者是有序的，后者是无序的，但是整体来看应该视为无序的<br>
<a data-href="Redis Set的操作以及底层#^c089ec" href="\2-领域\后端学习\redis\知识点\redis-set的操作以及底层.html#^c089ec" class="internal-link" target="_self" rel="noopener">Redis Set的操作以及底层 &gt; ^c089ec</a><br><br>Hash的底层有两种编码方式，一个是zipList，这里是把hash的field当作entry存到ziplist里面，一种是hashtable，在元素少的时候用zipList，元素多的时候用hashTable<br>
<a data-href="Redis Hash#^ebb717" href="\2-领域\后端学习\redis\知识点\redis-hash.html#^ebb717" class="internal-link" target="_self" rel="noopener">Redis Hash &gt; ^ebb717</a><br><br>他有两种编码方式，查找key的复杂度也不同，如果是ziplist的编码方式，查找一个key的复杂度就是O（n）因为需要遍历<a data-href="Redis Hash#^3c724e" href="\2-领域\后端学习\redis\知识点\redis-hash.html#^3c724e" class="internal-link" target="_self" rel="noopener">Redis Hash &gt; ^3c724e</a><br>
如果是hashTable 的编码方式，那查询的复杂度就是o(1)<br><br>hashTable的表头里面包含了储存键值对数量的字段，叫used ，所以复杂度为o(1)<br>
<a href=".?query=tag:RedisHashTable" class="tag" target="_blank" rel="noopener">#RedisHashTable</a> <br>// from Redis 5.0.52 
typedef struct dictht {
  dictEntry **table;
  unsigned long size;
  unsigned long sizemark;
  //键值对数量
  unsigned long used;
 } dictht;
复制<br><br>首先Set底层有两种编码方式<a data-tooltip-position="top" aria-label="Redis Set的操作以及底层 > ^c089ec" data-href="Redis Set的操作以及底层#^c089ec" href="\2-领域\后端学习\redis\知识点\redis-set的操作以及底层.html#^c089ec" class="internal-link" target="_self" rel="noopener">Set的两种编码方式</a><br>
intset的编码方式在较小的时候，更节约内存，<br>
而HashTable 的编码方式查询更快，更适合大数据的场景<br><br><a data-tooltip-position="top" aria-label="Redis hashTable 底层以及操作 > ^1840ae" data-href="Redis hashTable 底层以及操作#^1840ae" href="\2-领域\后端学习\redis\知识点\redis-hashtable-底层以及操作.html#^1840ae" class="internal-link" target="_self" rel="noopener">索引值</a><br>
通过计算key的哈希值与哈希掩码一起计算出索引值，索引值就是其储存位置<br><br><a data-tooltip-position="top" aria-label="Redis hashTable 底层以及操作 > ^5f424e" data-href="Redis hashTable 底层以及操作#^5f424e" href="\2-领域\后端学习\redis\知识点\redis-hashtable-底层以及操作.html#^5f424e" class="internal-link" target="_self" rel="noopener">渐进式扩容</a><br>
渐进式扩容，redis首先并没有直接把hashtable的结构暴露出去，而是做了封装，封装里面存了两张表和一个rehashIdx的标记，一张表是数据，一张表是预备扩容的空表<br>
首先程序会为HashTable的1号表分配空间，新表大小为第一个大于等于原表used的2次方幂。在rehash进行期间，标记位rehashidx从o开始，每次对字典的键值对执行增删改查操作后，都会将rehashidx位置的数据迁移到1号表，然后将rehashidx加1，随着字典操作的不断执行，最终0号表的所有键值对都会被rehash到1号表上。之后，1号表会被设置成0号表，接着在1号表的位置创建一个新的空白表。<br><br><a data-tooltip-position="top" aria-label="Redis ZSet > ^c884be" data-href="Redis ZSet#^c884be" href="\2-领域\后端学习\redis\知识点\redis-zset.html#^c884be" class="internal-link" target="_self" rel="noopener">Zset的编码</a><br>
有两种，一种就是ziplist，一种就是跳表加字典<br><br><a data-tooltip-position="top" aria-label="Redis 跳表 > ^b71a01" data-href="Redis 跳表#^b71a01" href="\2-领域\后端学习\redis\知识点\redis-跳表.html#^b71a01" class="internal-link" target="_self" rel="noopener">查询节点总数复杂度</a><br><br><a data-href="Redis ZSet#^b8687e" href="\2-领域\后端学习\redis\知识点\redis-zset.html#^b8687e" class="internal-link" target="_self" rel="noopener">Redis ZSet &gt; ^b8687e</a><br><br><a data-href="Redis 跳表#^39e5b0" href="\2-领域\后端学习\redis\知识点\redis-跳表.html#^39e5b0" class="internal-link" target="_self" rel="noopener">Redis 跳表 &gt; ^39e5b0</a><br><br><a href=".?query=tag:Redis单线程" class="tag" target="_blank" rel="noopener">#Redis单线程</a><br><br>核心逻辑一直都是单线程的，一些异步删除是多线程的，6.0以后就网络I/o都是多线程的了<br><br><a data-href="Redis单线程，还是多线程？#^f15b30" href="\2-领域\后端学习\redis\知识点\redis单线程，还是多线程？.html#^f15b30" class="internal-link" target="_self" rel="noopener">Redis单线程，还是多线程？ &gt; ^f15b30</a><br><br><a data-href="[Redis单线程，还是多线程？#^d66aec" href="\[Redis单线程，还是多线程？#^d66aec" class="internal-link" target="_self" rel="noopener">[Redis单线程，还是多线程？ &gt; ^d66aec</a>]<br>
另外还有6.0后引用了多线程，而且默认关闭，这些也可以扩展回答看看<br><br><br><a data-href="Redis持久化#^2a63d7" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^2a63d7" class="internal-link" target="_self" rel="noopener">Redis持久化 &gt; ^2a63d7</a><br>
本质就是一个是快照，一个是日志追加<br><br><a data-tooltip-position="top" aria-label="Redis持久化 > ^1036f5" data-href="Redis持久化#^1036f5" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^1036f5" class="internal-link" target="_self" rel="noopener">如何触发RDB</a><br><br>首先是AOF的流程，实际上分为请求到来，请求处理，然后就是写入AOF文件，而在写入的流程里面，我们就有了刷盘的动作<br>
时机时机上是看我们配置的策略  <a data-tooltip-position="top" aria-label="Redis持久化 > ^013abe" data-href="Redis持久化#^013abe" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^013abe" class="internal-link" target="_self" rel="noopener">AOF写入流程</a><br><br><a data-href="Redis持久化#^83f61a" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^83f61a" class="internal-link" target="_self" rel="noopener">Redis持久化 &gt; ^83f61a</a><br><br><a data-href="Redis持久化#^f91732" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^f91732" class="internal-link" target="_self" rel="noopener">Redis持久化 &gt; ^f91732</a><br><br><a data-tooltip-position="top" aria-label="Redis持久化 > ^125e38" data-href="Redis持久化#^125e38" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^125e38" class="internal-link" target="_self" rel="noopener">混合持久化</a><br><br><a data-href="Redis持久化#^0a9ef9" href="\2-领域\后端学习\redis\知识点\redis持久化.html#^0a9ef9" class="internal-link" target="_self" rel="noopener">Redis持久化 &gt; ^0a9ef9</a><br><br><br>一般来说用到的多的是Cache Aside 也就是旁路缓存<a data-href="Redis缓存基础#总结" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#总结" class="internal-link" target="_self" rel="noopener">Redis缓存基础 &gt; 总结</a><br><br>Set 命令，详见 <a data-tooltip-position="top" aria-label="Redis缓存基础 > ^7205e7" data-href="Redis缓存基础#^7205e7" href="\2-领域\后端学习\redis\redis场景\redis缓存基础.html#^7205e7" class="internal-link" target="_self" rel="noopener">怎么设置缓存</a><br><br>查询的话，先去查缓存，然后缓存里没有就去查数据库，数据库没有就返回结果，有的话，就先更新缓存再返回结果<br><br>正常情况下，我们就是采用的就是新增的时候，就是正常的写入数据库然后加到缓存里，如果是更新数据，就去删除缓存中对应的 key value，如果删除失败也不会影响核心流程，我们直接忽略就行，因为我们还做了过期时间来兜底<br><br><br>先说怎么做，再讲解，不然显得像背书的<br>
我们就是比较重的方式去做的，用 redis 的 set key value nx ex second 配合多机部署做的，value 里面存的是时间戳凑出来的 id，这样每个线程只能解锁自己的，然后解锁是用的 delete，直接删除锁，使用了 Lua 脚本保证了原子性<br>
<br>首先分布式锁我们实现需要有四大特性，互斥安全对称可靠
<br>互斥性很简单，set 一个 key 值作为锁，利用 redis 里面的 set key value nx 如果 key 有值会返回 0 的特点，我们可以轻松做到互斥
<br>安全性是说，锁需要及时解锁，不能因为一些特殊情况导致卡住，设置过期时间，保证不会卡太久
<br>可靠性就是集群和多机部署<br>
<a data-href="分布式锁（很重要）" href="\2-领域\后端学习\redis\redis场景\分布式锁（很重要）.html" class="internal-link" target="_self" rel="noopener">分布式锁（很重要）</a>
<br><br>lua 脚本本身不具备原子性，但是 Redis 是单线程执行的，所以 lua 打包在一起的花会当作一个流程来执行的，整个流程会保证原子性，（这个流程是指，查询锁是不是自己的，然后如果是自己的就删除）<br><br>没有完全可靠的分布式锁，关键的业务还是要靠幂等来兜底，（幂等性指进行了多次操作，得到的结果是一致的）<br><br>RedLock 就是集群化部署的思路，多个机器多机部署详见 <a data-href="分布式锁（很重要）#多机部署" href="\2-领域\后端学习\redis\redis场景\分布式锁（很重要）.html#多机部署" class="internal-link" target="_self" rel="noopener">分布式锁（很重要） &gt; 多机部署</a><br>
需要说明的是，RedLock 比较重，看情况需不需要加]]></description><link>2-领域\后端学习\redis\redis面试题.html</link><guid isPermaLink="false">2-领域/后端学习/Redis/redis面试题.md</guid><pubDate>Tue, 20 Feb 2024 11:34:11 GMT</pubDate></item><item><title><![CDATA[浏览器加载前端]]></title><description><![CDATA[ 
 <br><br>
<br>预检请求 ✅ 2024-05-01<br>
首先要明确的是，预检请求由浏览器发送，并不是由前端代码发送，是一种检测跨域的安全保障
<br><br>OPTIONS 请求即预检请求，可用于检测服务器允许的 http 方法。当发起跨域请求时，由于安全原因，触发一定条件时浏览器会在正式请求之前自动先发起 OPTIONS 请求，即 CORS 预检请求，服务器若接受该跨域请求，浏览器才继续发起正式请求<br><br>跨域时候，我们会发送 cors 请求<br>
cors，把请求分为2种<br>
<br>简单请求（simple request）
<br>非简单请求 （preflight request），请求方法是OPTIONS
<br>那区分二者的标准是什么？ 如下：<br>
<br>请求的方法只能是GET, POST, HEAD的一种
<br>请求的header的只能是Accept，Accept-Language, Content-Language，Content-Type这些字段，不能超出这些字段
<br>对于请求的header的Content-Type字段，只能是以下值

<br>text/plain
<br>multipart/form-data
<br>application/x-www-form-urlencoded


<br>都满足以上条件的就是简单请求，否则就是非简单请求。<br>比如我们经常使用的Content-Type:application/json; charset=utf-8，这个请求就是非简单请求<br>非简单请求发送的时候，浏览器就会触发预检请求，这个在开发过程中可以通过修改浏览器的配置进行改变，比如 chrome 浏览器的非安全模式就能做到不发送预检请求，直接跨域<br><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/66484450" rel="noopener" class="external-link" href="https://zhuanlan.zhihu.com/p/66484450" target="_blank">一文搞懂跨域的所有问题，生活从此669~ - 知乎</a>]]></description><link>2-领域\前端学习\浏览器加载前端.html</link><guid isPermaLink="false">2-领域/前端学习/浏览器加载前端.md</guid><pubDate>Thu, 09 May 2024 11:14:22 GMT</pubDate></item><item><title><![CDATA[ant.design.pro框架]]></title><description><![CDATA[ 
 <br><br>关于 proTable 里面的一些 api 的初步使用<br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/yutingwu816/article/details/126620983" rel="noopener" class="external-link" href="https://blog.csdn.net/yutingwu816/article/details/126620983" target="_blank">ant-design proTable 之 request 和 actionRef初识_protable request-CSDN博客</a><br><br>只需要后端维护好 swagger 文档或者任意满足规范的文档，去这个地址<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512161219.png" referrerpolicy="no-referrer"><br>然后在 config. ts 里面有 openapi 的配置，配置好地址，<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512154934.png" referrerpolicy="no-referrer"><br>
运行 openapi 即可<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512154948.png" referrerpolicy="no-referrer"><br>
在这个目录下就会生成全部的接口和type<br>
<img alt="image.png" src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512155030.png" referrerpolicy="no-referrer">]]></description><link>2-领域\前端学习\ant.design.pro框架.html</link><guid isPermaLink="false">2-领域/前端学习/ant.design.pro框架.md</guid><pubDate>Sun, 12 May 2024 08:12:27 GMT</pubDate><enclosure url="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512161219.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://obsidian-pic-1317906728.cos.ap-nanjing.myqcloud.com/obsidian/20240512161219.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>